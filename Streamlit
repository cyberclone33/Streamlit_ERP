# Version 1.1: Added Daily Sales Chart
import streamlit as st
import pandas as pd
import os
import base64
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
import numpy as np

# --------------------------------------------------
# ğŸ”§ CONFIG & CONSTANTS ---------------------------
# --------------------------------------------------
st.set_page_config(page_title="æ•¸æ“šåœ–è¡¨ç”Ÿæˆå™¨", layout="wide")

UPLOADED_SALES_DIR = "uploaded_data/sales_files"
UPLOADED_BC_DIR = "uploaded_data/bc_files"
os.makedirs(UPLOADED_SALES_DIR, exist_ok=True)
os.makedirs(UPLOADED_BC_DIR, exist_ok=True)

# Data structure definitions
ORDER_LEVEL_COLUMNS = [
    "éŠ·è²¨å–®è™Ÿ", "è¨‚å–®å–®è™Ÿ", "éŠ·è²¨æ—¥æœŸ", "å®¢æˆ¶ä»£è™Ÿ", "å®¢æˆ¶åç¨±", "éƒ¨é–€ä»£è™Ÿ", "éƒ¨é–€åç¨±", 
    "ç™¼ç¥¨è™Ÿç¢¼", "æœªç¨…å°è¨ˆ", "ç‡Ÿæ¥­ç¨…", "æŠ˜è®“é‡‘é¡", "ç¨…å‰æŠ˜åƒ¹", "ç¸½è¨ˆé‡‘é¡", "å¯¦æ”¶ç¸½é¡", 
    "æˆæœ¬ç¸½é¡", "æ¯›åˆ©", "æ¯›åˆ©ç‡"
]

PRODUCT_LEVEL_COLUMNS = [
    "ç”¢å“ä»£è™Ÿ", "ç”¢å“åç¨±", "å€‰åˆ¥ä»£è™Ÿ", "å€‰åˆ¥åç¨±", "æ•¸é‡", "å–®ä½", "å–®åƒ¹", "å°è¨ˆ", 
    "æˆæœ¬ç¸½å€¼", "ç”¢å“æ¯›åˆ©", "ç”¢å“æ¯›åˆ©ç‡", "éŠ·å”®å–®åƒ¹1", "ç²¾æº–æˆæœ¬", "ç²¾æº–æ¯›åˆ©", 
    "å–®ä½ç®¡éŠ·æˆæœ¬", "ç®¡éŠ·æˆæœ¬åˆè¨ˆ", "*éŠ·è²¨æ—¥æœŸ", "*å®¢æˆ¶ä»£è™Ÿ", "*å®¢æˆ¶æ¢ä»¶", 
    "*éƒ¨é–€ä»£è™Ÿ", "*æ¥­å‹™ä»£è™Ÿ", "*æ¥­å‹™æ¢ä»¶"
]

BC_COLUMNS = [
    "ç”¢å“ä»£è™Ÿ", "ç”¢å“åç¨±", "æ•¸é‡", "å€‰åº«", "å–®ä½", "æˆæœ¬å–®åƒ¹", "æˆæœ¬ç¸½åƒ¹", 
    "å®‰å…¨å­˜é‡", "å» å•†ä»£è™Ÿ", "å» å•†ç°¡ç¨±", "æœ€å¾Œå‡ºè²¨æ—¥", "æœ€å¾Œé€²è²¨æ—¥", "éŠ·å”®å–®åƒ¹1", 
    "éŠ·å”®å–®åƒ¹2", "éŠ·å”®å–®åƒ¹3", "éŠ·å”®å–®åƒ¹4", "æœ€ä½å”®åƒ¹", "æ•¸é‡ç‚ºé›¶è‡ªå‹•ä¸‹æ¶", 
    "æŒçºŒä¸Šæ¶", "åœæ­¢ä¸Šæ¶", "å¤§é¡åç¨±", "ä¸­é¡åç¨±", "å°é¡åç¨±", "å‚™è¨»", 
    "EAN13ç¢¼", "CO128ç¢¼", "å»ºè­°å”®åƒ¹", "æ¯›åˆ©ç‡"
]

# Column definitions with explanations
COLUMN_DEFINITIONS = {
    # Order level
    "éŠ·è²¨å–®è™Ÿ": "è¨‚å–®çš„å”¯ä¸€è­˜åˆ¥ç¢¼",
    "è¨‚å–®å–®è™Ÿ": "å®¢æˆ¶çš„è¨‚å–®ç·¨è™Ÿ",
    "éŠ·è²¨æ—¥æœŸ": "å•†å“éŠ·å”®çš„æ—¥æœŸ",
    "å®¢æˆ¶ä»£è™Ÿ": "å®¢æˆ¶çš„ç·¨è™Ÿä»£ç¢¼",
    "å®¢æˆ¶åç¨±": "å®¢æˆ¶çš„å…¨å",
    "éƒ¨é–€ä»£è™Ÿ": "éƒ¨é–€çš„ç·¨è™Ÿä»£ç¢¼",
    "éƒ¨é–€åç¨±": "éƒ¨é–€çš„å…¨å",
    "ç™¼ç¥¨è™Ÿç¢¼": "ç™¼ç¥¨çš„è­˜åˆ¥è™Ÿç¢¼",
    "æœªç¨…å°è¨ˆ": "æœªåŒ…å«ç¨…é‡‘çš„ç¸½é‡‘é¡",
    "ç‡Ÿæ¥­ç¨…": "äº¤æ˜“ä¸­çš„ç¨…é‡‘é‡‘é¡",
    "æŠ˜è®“é‡‘é¡": "æŠ˜æ‰£æˆ–é€€æ¬¾çš„é‡‘é¡",
    "ç¨…å‰æŠ˜åƒ¹": "ç¨…å‰çš„æŠ˜æ‰£é‡‘é¡",
    "ç¸½è¨ˆé‡‘é¡": "åŒ…å«ç¨…é‡‘çš„ç¸½é‡‘é¡",
    "å¯¦æ”¶ç¸½é¡": "å¯¦éš›æ”¶åˆ°çš„ç¸½é‡‘é¡",
    "æˆæœ¬ç¸½é¡": "æ‰€æœ‰ç”¢å“çš„ç¸½æˆæœ¬",
    "æ¯›åˆ©": "ç¸½è¨ˆé‡‘é¡æ¸›å»æˆæœ¬ç¸½é¡çš„å·®å€¼",
    "æ¯›åˆ©ç‡": "æ¯›åˆ©é™¤ä»¥ç¸½è¨ˆé‡‘é¡çš„ç™¾åˆ†æ¯”",
    
    # Product level
    "ç”¢å“ä»£è™Ÿ": "ç”¢å“çš„å”¯ä¸€è­˜åˆ¥ç¢¼",
    "ç”¢å“åç¨±": "ç”¢å“çš„åç¨±",
    "å€‰åˆ¥ä»£è™Ÿ": "å€‰åº«çš„ç·¨è™Ÿä»£ç¢¼",
    "å€‰åˆ¥åç¨±": "å€‰åº«çš„åç¨±",
    "æ•¸é‡": "éŠ·å”®çš„ç”¢å“æ•¸é‡",
    "å–®ä½": "ç”¢å“çš„è¨ˆé‡å–®ä½",
    "å–®åƒ¹": "æ¯å–®ä½ç”¢å“çš„åƒ¹æ ¼",
    "å°è¨ˆ": "å–®åƒ¹ä¹˜ä»¥æ•¸é‡çš„é‡‘é¡",
    "æˆæœ¬ç¸½å€¼": "ç”¢å“çš„ç¸½æˆæœ¬",
    "ç”¢å“æ¯›åˆ©": "å°è¨ˆæ¸›å»æˆæœ¬ç¸½å€¼çš„å·®é¡",
    "ç”¢å“æ¯›åˆ©ç‡": "ç”¢å“æ¯›åˆ©é™¤ä»¥å°è¨ˆçš„ç™¾åˆ†æ¯”",
    "éŠ·å”®å–®åƒ¹1": "ç”¢å“çš„éŠ·å”®å–®åƒ¹",
    "ç²¾æº–æˆæœ¬": "è¨ˆç®—å¾Œçš„ç²¾ç¢ºæˆæœ¬",
    "ç²¾æº–æ¯›åˆ©": "è¨ˆç®—å¾Œçš„ç²¾ç¢ºæ¯›åˆ©",
    "å–®ä½ç®¡éŠ·æˆæœ¬": "æ¯å–®ä½ç”¢å“çš„ç®¡ç†å’ŒéŠ·å”®æˆæœ¬",
    "ç®¡éŠ·æˆæœ¬åˆè¨ˆ": "ç¸½ç®¡ç†å’ŒéŠ·å”®æˆæœ¬",
    "*éŠ·è²¨æ—¥æœŸ": "ç”¢å“å±¤ç´šçš„éŠ·è²¨æ—¥æœŸ",
    "*å®¢æˆ¶ä»£è™Ÿ": "ç”¢å“å±¤ç´šçš„å®¢æˆ¶ä»£è™Ÿ",
    "*å®¢æˆ¶æ¢ä»¶": "èˆ‡å®¢æˆ¶ç›¸é—œçš„ç‰¹æ®Šæ¢ä»¶",
    "*éƒ¨é–€ä»£è™Ÿ": "ç”¢å“å±¤ç´šçš„éƒ¨é–€ä»£è™Ÿ",
    "*æ¥­å‹™ä»£è™Ÿ": "è² è²¬éŠ·å”®çš„æ¥­å‹™ä»£è¡¨ç·¨è™Ÿ",
    "*æ¥­å‹™æ¢ä»¶": "èˆ‡æ¥­å‹™ä»£è¡¨ç›¸é—œçš„ç‰¹æ®Šæ¢ä»¶",
    
    # BC specific
    "å€‰åº«": "ç”¢å“å„²å­˜çš„å€‰åº«",
    "æˆæœ¬å–®åƒ¹": "æ¯å–®ä½ç”¢å“çš„æˆæœ¬",
    "æˆæœ¬ç¸½åƒ¹": "æˆæœ¬å–®åƒ¹ä¹˜ä»¥æ•¸é‡çš„ç¸½é‡‘é¡",
    "å®‰å…¨å­˜é‡": "åº«å­˜æ‡‰ä¿æŒçš„æœ€ä½æ•¸é‡",
    "å» å•†ä»£è™Ÿ": "ä¾›æ‡‰å•†çš„ç·¨è™Ÿä»£ç¢¼",
    "å» å•†ç°¡ç¨±": "ä¾›æ‡‰å•†çš„ç°¡ç¨±",
    "æœ€å¾Œå‡ºè²¨æ—¥": "æœ€è¿‘ä¸€æ¬¡å‡ºè²¨çš„æ—¥æœŸ",
    "æœ€å¾Œé€²è²¨æ—¥": "æœ€è¿‘ä¸€æ¬¡é€²è²¨çš„æ—¥æœŸ",
    "éŠ·å”®å–®åƒ¹2": "ç¬¬äºŒç¨®éŠ·å”®å–®åƒ¹",
    "éŠ·å”®å–®åƒ¹3": "ç¬¬ä¸‰ç¨®éŠ·å”®å–®åƒ¹",
    "éŠ·å”®å–®åƒ¹4": "ç¬¬å››ç¨®éŠ·å”®å–®åƒ¹",
    "æœ€ä½å”®åƒ¹": "ç”¢å“çš„æœ€ä½éŠ·å”®åƒ¹æ ¼",
    "æ•¸é‡ç‚ºé›¶è‡ªå‹•ä¸‹æ¶": "åº«å­˜ç‚ºé›¶æ™‚æ˜¯å¦è‡ªå‹•ä¸‹æ¶",
    "æŒçºŒä¸Šæ¶": "ç”¢å“æ˜¯å¦æŒçºŒä¸Šæ¶éŠ·å”®",
    "åœæ­¢ä¸Šæ¶": "ç”¢å“æ˜¯å¦åœæ­¢ä¸Šæ¶éŠ·å”®",
    "å¤§é¡åç¨±": "ç”¢å“çš„ä¸»è¦åˆ†é¡",
    "ä¸­é¡åç¨±": "ç”¢å“çš„æ¬¡è¦åˆ†é¡",
    "å°é¡åç¨±": "ç”¢å“çš„ç´°åˆ†é¡åˆ¥",
    "å‚™è¨»": "é—œæ–¼ç”¢å“çš„é™„åŠ èªªæ˜",
    "EAN13ç¢¼": "ç”¢å“çš„EAN13ç¢¼æ¢ç¢¼",
    "CO128ç¢¼": "ç”¢å“çš„Code 128æ¢ç¢¼",
    "å»ºè­°å”®åƒ¹": "å»ºè­°çš„é›¶å”®åƒ¹æ ¼",
    "æ¯›åˆ©ç‡": "æ¯›åˆ©é™¤ä»¥éŠ·å”®åƒ¹æ ¼çš„ç™¾åˆ†æ¯”"
}

# Initialize session state
if 'sales_files_uploaded' not in st.session_state:
    st.session_state.sales_files_uploaded = False
if 'bc_file_uploaded' not in st.session_state:
    st.session_state.bc_file_uploaded = False
if 'sales_data' not in st.session_state:
    st.session_state.sales_data = None
if 'bc_data' not in st.session_state:
    st.session_state.bc_data = None
if 'debug_info' not in st.session_state:
    st.session_state.debug_info = []
if 'processed_data' not in st.session_state:
    st.session_state.processed_data = {
        'order_level': None,
        'product_level': None,
        'monthly_summary': None
    }

# --------------------------------------------------
# ğŸ› ï¸ UTILITY FUNCTIONS ----------------------------
# --------------------------------------------------

def get_uploaded_files(directory):
    """Lists files in the specified directory."""
    if not os.path.exists(directory):
        return []
    return [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]

def save_uploaded_file(uploaded_file, directory):
    """Saves an uploaded file to the specified directory."""
    if uploaded_file is not None:
        file_path = os.path.join(directory, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        return file_path
    return None

def load_excel(file_source, is_path=False):
    """Generic Excel loader."""
    try:
        if is_path:
            return pd.read_excel(file_source)
        else:
            return pd.read_excel(file_source)
    except Exception as e:
        display_name = "[Unknown Source]"
        if isinstance(file_source, str):
            display_name = os.path.basename(file_source)
        elif hasattr(file_source, 'name'):
            display_name = file_source.name
        st.session_state.debug_info.append(f"è®€å–æª”æ¡ˆ '{display_name}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return pd.DataFrame()

def debug_file_headers(df, file_type):
    """Debug file headers and check if the required columns exist."""
    if df is None or df.empty:
        return f"âš ï¸ {file_type}æª”æ¡ˆç‚ºç©ºæˆ–ç„¡æ³•è®€å–"
    
    # Define required columns based on file type
    required_cols = []
    if file_type == "éŠ·å”®":
        # Check for order level columns as minimum requirement
        required_cols = ORDER_LEVEL_COLUMNS
    elif file_type == "BC":
        # Check for basic BC columns
        required_cols = ["ç”¢å“ä»£è™Ÿ", "ç”¢å“åç¨±", "æ•¸é‡", "å€‰åº«", "å–®ä½", "æˆæœ¬å–®åƒ¹", "æˆæœ¬ç¸½åƒ¹"]
    
    # Check columns
    columns = df.columns.tolist()
    st.session_state.debug_info.append(f"{file_type}æª”æ¡ˆæ¬„ä½: {', '.join(columns[:10])}...")
    
    missing_cols = [col for col in required_cols if col not in columns]
    
    if missing_cols:
        return f"âš ï¸ {file_type}æª”æ¡ˆç¼ºå°‘å¿…è¦æ¬„ä½: {', '.join(missing_cols)}"
    else:
        # If this is a sales file, also debug the hierarchical structure
        if file_type == "éŠ·å”®":
            debug_hierarchical_structure(df)
        return f"âœ… {file_type}æª”æ¡ˆæ¬„ä½æ­£ç¢º"

def debug_hierarchical_structure(df):
    """Debug the hierarchical structure of the sales data with order-level and product-level rows."""
    if df is None or df.empty:
        st.session_state.debug_info.append("ç„¡æ³•æª¢æŸ¥åˆ†å±¤çµæ§‹: æ•¸æ“šç‚ºç©º")
        return
    
    try:
        # Check for a key order column that should be present for order rows
        key_order_col = 'éŠ·è²¨å–®è™Ÿ'
        
        if key_order_col not in df.columns:
            st.session_state.debug_info.append(f"ç„¡æ³•æª¢æŸ¥åˆ†å±¤çµæ§‹: æ‰¾ä¸åˆ°é—œéµæ¬„ä½ '{key_order_col}'")
            return
        
        # Identify order rows (where key column is not null) and product rows (where key column is null)
        order_mask = df[key_order_col].notna()
        
        # Count order and product rows
        order_count = order_mask.sum()
        product_count = len(df) - order_count
        
        st.session_state.debug_info.append(f"æª”æ¡ˆåˆ†å±¤çµæ§‹: {order_count} ç­†è¨‚å–®è¡Œ, {product_count} ç­†ç”¢å“è¡Œ")
        
        # Check the pattern of order and product rows
        # The expectation is: order row, followed by 0+ product rows, then the next order row
        sequential_errors = 0
        prev_is_order = None
        consecutive_orders = 0
        consecutive_products = 0
        
        for i, is_order in enumerate(order_mask):
            if prev_is_order is not None:
                if prev_is_order and is_order:
                    # Two consecutive order rows - could be legitimate but log for awareness
                    consecutive_orders += 1
                elif not prev_is_order and not is_order:
                    # Multiple consecutive product rows - normal
                    consecutive_products += 1
            
            prev_is_order = is_order
        
        if consecutive_orders > 0:
            st.session_state.debug_info.append(f"æ³¨æ„: æª¢æ¸¬åˆ° {consecutive_orders} æ¬¡é€£çºŒçš„è¨‚å–®è¡Œï¼Œè«‹ç¢ºèªæ•¸æ“šçµæ§‹æ­£ç¢º")
        
        # Sample some order rows
        if order_count > 0:
            sample_order_indices = df[order_mask].head(2).index.tolist()
            for idx in sample_order_indices:
                sample_row = df.loc[idx]
                st.session_state.debug_info.append(f"è¨‚å–®è¡Œæ¨£æœ¬ (ç´¢å¼• {idx}):")
                # Show a few key columns from the order row
                for col in ['éŠ·è²¨å–®è™Ÿ', 'è¨‚å–®å–®è™Ÿ', 'éŠ·è²¨æ—¥æœŸ', 'å®¢æˆ¶åç¨±'][:3]:
                    if col in sample_row:
                        st.session_state.debug_info.append(f"  {col}: {sample_row[col]}")
        
        # Sample some product rows
        if product_count > 0:
            sample_product_indices = df[~order_mask].head(2).index.tolist()
            for idx in sample_product_indices:
                sample_row = df.loc[idx]
                st.session_state.debug_info.append(f"ç”¢å“è¡Œæ¨£æœ¬ (ç´¢å¼• {idx}):")
                # Show a few key columns from the product row
                for col in ['ç”¢å“ä»£è™Ÿ', 'ç”¢å“åç¨±', 'æ•¸é‡', 'å–®åƒ¹'][:3]:
                    if col in sample_row:
                        st.session_state.debug_info.append(f"  {col}: {sample_row[col]}")
    
    except Exception as e:
        st.session_state.debug_info.append(f"æª¢æŸ¥åˆ†å±¤çµæ§‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def identify_order_rows(df):
    """
    Identify rows that represent order headers in the hierarchical data structure.
    
    In the Excel sheet, order rows are those where all order-level columns are populated,
    while product rows have empty cells in order-level columns.
    """
    if df is None or df.empty:
        st.session_state.debug_info.append("ç„¡æ³•è­˜åˆ¥è¨‚å–®è¡Œ: æ•¸æ“šç‚ºç©º")
        return None
    
    try:
        # Define a key order column that should always be populated for order rows
        # and null for product rows (e.g., 'éŠ·è²¨å–®è™Ÿ')
        key_order_col = 'éŠ·è²¨å–®è™Ÿ'
        
        if key_order_col not in df.columns:
            st.session_state.debug_info.append(f"è­˜åˆ¥è¨‚å–®è¡Œå¤±æ•—: æ‰¾ä¸åˆ°é—œéµæ¬„ä½ '{key_order_col}'")
            return None
        
        # Create a mask where the key column is not null (identifying order rows)
        order_mask = df[key_order_col].notna()
        
        # Create a Series with row indices as values and their classification
        # True for order rows, False for product rows
        row_types = pd.Series(order_mask.values, index=df.index)
        
        st.session_state.debug_info.append(f"è­˜åˆ¥å‡º {row_types.sum()} ç­†è¨‚å–®è¡Œå’Œ {len(row_types) - row_types.sum()} ç­†ç”¢å“è¡Œ")
        
        return row_types
    
    except Exception as e:
        st.session_state.debug_info.append(f"è­˜åˆ¥è¨‚å–®è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        st.session_state.debug_info.append(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
        return None

def assign_order_to_products(df, row_types):
    """
    Assign each product row to its parent order by creating a mapping from product rows
    to their respective order rows based on the visual hierarchy in the Excel sheet.
    """
    if df is None or df.empty or row_types is None:
        st.session_state.debug_info.append("ç„¡æ³•åˆ†é…ç”¢å“åˆ°è¨‚å–®: è¼¸å…¥æ•¸æ“šç„¡æ•ˆ")
        return None
    
    try:
        # Create a mapping dictionary {product_row_index: order_row_index}
        order_to_product_map = {}
        
        # Get indices of order rows
        order_indices = row_types[row_types].index.tolist()
        
        # For each order row, find all product rows that follow it until the next order row
        for i in range(len(order_indices)):
            current_order_idx = order_indices[i]
            
            # Determine the end boundary (next order row or end of dataframe)
            next_order_idx = order_indices[i + 1] if i < len(order_indices) - 1 else len(df)
            
            # All rows between current_order_idx and next_order_idx (exclusive) are product rows
            for product_idx in range(current_order_idx + 1, next_order_idx):
                # Check if this is indeed a product row (not an order row)
                if not row_types.get(product_idx, True):  # Default to True if key doesn't exist
                    order_to_product_map[product_idx] = current_order_idx
        
        # Verify the mapping
        product_count = len(order_to_product_map)
        expected_product_count = len(row_types) - len(order_indices)
        
        st.session_state.debug_info.append(f"å·²ç‚º {product_count} ç­†ç”¢å“é …ç›®åˆ†é…è¨‚å–®")
        
        if product_count != expected_product_count:
            st.session_state.debug_info.append(f"è­¦å‘Š: ç”¢å“åˆ†é…æ•¸é‡({product_count})èˆ‡é æœŸæ•¸é‡({expected_product_count})ä¸ç¬¦")
        
        return order_to_product_map
    
    except Exception as e:
        st.session_state.debug_info.append(f"åˆ†é…ç”¢å“åˆ°è¨‚å–®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        st.session_state.debug_info.append(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
        return None

def split_data_levels(df):
    """
    Split the data into order level and product level information based on the hierarchical 
    structure in the Excel sheet, where order rows have all order-level columns populated
    and product rows have empty cells in order-level columns.
    """
    if df is None or df.empty:
        st.session_state.debug_info.append("ç„¡æ³•åˆ†é›¢è¨‚å–®å’Œç”¢å“å±¤ç´š: æ•¸æ“šç‚ºç©º")
        return None, None
    
    st.session_state.debug_info.append("æ­£åœ¨åˆ†é›¢è¨‚å–®å±¤ç´šå’Œç”¢å“å±¤ç´šæ•¸æ“š...")
    
    try:
        # Check if required columns exist
        missing_order_cols = [col for col in ORDER_LEVEL_COLUMNS if col not in df.columns]
        missing_product_cols = [col for col in PRODUCT_LEVEL_COLUMNS if col not in df.columns]
        
        if missing_order_cols:
            st.session_state.debug_info.append(f"ç¼ºå°‘è¨‚å–®å±¤ç´šæ¬„ä½: {missing_order_cols}")
        if missing_product_cols:
            st.session_state.debug_info.append(f"ç¼ºå°‘ç”¢å“å±¤ç´šæ¬„ä½: {missing_product_cols}")
        
        # First check: Verify if the data already has a hierarchical structure identified
        # in the preprocessing step (preprocess_hierarchical_excel)
        
        # We need to re-check how to identify order rows vs product rows
        # Let's first look at the data structure
        if 'éŠ·è²¨å–®è™Ÿ' in df.columns:
            # Initial check for basic order row identification
            possible_order_mask = df['éŠ·è²¨å–®è™Ÿ'].notna()
            order_count = possible_order_mask.sum()
            total_rows = len(df)
            
            if 0 < order_count < total_rows:
                # This looks like a proper hierarchical structure with both order and product rows
                st.session_state.debug_info.append(f"æª¢æ¸¬åˆ°åˆ†å±¤çµæ§‹: {order_count} ç­†è¨‚å–®è¡Œ å’Œ {total_rows - order_count} ç­†ç”¢å“è¡Œ")
                row_types = possible_order_mask
            else:
                # If all rows have order numbers, we need a more sophisticated check
                st.session_state.debug_info.append(f"æ‰€æœ‰è¡Œéƒ½æœ‰è¨‚å–®è™Ÿï¼Œå˜—è©¦ä½¿ç”¨ç”¢å“è­˜åˆ¥å­—æ®µé€²è¡Œæª¢æŸ¥")
                
                # Check if we have product-specific columns that can help identify product rows
                product_identifier_cols = ['ç”¢å“ä»£è™Ÿ', 'ç”¢å“åç¨±', 'æ•¸é‡', 'å–®åƒ¹', 'å°è¨ˆ']
                has_product_cols = any(col in df.columns for col in product_identifier_cols)
                
                if has_product_cols:
                    # Use product columns to help identify product vs order rows
                    for col in product_identifier_cols:
                        if col in df.columns:
                            # Check if this column helps distinguish order vs product rows
                            values_present = df[col].notna()
                            present_count = values_present.sum()
                            
                            # If this column has values for a subset of rows, it might help identify product rows
                            if 0 < present_count < total_rows:
                                st.session_state.debug_info.append(f"ä½¿ç”¨ç”¢å“æ¬„ä½ '{col}' è­˜åˆ¥ç”¢å“è¡Œ")
                                # Assume rows with product info are product rows, others are order rows
                                unique_invoice_count = df['éŠ·è²¨å–®è™Ÿ'].nunique()
                                st.session_state.debug_info.append(f"æ•¸æ“šä¸­æœ‰ {unique_invoice_count} å€‹å”¯ä¸€è¨‚å–®è™Ÿ")
                                
                                # If the count of unique invoice numbers is close to what we expect for order rows
                                if abs(unique_invoice_count - order_count) < 0.1 * total_rows:
                                    # Use this column to help identify product rows
                                    product_rows = values_present
                                    row_types = ~product_rows
                                    st.session_state.debug_info.append(f"æ ¹æ“š '{col}' è­˜åˆ¥å‡º {row_types.sum()} ç­†è¨‚å–®è¡Œ")
                                    break
                
                # If we still couldn't identify, fall back to unique invoice numbers
                if 'row_types' not in locals():
                    # Group by invoice number and only keep the first row of each group as an order row
                    unique_invoices = df.drop_duplicates(subset=['éŠ·è²¨å–®è™Ÿ'])
                    row_types = df.index.isin(unique_invoices.index)
                    st.session_state.debug_info.append(f"ä½¿ç”¨å”¯ä¸€è¨‚å–®è™Ÿè­˜åˆ¥å‡º {row_types.sum()} ç­†è¨‚å–®è¡Œ")
        else:
            st.session_state.debug_info.append("æ‰¾ä¸åˆ°è¨‚å–®è™Ÿæ¬„ä½ï¼Œç„¡æ³•è­˜åˆ¥è¨‚å–®è¡Œ")
            return None, None
        
        # Step 2: Create order level dataframe from identified order rows
        order_df = df[row_types].copy()
        
        # Ensure all required order columns are present
        order_cols = [col for col in ORDER_LEVEL_COLUMNS if col in df.columns]
        order_df = order_df[order_cols]
        
        # Step 3: Create product level dataframe from product rows
        product_df = df[~row_types].copy()
        
        # Ensure numeric columns are properly typed
        for col in ['ç¸½è¨ˆé‡‘é¡', 'æ¯›åˆ©', 'æˆæœ¬ç¸½é¡']:
            if col in order_df.columns and not pd.api.types.is_numeric_dtype(order_df[col]):
                order_df[col] = pd.to_numeric(order_df[col], errors='coerce')
        
        # Log the unique order count for verification
        if 'éŠ·è²¨å–®è™Ÿ' in order_df.columns:
            unique_orders = order_df['éŠ·è²¨å–®è™Ÿ'].nunique()
            st.session_state.debug_info.append(f"è¨‚å–®æ•¸æ“šä¸­æœ‰ {unique_orders} å€‹å”¯ä¸€è¨‚å–®è™Ÿ")
            
            # Check if we have duplicated orders
            if unique_orders < len(order_df):
                st.session_state.debug_info.append(f"è­¦å‘Š: ç™¼ç¾ {len(order_df) - unique_orders} å€‹é‡è¤‡è¨‚å–®")
                
                # Keep only the first occurrence of each order
                order_df = order_df.drop_duplicates(subset=['éŠ·è²¨å–®è™Ÿ'])
                st.session_state.debug_info.append(f"ç§»é™¤é‡è¤‡å¾Œæœ‰ {len(order_df)} ç­†è¨‚å–®")
        
        st.session_state.debug_info.append(f"æˆåŠŸå»ºç«‹è¨‚å–®å±¤ç´šæ•¸æ“šï¼Œå…± {len(order_df)} ç­†è¨‚å–®")
        
        # Step 4: Assign each product to its parent order
        if not product_df.empty:
            # Create a mapping from order number to order row
            if 'éŠ·è²¨å–®è™Ÿ' in order_df.columns and 'éŠ·è²¨å–®è™Ÿ' in product_df.columns:
                # Some product rows might already have order number
                order_map = {}
                for idx, row in order_df.iterrows():
                    if pd.notna(row['éŠ·è²¨å–®è™Ÿ']):
                        order_map[row['éŠ·è²¨å–®è™Ÿ']] = idx
                
                # For products with missing order info, try to find their parent order
                for idx, row in product_df.iterrows():
                    if pd.isna(row['éŠ·è²¨å–®è™Ÿ']) and idx > 0:
                        # Look for the closest preceding order row
                        preceding_orders = [o_idx for o_idx in order_df.index if o_idx < idx]
                        if preceding_orders:
                            closest_order_idx = max(preceding_orders)
                            if closest_order_idx in order_df.index:
                                product_df.at[idx, 'éŠ·è²¨å–®è™Ÿ'] = order_df.at[closest_order_idx, 'éŠ·è²¨å–®è™Ÿ']
            
            # Copy order information to product rows for key fields
            order_keys = ["éŠ·è²¨æ—¥æœŸ", "å®¢æˆ¶ä»£è™Ÿ", "å®¢æˆ¶åç¨±"]
            for key in order_keys:
                if key in order_df.columns and key in product_df.columns:
                    # Check if values are already present
                    missing_mask = product_df[key].isna()
                    if missing_mask.any() and 'éŠ·è²¨å–®è™Ÿ' in product_df.columns:
                        # For each product with missing value, look up from parent order
                        for idx in product_df[missing_mask].index:
                            order_num = product_df.at[idx, 'éŠ·è²¨å–®è™Ÿ']
                            if pd.notna(order_num) and order_num in order_df['éŠ·è²¨å–®è™Ÿ'].values:
                                # Find the order row with this order number
                                matching_order = order_df[order_df['éŠ·è²¨å–®è™Ÿ'] == order_num]
                                if not matching_order.empty:
                                    product_df.at[idx, key] = matching_order.iloc[0][key]
        
        st.session_state.debug_info.append(f"æˆåŠŸå»ºç«‹ç”¢å“å±¤ç´šæ•¸æ“šï¼Œå…± {len(product_df)} ç­†ç”¢å“é …ç›®")
        
        # Debug output - log a sample of order totals to help diagnose inflated totals
        if not order_df.empty and 'ç¸½è¨ˆé‡‘é¡' in order_df.columns:
            total_sum = order_df['ç¸½è¨ˆé‡‘é¡'].sum()
            st.session_state.debug_info.append(f"è¨‚å–®ç¸½é¡æ ¸å°: {total_sum}")
            
            # Show top 5 orders by amount
            top_orders = order_df.sort_values('ç¸½è¨ˆé‡‘é¡', ascending=False).head(5)
            st.session_state.debug_info.append("æœ€å¤§é‡‘é¡è¨‚å–®:")
            for idx, row in top_orders.iterrows():
                order_num = row['éŠ·è²¨å–®è™Ÿ'] if 'éŠ·è²¨å–®è™Ÿ' in row else 'Unknown'
                amount = row['ç¸½è¨ˆé‡‘é¡'] if 'ç¸½è¨ˆé‡‘é¡' in row else 0
                st.session_state.debug_info.append(f"  è¨‚å–® {order_num}: {amount}")
        
        return order_df, product_df
        
    except Exception as e:
        st.session_state.debug_info.append(f"åˆ†é›¢æ•¸æ“šå±¤ç´šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        st.session_state.debug_info.append(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
        return None, None

def get_top_products(df, n=50):
    """
    Get the top N products from the data based on sales.
    
    This function works with the hierarchical data structure where product-level data
    is identified by rows with product information but potentially missing order-level info.
    """
    if df is None or df.empty:
        st.session_state.debug_info.append("ç„¡æ³•ç²å–ç†±é–€ç”¢å“: æ•¸æ“šç‚ºç©º")
        return None
    
    try:
        # Log information about the input DataFrame
        st.session_state.debug_info.append(f"é–‹å§‹åˆ†æç”¢å“æ•¸æ“šï¼Œå…± {len(df)} è¡Œ")
        
        # Display available columns to help with debugging
        st.session_state.debug_info.append(f"å¯ç”¨æ¬„ä½: {', '.join(df.columns.tolist())}")
        
        # Check for presence of required columns
        required_cols = ["ç”¢å“ä»£è™Ÿ", "ç”¢å“åç¨±", "æ•¸é‡", "å°è¨ˆ"]
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            st.session_state.debug_info.append(f"ç¼ºå°‘å¿…è¦çš„ç”¢å“æ¬„ä½: {missing_cols}")
            
            # Try with alternative column names
            alternative_mappings = {
                "ç”¢å“ä»£è™Ÿ": ["å•†å“ä»£è™Ÿ", "å“é …ä»£è™Ÿ", "SKU", "å•†å“ç·¨è™Ÿ"],
                "ç”¢å“åç¨±": ["å•†å“åç¨±", "å“é …åç¨±", "å•†å“èªªæ˜"],
                "æ•¸é‡": ["è³¼è²·æ•¸é‡", "éŠ·å”®æ•¸é‡", "qty", "quantity"],
                "å°è¨ˆ": ["é‡‘é¡", "ç¸½é¡", "ç”¢å“é‡‘é¡", "éŠ·å”®é‡‘é¡", "amount"]
            }
            
            # Check if we can find alternative columns
            alternative_cols = {}
            for missing_col in missing_cols:
                if missing_col in alternative_mappings:
                    for alt_col in alternative_mappings[missing_col]:
                        if alt_col in df.columns:
                            alternative_cols[missing_col] = alt_col
                            st.session_state.debug_info.append(f"å°‡ä½¿ç”¨ '{alt_col}' ä»£æ›¿ '{missing_col}'")
                            break
            
            # If we still have missing columns, return None
            still_missing = [col for col in missing_cols if col not in alternative_cols]
            if still_missing:
                st.session_state.debug_info.append(f"ç„¡æ³•æ‰¾åˆ°æ›¿ä»£æ¬„ä½: {still_missing}")
                return None
            
            # Create a copy of the DataFrame with renamed columns
            working_df = df.copy()
            for original, alternative in alternative_cols.items():
                working_df[original] = working_df[alternative]
        else:
            working_df = df.copy()
        
        # Filter for product rows only - in hierarchical data, product rows typically have
        # product code and name but might be missing some order-level information
        if 'éŠ·è²¨å–®è™Ÿ' in working_df.columns:
            # For product rows in a hierarchical structure, filter appropriate rows
            # In your data structure, product rows might have empty order-level fields
            # Check the first few rows to see if we should filter
            sample_rows = working_df.head(10)
            has_hierarchical_structure = (sample_rows['ç”¢å“ä»£è™Ÿ'].notna().sum() > 0 and 
                                         sample_rows['éŠ·è²¨å–®è™Ÿ'].isna().any())
            
            if has_hierarchical_structure:
                st.session_state.debug_info.append("æª¢æ¸¬åˆ°åˆ†å±¤æ•¸æ“šçµæ§‹ï¼Œå°‡ç¯©é¸ç”¢å“å±¤ç´šè¡Œ")
                product_rows = working_df[working_df['ç”¢å“ä»£è™Ÿ'].notna()]
                st.session_state.debug_info.append(f"ç¯©é¸å¾Œæœ‰ {len(product_rows)} è¡Œç”¢å“æ•¸æ“š")
            else:
                product_rows = working_df
        else:
            product_rows = working_df
        
        # Ensure numeric data types
        for col in ['æ•¸é‡', 'å°è¨ˆ']:
            if not pd.api.types.is_numeric_dtype(product_rows[col]):
                st.session_state.debug_info.append(f"å°‡ {col} è½‰æ›ç‚ºæ•¸å€¼é¡å‹")
                product_rows[col] = pd.to_numeric(product_rows[col], errors='coerce')
        
        # Drop rows with NaN in critical columns
        product_rows = product_rows.dropna(subset=['ç”¢å“ä»£è™Ÿ', 'ç”¢å“åç¨±', 'æ•¸é‡', 'å°è¨ˆ'])
        st.session_state.debug_info.append(f"æ¸…ç†å¾Œæœ‰ {len(product_rows)} è¡Œæœ‰æ•ˆç”¢å“æ•¸æ“š")
        
        if product_rows.empty:
            st.session_state.debug_info.append("æ¸…ç†å¾Œæ²’æœ‰æœ‰æ•ˆçš„ç”¢å“æ•¸æ“š")
            return None
        
        # Sum quantities and amounts by product
        st.session_state.debug_info.append("é–‹å§‹æŒ‰ç”¢å“å½™ç¸½éŠ·å”®æ•¸æ“š")
        product_summary = product_rows.groupby(["ç”¢å“ä»£è™Ÿ", "ç”¢å“åç¨±"]).agg({
            "æ•¸é‡": "sum",
            "å°è¨ˆ": "sum"
        }).reset_index()
        
        st.session_state.debug_info.append(f"å½™ç¸½å¾Œæœ‰ {len(product_summary)} å€‹ä¸åŒç”¢å“")
        
        # Sort by sales amount (å°è¨ˆ) descending
        product_summary = product_summary.sort_values("å°è¨ˆ", ascending=False)
        
        # Take top N products
        top_products = product_summary.head(n)
        
        # Add rank column
        top_products.insert(0, "æ’å", range(1, len(top_products) + 1))
        
        # Format å°è¨ˆ for display
        top_products["å°è¨ˆ"] = top_products["å°è¨ˆ"].round(2).apply(lambda x: f"{x:,.2f}")
        
        st.session_state.debug_info.append(f"æˆåŠŸç”Ÿæˆç†±é–€ç”¢å“åˆ—è¡¨ï¼Œå…± {len(top_products)} é …")
        
        # Show a sample of the top products
        if len(top_products) > 0:
            sample = top_products.head(3).to_dict('records')
            st.session_state.debug_info.append(f"ç†±é–€ç”¢å“ç¯„ä¾‹: {sample}")
        
        return top_products
    
    except Exception as e:
        st.session_state.debug_info.append(f"ç²å–ç†±é–€ç”¢å“æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        st.session_state.debug_info.append(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
        return None

def convert_minguo_to_gregorian(date_str):
    """
    Convert date string from Minguo calendar (Taiwan) to Gregorian format.
    Example: "114.03.31" (Minguo) -> "2025-03-31" (Gregorian)
    """
    if not isinstance(date_str, str):
        return None
    
    # Handle different separators
    if '.' in date_str:
        parts = date_str.split('.')
    elif '/' in date_str:
        parts = date_str.split('/')
    elif '-' in date_str:
        parts = date_str.split('-')
    else:
        return None
    
    if len(parts) != 3:
        return None
    
    try:
        minguo_year = int(parts[0])
        month = int(parts[1])
        day = int(parts[2])
        
        # Convert Minguo year to Gregorian year (add 1911)
        gregorian_year = minguo_year + 1911
        
        # Return ISO format date string
        return f"{gregorian_year}-{month:02d}-{day:02d}"
    except:
        return None

def clean_numeric_value(value):
    """
    Clean numeric values by removing commas and converting to float.
    Example: "9,603" -> 9603.0
    """
    if pd.isna(value):
        return None
    
    if isinstance(value, (int, float)):
        return float(value)
    
    if not isinstance(value, str):
        return None
    
    # Remove commas and other non-numeric characters except for decimal points
    cleaned = value.replace(',', '')
    
    try:
        return float(cleaned)
    except:
        return None

def inspect_and_clean_data(df):
    """Inspect and clean data, providing details about critical columns."""
    if df is None or df.empty:
        st.session_state.debug_info.append("ç„¡æ³•æª¢æŸ¥æ•¸æ“šï¼šæ•¸æ“šç‚ºç©º")
        return None
    
    st.session_state.debug_info.append(f"é–‹å§‹æª¢æŸ¥å’Œæ¸…ç†æ•¸æ“šï¼ŒåŸå§‹æ•¸æ“šæœ‰ {len(df)} è¡Œ")
    
    # Create a working copy
    cleaned_df = df.copy()
    
    # Check critical columns for report generation
    critical_columns = ['éŠ·è²¨æ—¥æœŸ', 'ç¸½è¨ˆé‡‘é¡', 'æ¯›åˆ©']
    for col in critical_columns:
        if col not in cleaned_df.columns:
            st.session_state.debug_info.append(f"è‡´å‘½éŒ¯èª¤: ç¼ºå°‘å¿…è¦æ¬„ä½ {col}")
            return None
    
    # Check and convert ç¸½è¨ˆé‡‘é¡ column
    if 'ç¸½è¨ˆé‡‘é¡' in cleaned_df.columns:
        # Display sample values
        samples = cleaned_df['ç¸½è¨ˆé‡‘é¡'].head(5).tolist()
        st.session_state.debug_info.append(f"ç¸½è¨ˆé‡‘é¡ æ¨£æœ¬å€¼: {samples}")
        
        # Check data type
        original_type = cleaned_df['ç¸½è¨ˆé‡‘é¡'].dtype
        st.session_state.debug_info.append(f"ç¸½è¨ˆé‡‘é¡ åŸå§‹é¡å‹: {original_type}")
        
        # Convert to numeric if needed
        if not pd.api.types.is_numeric_dtype(cleaned_df['ç¸½è¨ˆé‡‘é¡']):
            st.session_state.debug_info.append("æ­£åœ¨å°‡ç¸½è¨ˆé‡‘é¡è½‰æ›ç‚ºæ•¸å€¼é¡å‹...")
            try:
                # First, try cleaning the values by removing commas
                cleaned_df['ç¸½è¨ˆé‡‘é¡'] = cleaned_df['ç¸½è¨ˆé‡‘é¡'].apply(clean_numeric_value)
                
                # Report conversion results
                nan_count = cleaned_df['ç¸½è¨ˆé‡‘é¡'].isna().sum()
                st.session_state.debug_info.append(f"è½‰æ›å¾Œæœ‰ {nan_count} å€‹ç©ºå€¼")
                new_samples = cleaned_df['ç¸½è¨ˆé‡‘é¡'].head(5).tolist()
                st.session_state.debug_info.append(f"è½‰æ›å¾Œçš„ç¸½è¨ˆé‡‘é¡æ¨£æœ¬: {new_samples}")
            except Exception as e:
                st.session_state.debug_info.append(f"è½‰æ›ç¸½è¨ˆé‡‘é¡æ™‚å‡ºéŒ¯: {e}")
                return None
    
    # Check and convert æ¯›åˆ© column
    if 'æ¯›åˆ©' in cleaned_df.columns:
        # Display sample values
        samples = cleaned_df['æ¯›åˆ©'].head(5).tolist()
        st.session_state.debug_info.append(f"æ¯›åˆ© æ¨£æœ¬å€¼: {samples}")
        
        # Check data type
        original_type = cleaned_df['æ¯›åˆ©'].dtype
        st.session_state.debug_info.append(f"æ¯›åˆ© åŸå§‹é¡å‹: {original_type}")
        
        # Convert to numeric if needed
        if not pd.api.types.is_numeric_dtype(cleaned_df['æ¯›åˆ©']):
            st.session_state.debug_info.append("æ­£åœ¨å°‡æ¯›åˆ©è½‰æ›ç‚ºæ•¸å€¼é¡å‹...")
            try:
                # First, try cleaning the values by removing commas
                cleaned_df['æ¯›åˆ©'] = cleaned_df['æ¯›åˆ©'].apply(clean_numeric_value)
                
                # Report conversion results
                nan_count = cleaned_df['æ¯›åˆ©'].isna().sum()
                st.session_state.debug_info.append(f"è½‰æ›å¾Œæœ‰ {nan_count} å€‹ç©ºå€¼")
                new_samples = cleaned_df['æ¯›åˆ©'].head(5).tolist()
                st.session_state.debug_info.append(f"è½‰æ›å¾Œçš„æ¯›åˆ©æ¨£æœ¬: {new_samples}")
            except Exception as e:
                st.session_state.debug_info.append(f"è½‰æ›æ¯›åˆ©æ™‚å‡ºéŒ¯: {e}")
                return None
    
    # Check and convert éŠ·è²¨æ—¥æœŸ column - special handling for Minguo calendar
    if 'éŠ·è²¨æ—¥æœŸ' in cleaned_df.columns:
        # Display sample values
        samples = cleaned_df['éŠ·è²¨æ—¥æœŸ'].head(5).tolist()
        st.session_state.debug_info.append(f"éŠ·è²¨æ—¥æœŸ æ¨£æœ¬å€¼: {samples}")
        
        # Check data type
        original_type = cleaned_df['éŠ·è²¨æ—¥æœŸ'].dtype
        st.session_state.debug_info.append(f"éŠ·è²¨æ—¥æœŸ åŸå§‹é¡å‹: {original_type}")
        
        # Convert to datetime if needed
        if not pd.api.types.is_datetime64_dtype(cleaned_df['éŠ·è²¨æ—¥æœŸ']):
            st.session_state.debug_info.append("æ­£åœ¨å°‡éŠ·è²¨æ—¥æœŸè½‰æ›ç‚ºæ—¥æœŸæ™‚é–“é¡å‹...")
            
            # Check if the date appears to be in Minguo format (Taiwan calendar)
            date_sample = str(samples[0]) if samples and samples[0] is not None else ""
            if isinstance(date_sample, str) and '.' in date_sample and len(date_sample.split('.')[0]) <= 3:
                st.session_state.debug_info.append("æª¢æ¸¬åˆ°æ°‘åœ‹æ—¥æœŸæ ¼å¼ (ä¾‹å¦‚: 114.03.31)")
                try:
                    # Convert Minguo dates to Gregorian dates
                    gregorian_dates = []
                    for date_val in cleaned_df['éŠ·è²¨æ—¥æœŸ']:
                        greg_date = convert_minguo_to_gregorian(str(date_val) if date_val is not None else "")
                        gregorian_dates.append(greg_date)
                    
                    # Create a new column with the converted dates
                    cleaned_df['éŠ·è²¨æ—¥æœŸ_å…¬æ›†'] = gregorian_dates
                    
                    # Convert to datetime
                    cleaned_df['éŠ·è²¨æ—¥æœŸ'] = pd.to_datetime(cleaned_df['éŠ·è²¨æ—¥æœŸ_å…¬æ›†'], errors='coerce')
                    
                    # Drop the temporary column
                    cleaned_df = cleaned_df.drop('éŠ·è²¨æ—¥æœŸ_å…¬æ›†', axis=1)
                    
                    # Report conversion results
                    nan_count = cleaned_df['éŠ·è²¨æ—¥æœŸ'].isna().sum()
                    st.session_state.debug_info.append(f"è½‰æ›å¾Œæœ‰ {nan_count} å€‹ç©ºå€¼")
                    
                    # Show sample of converted dates
                    new_samples = cleaned_df['éŠ·è²¨æ—¥æœŸ'].head(5).tolist()
                    st.session_state.debug_info.append(f"æ°‘åœ‹æ—¥æœŸè½‰æ›å¾Œçš„æ¨£æœ¬: {new_samples}")
                except Exception as e:
                    st.session_state.debug_info.append(f"è½‰æ›æ°‘åœ‹æ—¥æœŸæ™‚å‡ºéŒ¯: {e}")
                    import traceback
                    st.session_state.debug_info.append(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
            else:
                # Try standard format conversions
                try:
                    if isinstance(date_sample, str) and '/' in date_sample:
                        st.session_state.debug_info.append("æª¢æ¸¬åˆ°æ—¥æœŸæ ¼å¼åŒ…å« '/'")
                        cleaned_df['éŠ·è²¨æ—¥æœŸ'] = pd.to_datetime(cleaned_df['éŠ·è²¨æ—¥æœŸ'], format='%Y/%m/%d', errors='coerce')
                    else:
                        cleaned_df['éŠ·è²¨æ—¥æœŸ'] = pd.to_datetime(cleaned_df['éŠ·è²¨æ—¥æœŸ'], errors='coerce')
                    
                    # Report conversion results
                    nan_count = cleaned_df['éŠ·è²¨æ—¥æœŸ'].isna().sum()
                    st.session_state.debug_info.append(f"è½‰æ›å¾Œæœ‰ {nan_count} å€‹ç©ºå€¼")
                    new_samples = cleaned_df['éŠ·è²¨æ—¥æœŸ'].head(5).tolist()
                    st.session_state.debug_info.append(f"è½‰æ›å¾Œçš„éŠ·è²¨æ—¥æœŸæ¨£æœ¬: {new_samples}")
                except Exception as e:
                    st.session_state.debug_info.append(f"æ¨™æº–æ—¥æœŸè½‰æ›å‡ºéŒ¯: {e}")
    
    # Extract date components for grouping
    try:
        if pd.api.types.is_datetime64_dtype(cleaned_df['éŠ·è²¨æ—¥æœŸ']):
            # Normal processing for datetime fields
            cleaned_df['å¹´'] = cleaned_df['éŠ·è²¨æ—¥æœŸ'].dt.year
            cleaned_df['æœˆ'] = cleaned_df['éŠ·è²¨æ—¥æœŸ'].dt.month
            cleaned_df['å¹´æœˆ'] = cleaned_df['éŠ·è²¨æ—¥æœŸ'].dt.strftime('%Y-%m')
            
            # Check for missing values in the new columns
            year_na_count = cleaned_df['å¹´'].isna().sum()
            month_na_count = cleaned_df['æœˆ'].isna().sum()
            yearmonth_na_count = cleaned_df['å¹´æœˆ'].isna().sum()
            
            st.session_state.debug_info.append(f"æ—¥æœŸæ¬„ä½è™•ç†: å¹´æœ‰ {year_na_count} å€‹ç©ºå€¼, æœˆæœ‰ {month_na_count} å€‹ç©ºå€¼, å¹´æœˆæœ‰ {yearmonth_na_count} å€‹ç©ºå€¼")
            
            # Show some samples of the derived columns
            year_samples = cleaned_df['å¹´'].head(5).tolist()
            month_samples = cleaned_df['æœˆ'].head(5).tolist()
            yearmonth_samples = cleaned_df['å¹´æœˆ'].head(5).tolist()
            
            st.session_state.debug_info.append(f"å¹´ æ¨£æœ¬: {year_samples}")
            st.session_state.debug_info.append(f"æœˆ æ¨£æœ¬: {month_samples}")
            st.session_state.debug_info.append(f"å¹´æœˆ æ¨£æœ¬: {yearmonth_samples}")
        else:
            # Manual extraction for dates that failed datetime conversion
            st.session_state.debug_info.append("éŠ·è²¨æ—¥æœŸè½‰æ›å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥å¾åŸå§‹æ°‘åœ‹æ—¥æœŸæå–å¹´æœˆ...")
            
            # Check if we have the original date strings to work with
            if cleaned_df['éŠ·è²¨æ—¥æœŸ'].dtype == 'object':
                # Extract from Minguo format like "114.03.31"
                # First, extract the year and add 1911 to convert to Gregorian
                cleaned_df['å¹´'] = cleaned_df['éŠ·è²¨æ—¥æœŸ'].astype(str).str.extract(r'^(\d{1,3})\.').astype(float) + 1911
                
                # Extract month
                cleaned_df['æœˆ'] = cleaned_df['éŠ·è²¨æ—¥æœŸ'].astype(str).str.extract(r'\.(\d{1,2})\.').astype(float)
                
                # Create year-month string in the format YYYY-MM
                cleaned_df['å¹´æœˆ'] = cleaned_df['å¹´'].astype(int).astype(str) + '-' + cleaned_df['æœˆ'].astype(int).astype(str).str.zfill(2)
                
                # Check results
                sample_years = cleaned_df['å¹´'].head(5).tolist()
                sample_months = cleaned_df['æœˆ'].head(5).tolist()
                sample_yearmonths = cleaned_df['å¹´æœˆ'].head(5).tolist()
                
                st.session_state.debug_info.append(f"æ‰‹å‹•æå–çš„å¹´æ¨£æœ¬: {sample_years}")
                st.session_state.debug_info.append(f"æ‰‹å‹•æå–çš„æœˆæ¨£æœ¬: {sample_months}")
                st.session_state.debug_info.append(f"æ‰‹å‹•æå–çš„å¹´æœˆæ¨£æœ¬: {sample_yearmonths}")
            else:
                st.session_state.debug_info.append(f"ç„¡æ³•è­˜åˆ¥çš„æ—¥æœŸæ ¼å¼é¡å‹: {cleaned_df['éŠ·è²¨æ—¥æœŸ'].dtype}")
                raise ValueError("ç„¡æ³•è­˜åˆ¥çš„æ—¥æœŸæ ¼å¼")
    
    except Exception as e:
        st.session_state.debug_info.append(f"æå–æ—¥æœŸå…ƒç´ æ™‚å‡ºéŒ¯: {e}")
        import traceback
        st.session_state.debug_info.append(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
        
        # As a fallback, create year/month columns for order rows based on file name pattern
        # This assumes file names have a date pattern like "_20250301_" (for March 1, 2025)
        try:
            st.session_state.debug_info.append("å˜—è©¦å¾æª”æ¡ˆåç¨±æå–æ—¥æœŸè³‡è¨Šä½œç‚ºå‚™ç”¨...")
            
            # Get the current filename from session state if available
            current_file = st.session_state.get('current_filename', '')
            
            # Extract date pattern like "20250301" from the filename
            import re
            date_match = re.search(r'_(\d{8})_', current_file)
            
            if date_match:
                date_str = date_match.group(1)
                year = int(date_str[:4])
                month = int(date_str[4:6])
                
                st.session_state.debug_info.append(f"å¾æª”æ¡ˆåç¨± '{current_file}' ä¸­æå–çš„æ—¥æœŸ: å¹´={year}, æœˆ={month}")
                
                # Assign these values to all rows
                cleaned_df['å¹´'] = year
                cleaned_df['æœˆ'] = month
                cleaned_df['å¹´æœˆ'] = f"{year}-{month:02d}"
            else:
                # Just use current date as last resort
                st.session_state.debug_info.append("ä½¿ç”¨ç•¶å‰æ—¥æœŸä½œç‚ºå‚™ç”¨å€¼")
                cleaned_df['å¹´'] = datetime.now().year
                cleaned_df['æœˆ'] = datetime.now().month
                cleaned_df['å¹´æœˆ'] = f"{datetime.now().year}-{datetime.now().month:02d}"
        except Exception as sub_e:
            st.session_state.debug_info.append(f"å¾æª”æ¡ˆåç¨±æå–æ—¥æœŸå¤±æ•—: {sub_e}")
            # Absolute last resort - use current date
            cleaned_df['å¹´'] = datetime.now().year
            cleaned_df['æœˆ'] = datetime.now().month
            cleaned_df['å¹´æœˆ'] = f"{datetime.now().year}-{datetime.now().month:02d}"
    
    # Verify critical columns are properly formatted
    for col in ['ç¸½è¨ˆé‡‘é¡', 'æ¯›åˆ©']:
        if col in cleaned_df.columns and not pd.api.types.is_numeric_dtype(cleaned_df[col]):
            st.session_state.debug_info.append(f"å˜—è©¦å†æ¬¡è½‰æ› {col} ç‚ºæ•¸å€¼é¡å‹")
            try:
                cleaned_df[col] = pd.to_numeric(cleaned_df[col], errors='coerce')
            except:
                st.session_state.debug_info.append(f"è‡´å‘½éŒ¯èª¤: {col} ç„¡æ³•è½‰æ›ç‚ºæ•¸å€¼é¡å‹")
                return None
    
    # Check for the required columns for grouping
    if 'å¹´æœˆ' not in cleaned_df.columns:
        st.session_state.debug_info.append("è‡´å‘½éŒ¯èª¤: ç„¡æ³•ç”Ÿæˆå¹´æœˆæ¬„ä½")
        return None
    
    # Final dataframe stats
    final_rows = len(cleaned_df)
    null_counts = {col: cleaned_df[col].isna().sum() for col in ['å¹´æœˆ', 'ç¸½è¨ˆé‡‘é¡', 'æ¯›åˆ©'] if col in cleaned_df.columns}
    st.session_state.debug_info.append(f"æ•¸æ“šæ¸…ç†å®Œæˆï¼Œæœ€çµ‚æ•¸æ“šæœ‰ {final_rows} è¡Œ")
    st.session_state.debug_info.append(f"ç©ºå€¼çµ±è¨ˆ: {null_counts}")
    
    # Check if we have valid data for grouping - only for order-level rows
    if 'éŠ·è²¨å–®è™Ÿ' in cleaned_df.columns:
        valid_mask = cleaned_df['éŠ·è²¨å–®è™Ÿ'].notna()
        valid_rows = cleaned_df[valid_mask].dropna(subset=['å¹´æœˆ']).shape[0]
        st.session_state.debug_info.append(f"æœ‰æ•ˆè¨‚å–®è¡Œæ•¸: {valid_rows}")
        
        if valid_rows == 0:
            st.session_state.debug_info.append("è­¦å‘Š: æ²’æœ‰æœ‰æ•ˆçš„è¨‚å–®è¡Œå¯ä»¥é€²è¡ŒåŒ¯ç¸½")
            return None
    else:
        valid_rows = cleaned_df.dropna(subset=['å¹´æœˆ']).shape[0]
        st.session_state.debug_info.append(f"æœ‰æ•ˆæ•¸æ“šè¡Œæ•¸: {valid_rows}")
        
        if valid_rows == 0:
            st.session_state.debug_info.append("è­¦å‘Š: æ²’æœ‰æœ‰æ•ˆçš„æ•¸æ“šè¡Œå¯ä»¥é€²è¡ŒåŒ¯ç¸½")
            return None
    
    return cleaned_df

def preprocess_hierarchical_excel(df):
    """
    Preprocess the raw Excel data that has a hierarchical structure with order-level and
    product-level rows. This function handles the layout pattern where:
    
    1. Order rows have populated order-level columns
    2. Product rows below have empty order-level columns (white space)
    3. This pattern repeats until the next order row
    
    Returns a cleaned dataframe that preserves the hierarchical relationship.
    """
    if df is None or df.empty:
        st.session_state.debug_info.append("é è™•ç†æ•¸æ“š: è¼¸å…¥æ•¸æ“šç‚ºç©º")
        return None
    
    try:
        st.session_state.debug_info.append(f"é–‹å§‹é è™•ç†åˆ†å±¤æ•¸æ“šï¼ŒåŸå§‹æ•¸æ“šæœ‰ {len(df)} è¡Œ")
        
        # First, check if we have the necessary columns
        order_col_check = 'éŠ·è²¨å–®è™Ÿ'  # A key column that should be populated for order rows
        if order_col_check not in df.columns:
            st.session_state.debug_info.append(f"é è™•ç†å¤±æ•—: æ‰¾ä¸åˆ°é—œéµæ¬„ä½ '{order_col_check}'")
            return None
        
        # Create a copy to avoid modifying the original
        processed_df = df.copy()
        
        # Identify which rows are order rows (have populated order-level columns)
        # and which are product rows (have empty order-level columns)
        order_rows = processed_df[order_col_check].notna()
        
        st.session_state.debug_info.append(f"è­˜åˆ¥å‡º {order_rows.sum()} ç­†è¨‚å–®è¡Œå’Œ {len(order_rows) - order_rows.sum()} ç­†ç”¢å“è¡Œ")
        
        # Now create a column that indicates which order each row belongs to
        processed_df['order_id_internal'] = None
        
        # For each order row, assign its index as the order_id_internal
        # For product rows, assign the order_id_internal of the previous order
        current_order_id = None
        for idx in processed_df.index:
            if order_rows[idx]:
                # This is an order row, use its own value as order_id
                current_order_id = processed_df.at[idx, order_col_check]
                processed_df.at[idx, 'order_id_internal'] = current_order_id
            else:
                # This is a product row, use the current order_id
                processed_df.at[idx, 'order_id_internal'] = current_order_id
        
        # Count how many product rows got assigned to orders
        assigned_products = processed_df[~order_rows]['order_id_internal'].notna().sum()
        st.session_state.debug_info.append(f"å·²ç‚º {assigned_products} ç­†ç”¢å“é …ç›®åˆ†é…è¨‚å–®")
        
        # Now propagate order-level information to product rows that belong to each order
        for order_col in ORDER_LEVEL_COLUMNS:
            if order_col in processed_df.columns:
                # For each group of rows with the same order_id_internal, fill the NaN values
                # in order columns with the value from the order row
                # Using ffill() instead of fillna(method='ffill') to avoid FutureWarning
                processed_df[order_col] = processed_df.groupby('order_id_internal')[order_col].transform(
                    lambda x: x.ffill()
                )
        
        # Drop the temporary column
        processed_df = processed_df.drop('order_id_internal', axis=1)
        
        st.session_state.debug_info.append(f"é è™•ç†å®Œæˆï¼Œä¿ç•™äº† {len(processed_df)} è¡Œæ•¸æ“š")
        return processed_df
        
    except Exception as e:
        st.session_state.debug_info.append(f"é è™•ç†åˆ†å±¤æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        st.session_state.debug_info.append(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
        return None

def process_sales_data(df_list, filenames=None):
    """
    Process sales data from multiple DataFrames with hierarchical structure.
    The filenames parameter is used to extract date information as a fallback.
    """
    if not df_list:
        st.session_state.debug_info.append("è™•ç†éŠ·å”®æ•¸æ“š: æ²’æœ‰æä¾›æ•¸æ“šæ¡†")
        return None
    
    # Store filenames for potential date extraction
    if filenames and len(filenames) > 0:
        st.session_state.current_filename = filenames[0]
        st.session_state.debug_info.append(f"è¨­å®šç•¶å‰æª”æ¡ˆåç¨±: {st.session_state.current_filename}")
    
    # Combine all DataFrames
    try:
        # Process each dataframe to handle the hierarchical structure before combining
        processed_dfs = []
        for i, df in enumerate(df_list):
            current_file = filenames[i] if filenames and i < len(filenames) else f"æª”æ¡ˆ {i+1}"
            st.session_state.debug_info.append(f"æ­£åœ¨è™•ç† {current_file}ï¼ŒåŸå§‹è¡Œæ•¸: {len(df)}")
            processed_df = preprocess_hierarchical_excel(df)
            if processed_df is not None:
                processed_dfs.append(processed_df)
        
        if not processed_dfs:
            st.session_state.debug_info.append("é è™•ç†å¾Œæ²’æœ‰å¯ç”¨çš„æ•¸æ“šæ¡†")
            return None
        
        # Combine the processed DataFrames
        combined_df = pd.concat(processed_dfs, ignore_index=True)
        st.session_state.debug_info.append(f"æˆåŠŸåˆä½µ {len(processed_dfs)} å€‹è™•ç†å¾Œçš„æ•¸æ“šæª”æ¡ˆï¼Œå…± {len(combined_df)} è¡Œ")
        
        # Check for any data in the DataFrame
        if combined_df.empty:
            st.session_state.debug_info.append("åˆä½µå¾Œçš„éŠ·å”®æ•¸æ“šç‚ºç©º")
            return None
        
        # Inspect and clean the data
        cleaned_df = inspect_and_clean_data(combined_df)
        
        # Split into order and product level
        if cleaned_df is not None:
            order_df, product_df = split_data_levels(cleaned_df)
            st.session_state.processed_data['order_level'] = order_df
            st.session_state.processed_data['product_level'] = product_df
            
            # Show order level stats
            if order_df is not None and not order_df.empty:
                # Count non-null values in key columns
                totals_count = order_df['ç¸½è¨ˆé‡‘é¡'].notna().sum()
                profit_count = order_df['æ¯›åˆ©'].notna().sum()
                date_count = order_df['éŠ·è²¨æ—¥æœŸ'].notna().sum()
                
                st.session_state.debug_info.append(f"è¨‚å–®å±¤ç´šçµ±è¨ˆ: {len(order_df)} ç­†è¨‚å–®")
                st.session_state.debug_info.append(f"è¨‚å–®å±¤ç´šæœ‰æ•ˆå€¼: ç¸½è¨ˆé‡‘é¡ {totals_count}, æ¯›åˆ© {profit_count}, éŠ·è²¨æ—¥æœŸ {date_count}")
        
        return cleaned_df
        
    except Exception as e:
        st.session_state.debug_info.append(f"è™•ç†éŠ·å”®æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        st.session_state.debug_info.append(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
        return None

def generate_monthly_sales_report(df):
    """
    Generate monthly sales report from processed data.
    
    This function is designed to work with hierarchical data where order-level
    rows contain sales totals and dates. Only the order-level rows should be used
    for generating the monthly summary to avoid double-counting.
    """
    # éå¸¸é‡è¦ï¼šç›´æ¥ä½¿ç”¨å·²è™•ç†çš„è¨‚å–®å±¤ç´šæ•¸æ“šï¼Œè€Œä¸æ˜¯å‚³å…¥çš„ df
    if st.session_state.processed_data['order_level'] is not None:
        # ä½¿ç”¨è¨‚å–®å±¤ç´šæ•¸æ“š
        order_df = st.session_state.processed_data['order_level'].copy()
        st.session_state.debug_info.append(f"ç›´æ¥ä½¿ç”¨è¨‚å–®å±¤ç´šæ•¸æ“šï¼š{len(order_df)} è¡Œï¼Œè€Œéå®Œæ•´è³‡æ–™ {len(df)} è¡Œ")
    else:
        st.session_state.debug_info.append("ç„¡æ³•æ‰¾åˆ°è¨‚å–®å±¤ç´šæ•¸æ“š")
        return None
    
    # æª¢æŸ¥å¿…è¦æ¬„ä½
    if not all(col in order_df.columns for col in ['å¹´æœˆ', 'ç¸½è¨ˆé‡‘é¡', 'æ¯›åˆ©']):
        st.session_state.debug_info.append("è¨‚å–®å±¤ç´šæ•¸æ“šç¼ºå°‘å¿…è¦æ¬„ä½")
        return None
    
    # è¨˜éŒ„ç¸½é‡‘é¡ä½œç‚ºåƒè€ƒ
    total_amount = order_df['ç¸½è¨ˆé‡‘é¡'].sum()
    st.session_state.debug_info.append(f"è¨‚å–®å±¤ç´šç¸½è¨ˆé‡‘é¡ï¼š{total_amount}")
    
    # æŒ‰æœˆä»½åˆ†çµ„
    monthly_report = order_df.groupby('å¹´æœˆ').agg({
        'ç¸½è¨ˆé‡‘é¡': 'sum',
        'æ¯›åˆ©': 'sum'
    }).reset_index()
    
    # è¨ˆç®—æ¯›åˆ©ç‡
    monthly_report['æ¯›åˆ©ç‡'] = (monthly_report['æ¯›åˆ©'] / monthly_report['ç¸½è¨ˆé‡‘é¡']) * 100
    
    # æ’åº
    monthly_report = monthly_report.sort_values('å¹´æœˆ')
    
    # ç´€éŒ„çµæœ
    st.session_state.debug_info.append(f"æœˆåº¦å ±è¡¨ç”ŸæˆæˆåŠŸï¼Œå…± {len(monthly_report)} å€‹æœˆä»½")
    st.session_state.debug_info.append(f"æœˆåº¦å ±è¡¨ç¸½è¨ˆé‡‘é¡ï¼š{monthly_report['ç¸½è¨ˆé‡‘é¡'].sum()}")
    
    # é¡¯ç¤ºæ¨£æœ¬
    sample_data = monthly_report.head().to_dict('records')
    st.session_state.debug_info.append(f"æœˆåº¦å ±è¡¨æ¨£æœ¬ï¼š{sample_data}")
    
    # å„²å­˜åˆ° session state
    st.session_state.processed_data['monthly_summary'] = monthly_report
    
    return monthly_report

def create_sales_charts(df):
    """Create visualizations for sales data."""
    if df is None or df.empty:
        st.session_state.debug_info.append("ç„¡æ³•ç”Ÿæˆåœ–è¡¨: è¼¸å…¥æ•¸æ“šç‚ºç©º")
        return None, None
    
    try:
        # Ensure data is properly formatted for charts
        if 'å¹´æœˆ' not in df.columns or 'ç¸½è¨ˆé‡‘é¡' not in df.columns or 'æ¯›åˆ©' not in df.columns:
            st.session_state.debug_info.append(f"ç¹ªè£½åœ–è¡¨å¤±æ•—: ç¼ºå°‘å¿…è¦æ¬„ä½ã€‚ç¾æœ‰æ¬„ä½: {df.columns.tolist()}")
            return None, None
        
        # Monthly sales trend chart
        sales_fig = px.line(
            df, x='å¹´æœˆ', y='ç¸½è¨ˆé‡‘é¡', 
            title='æœˆåº¦ç‡Ÿæ¥­é¡è¶¨å‹¢',
            labels={'å¹´æœˆ': 'å¹´æœˆ', 'ç¸½è¨ˆé‡‘é¡': 'ç‡Ÿæ¥­é¡ (å…ƒ)'},
            markers=True
        )
        sales_fig.update_layout(
            xaxis_title='å¹´æœˆ',
            yaxis_title='ç‡Ÿæ¥­é¡ (å…ƒ)',
            height=400
        )
        
        # Monthly profit trend chart
        profit_fig = px.line(
            df, x='å¹´æœˆ', y=['æ¯›åˆ©', 'æ¯›åˆ©ç‡'], 
            title='æœˆåº¦æ¯›åˆ©è¶¨å‹¢',
            labels={'å¹´æœˆ': 'å¹´æœˆ', 'value': 'é‡‘é¡/ç™¾åˆ†æ¯”', 'variable': 'æŒ‡æ¨™'},
            markers=True
        )
        profit_fig.update_layout(
            xaxis_title='å¹´æœˆ',
            yaxis_title='é‡‘é¡/ç™¾åˆ†æ¯”',
            height=400
        )
        
        st.session_state.debug_info.append("åœ–è¡¨ç”ŸæˆæˆåŠŸ")
        return sales_fig, profit_fig
        
    except Exception as e:
        st.session_state.debug_info.append(f"ç”Ÿæˆåœ–è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None, None

def create_monthly_comparison_chart(order_df):
    """
    Create a daily revenue comparison chart for multiple months.
    Each month will be represented by a different colored line.
    
    Args:
        order_df: DataFrame containing order-level data with dates and amounts
    
    Returns:
        A plotly figure object with the comparison chart
    """
    if order_df is None or order_df.empty:
        st.session_state.debug_info.append("ç„¡æ³•ç”Ÿæˆæœˆåº¦æ¯”è¼ƒåœ–è¡¨: è¨‚å–®æ•¸æ“šç‚ºç©º")
        return None
    
    try:
        # Check if required columns exist
        if 'éŠ·è²¨æ—¥æœŸ' not in order_df.columns or 'ç¸½è¨ˆé‡‘é¡' not in order_df.columns:
            st.session_state.debug_info.append("ç”Ÿæˆæœˆåº¦æ¯”è¼ƒåœ–è¡¨å¤±æ•—: ç¼ºå°‘éŠ·è²¨æ—¥æœŸæˆ–ç¸½è¨ˆé‡‘é¡æ¬„ä½")
            return None
        
        # Ensure date column is datetime type
        if not pd.api.types.is_datetime64_dtype(order_df['éŠ·è²¨æ—¥æœŸ']):
            st.session_state.debug_info.append("å°‡éŠ·è²¨æ—¥æœŸè½‰æ›ç‚ºæ—¥æœŸæ™‚é–“æ ¼å¼ä»¥é€²è¡Œæ¯”è¼ƒ")
            order_df['éŠ·è²¨æ—¥æœŸ'] = pd.to_datetime(order_df['éŠ·è²¨æ—¥æœŸ'], errors='coerce')
        
        # Extract year-month and day for grouping
        order_df['å¹´æœˆ'] = order_df['éŠ·è²¨æ—¥æœŸ'].dt.strftime('%Y-%m')
        order_df['æ—¥'] = order_df['éŠ·è²¨æ—¥æœŸ'].dt.day
        
        # Get unique months
        months = order_df['å¹´æœˆ'].unique()
        
        # If there's only one month, we can't do a comparison
        if len(months) <= 1:
            st.session_state.debug_info.append(f"åªæœ‰ä¸€å€‹æœˆä»½çš„æ•¸æ“š ({months[0]})ï¼Œç„¡æ³•é€²è¡Œæ¯”è¼ƒ")
            return None
        
        st.session_state.debug_info.append(f"æ‰¾åˆ° {len(months)} å€‹ä¸åŒæœˆä»½: {', '.join(months)}")
        
        # Group by year-month and day, summing the total amount
        daily_sales = order_df.groupby(['å¹´æœˆ', 'æ—¥'])['ç¸½è¨ˆé‡‘é¡'].sum().reset_index()
        
        # Create figure
        fig = go.Figure()
        
        # Add a line for each month with different colors
        for month in months:
            month_data = daily_sales[daily_sales['å¹´æœˆ'] == month].copy()
            
            # Skip if no data for this month
            if month_data.empty:
                continue
                
            # Sort by day
            month_data = month_data.sort_values('æ—¥')
            
            # Add line with month name in legend
            fig.add_trace(go.Scatter(
                x=month_data['æ—¥'],
                y=month_data['ç¸½è¨ˆé‡‘é¡'],
                mode='lines+markers',
                name=month,
                hovertemplate='æ—¥æœŸ: %{x}æ—¥<br>é‡‘é¡: $%{y:,.2f}'
            ))
        
        # Update layout
        fig.update_layout(
            title='å„æœˆä»½æ¯æ—¥éŠ·å”®é‡‘é¡æ¯”è¼ƒ',
            xaxis_title='æ—¥æœŸ',
            yaxis_title='éŠ·å”®é‡‘é¡ (å…ƒ)',
            legend_title='æœˆä»½',
            xaxis=dict(
                tickmode='linear',
                tick0=1,
                dtick=1
            ),
            height=500,
            margin=dict(l=40, r=40, t=50, b=40)
        )
        
        return fig
        
    except Exception as e:
        st.session_state.debug_info.append(f"ç”Ÿæˆæœˆåº¦æ¯”è¼ƒåœ–è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        st.session_state.debug_info.append(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
        return None

def create_monthly_comparison_table(monthly_summary):
    """
    Create a comparison table for monthly sales data.
    
    Args:
        monthly_summary: DataFrame containing monthly summary data
    
    Returns:
        A formatted DataFrame for display
    """
    if monthly_summary is None or monthly_summary.empty:
        st.session_state.debug_info.append("ç„¡æ³•ç”Ÿæˆæœˆåº¦æ¯”è¼ƒè¡¨: æœˆåº¦æ‘˜è¦æ•¸æ“šç‚ºç©º")
        return None
    
    try:
        # Check if required columns exist
        required_cols = ['å¹´æœˆ', 'ç¸½è¨ˆé‡‘é¡', 'æ¯›åˆ©']
        missing_cols = [col for col in required_cols if col not in monthly_summary.columns]
        if missing_cols:
            st.session_state.debug_info.append(f"ç”Ÿæˆæœˆåº¦æ¯”è¼ƒè¡¨å¤±æ•—: ç¼ºå°‘å¿…è¦æ¬„ä½ {', '.join(missing_cols)}")
            return None
        
        # Make a copy to avoid modifying the original
        comparison_df = monthly_summary.copy()
        
        # Sort by year-month
        comparison_df = comparison_df.sort_values('å¹´æœˆ')
        
        # Create growth rate columns
        comparison_df['éŠ·å”®æˆé•·ç‡'] = comparison_df['ç¸½è¨ˆé‡‘é¡'].pct_change() * 100
        comparison_df['æ¯›åˆ©æˆé•·ç‡'] = comparison_df['æ¯›åˆ©'].pct_change() * 100
        
        # Calculate month-over-month growth for each row
        if 'æ¯›åˆ©' in comparison_df.columns and 'ç¸½è¨ˆé‡‘é¡' in comparison_df.columns:
            comparison_df['æ¯›åˆ©ç‡'] = (comparison_df['æ¯›åˆ©'] / comparison_df['ç¸½è¨ˆé‡‘é¡']) * 100
            comparison_df['æ¯›åˆ©ç‡è®ŠåŒ–'] = comparison_df['æ¯›åˆ©ç‡'].diff()
        
        # Format currency columns
        for col in ['ç¸½è¨ˆé‡‘é¡', 'æ¯›åˆ©']:
            if col in comparison_df.columns:
                comparison_df[col] = comparison_df[col].map('${:,.2f}'.format)
        
        # Format percentage columns
        for col in ['éŠ·å”®æˆé•·ç‡', 'æ¯›åˆ©æˆé•·ç‡']:
            if col in comparison_df.columns:
                comparison_df[col] = comparison_df[col].map('{:+.2f}%'.format)
        
        if 'æ¯›åˆ©ç‡' in comparison_df.columns:
            comparison_df['æ¯›åˆ©ç‡'] = comparison_df['æ¯›åˆ©ç‡'].map('{:.2f}%'.format)
        
        if 'æ¯›åˆ©ç‡è®ŠåŒ–' in comparison_df.columns:
            comparison_df['æ¯›åˆ©ç‡è®ŠåŒ–'] = comparison_df['æ¯›åˆ©ç‡è®ŠåŒ–'].map('{:+.2f}%'.format)
        
        # Rename columns for display
        column_rename = {
            'å¹´æœˆ': 'æœˆä»½',
            'ç¸½è¨ˆé‡‘é¡': 'ç¸½éŠ·å”®é¡',
            'æ¯›åˆ©': 'ç¸½æ¯›åˆ©',
            'æ¯›åˆ©ç‡': 'æ¯›åˆ©ç‡',
            'éŠ·å”®æˆé•·ç‡': 'éŠ·å”®æˆé•·',
            'æ¯›åˆ©æˆé•·ç‡': 'æ¯›åˆ©æˆé•·',
            'æ¯›åˆ©ç‡è®ŠåŒ–': 'æ¯›åˆ©ç‡è®ŠåŒ–'
        }
        
        display_df = comparison_df.rename(columns=column_rename)
        
        # Reorder columns for clarity
        column_order = ['æœˆä»½', 'ç¸½éŠ·å”®é¡', 'éŠ·å”®æˆé•·', 'ç¸½æ¯›åˆ©', 'æ¯›åˆ©æˆé•·', 'æ¯›åˆ©ç‡', 'æ¯›åˆ©ç‡è®ŠåŒ–']
        display_df = display_df[[col for col in column_order if col in display_df.columns]]
        
        return display_df
        
    except Exception as e:
        st.session_state.debug_info.append(f"ç”Ÿæˆæœˆåº¦æ¯”è¼ƒè¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        st.session_state.debug_info.append(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
        return None

# Version 1.1: Added Daily Sales Chart function
def create_daily_sales_chart(order_df, selected_month=None):
    """
    Create a bar chart showing daily sales within a specific month.
    
    Args:
        order_df: DataFrame containing order-level data with dates and amounts
        selected_month: The month to display (format: 'YYYY-MM'), if None, uses the most recent month
    
    Returns:
        A tuple containing (plotly figure object, list of available months)
    """
    import plotly.graph_objects as go
    import pandas as pd
    from datetime import datetime
    
    if order_df is None or order_df.empty:
        st.warning("ç„¡æ³•ç”Ÿæˆæ—¥éŠ·å”®åœ–è¡¨: è¨‚å–®æ•¸æ“šç‚ºç©º")
        return None, []
    
    try:
        # Check if required columns exist
        if 'éŠ·è²¨æ—¥æœŸ' not in order_df.columns or 'ç¸½è¨ˆé‡‘é¡' not in order_df.columns:
            st.warning("ç”Ÿæˆæ—¥éŠ·å”®åœ–è¡¨å¤±æ•—: ç¼ºå°‘éŠ·è²¨æ—¥æœŸæˆ–ç¸½è¨ˆé‡‘é¡æ¬„ä½")
            return None, []
        
        # Ensure date column is datetime type
        if not pd.api.types.is_datetime64_dtype(order_df['éŠ·è²¨æ—¥æœŸ']):
            st.session_state.debug_info.append("å°‡éŠ·è²¨æ—¥æœŸè½‰æ›ç‚ºæ—¥æœŸæ™‚é–“æ ¼å¼ä»¥ç”Ÿæˆæ—¥éŠ·å”®åœ–")
            order_df['éŠ·è²¨æ—¥æœŸ'] = pd.to_datetime(order_df['éŠ·è²¨æ—¥æœŸ'], errors='coerce')
        
        # Drop rows where date conversion failed
        order_df = order_df.dropna(subset=['éŠ·è²¨æ—¥æœŸ'])
        if order_df.empty:
            st.warning("æ—¥æœŸè½‰æ›å¾Œæ²’æœ‰æœ‰æ•ˆçš„æ•¸æ“š")
            return None, []

        # Extract year-month and day for grouping
        order_df['å¹´æœˆ'] = order_df['éŠ·è²¨æ—¥æœŸ'].dt.strftime('%Y-%m')
        order_df['æ—¥'] = order_df['éŠ·è²¨æ—¥æœŸ'].dt.day
        
        # Get unique months for selection
        months = sorted(order_df['å¹´æœˆ'].unique(), reverse=True)
        
        if not months:
            st.warning("æ‰¾ä¸åˆ°æœ‰æ•ˆçš„æœˆä»½æ•¸æ“š")
            return None, []
        
        # If no month selected or selected month not in data, use most recent
        if selected_month is None or selected_month not in months:
            selected_month = months[0]  # Most recent month
        
        # Filter data for the selected month
        month_data = order_df[order_df['å¹´æœˆ'] == selected_month].copy()
        
        if month_data.empty:
            st.warning(f"æ‰€é¸æœˆä»½ {selected_month} æ²’æœ‰éŠ·å”®æ•¸æ“š")
            # Return an empty chart structure but still provide months for selection
            fig = go.Figure()
            fig.update_layout(title=f'{selected_month} æ—¥éŠ·å”®é‡‘é¡ - ç„¡æ•¸æ“š')
            return fig, months
        
        # Group by day and sum total amount
        daily_sales = month_data.groupby('æ—¥')['ç¸½è¨ˆé‡‘é¡'].sum().reset_index()
        
        # Sort by day
        daily_sales = daily_sales.sort_values('æ—¥')
        
        # Create figure
        fig = go.Figure()
        
        # Add bar chart
        fig.add_trace(go.Bar(
            x=daily_sales['æ—¥'],
            y=daily_sales['ç¸½è¨ˆé‡‘é¡'],
            text=daily_sales['ç¸½è¨ˆé‡‘é¡'].apply(lambda x: f"${x:,.2f}"),
            textposition='auto',
            marker_color='#0d6efd', # Bootstrap primary blue
            hovertemplate='%{x}æ—¥: $%{y:,.2f}<extra></extra>' # Use <extra></extra> to hide trace info
        ))
        
        # Calculate month total for annotation
        month_total = daily_sales['ç¸½è¨ˆé‡‘é¡'].sum()
        
        # Update layout with improved styling
        fig.update_layout(
            title=f'{selected_month} æ—¥éŠ·å”®é‡‘é¡',
            xaxis_title='æ—¥æœŸ',
            yaxis_title='éŠ·å”®é‡‘é¡ (å…ƒ)',
            xaxis=dict(
                tickmode='linear', # Ensure all days are shown
                tick0=1,
                dtick=1,
                tickangle=0 # Keep labels horizontal
            ),
            yaxis=dict(
                tickformat="$,.2f" # Format y-axis labels as currency
            ),
            height=500,
            margin=dict(l=40, r=40, t=60, b=40), # Adjusted top margin for annotation
            annotations=[
                dict(
                    x=0.5, 
                    y=1.08, # Position above the title
                    xref='paper', 
                    yref='paper',
                    text=f'æœˆç¸½è¨ˆ: ${month_total:,.2f}',
                    showarrow=False,
                    font=dict(size=16, color="#333")
                )
            ],
            hovermode='x unified' # Show hover info for the specific day
        )
        
        return fig, months
        
    except Exception as e:
        st.error(f"ç”Ÿæˆæ—¥éŠ·å”®åœ–è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        st.error(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
        # Return empty structure on error but provide available months if possible
        months = []
        if 'å¹´æœˆ' in order_df.columns:
             months = sorted(order_df['å¹´æœˆ'].unique(), reverse=True)
        return None, months

# --------------------------------------------------
# ğŸ“Š STREAMLIT UI ---------------------------------
# --------------------------------------------------

# Setup sidebar for all functionality
with st.sidebar:
    st.header("è³‡æ–™ä¸Šå‚³")
    
    # Data structure definitions (expandable)
    with st.expander("æ•¸æ“šæ¬„ä½å®šç¾©", expanded=False):
        # Order Level Tab
        st.markdown("#### è¨‚å–®å±¤ç´šæ¬„ä½å®šç¾©")
        for col in ORDER_LEVEL_COLUMNS:
            definition = COLUMN_DEFINITIONS.get(col, "ç„¡æè¿°")
            st.markdown(f"**{col}**: {definition}")
        
        st.markdown("---")
        
        # Product Level Tab
        st.markdown("#### ç”¢å“å±¤ç´šæ¬„ä½å®šç¾©")
        for col in PRODUCT_LEVEL_COLUMNS:
            definition = COLUMN_DEFINITIONS.get(col, "ç„¡æè¿°")
            st.markdown(f"**{col}**: {definition}")
        
        st.markdown("---")
        
        # BC Tab
        st.markdown("#### BC ç”¢å“æ¬„ä½å®šç¾©")
        for col in BC_COLUMNS:
            if col not in ORDER_LEVEL_COLUMNS and col not in PRODUCT_LEVEL_COLUMNS:
                definition = COLUMN_DEFINITIONS.get(col, "ç„¡æè¿°")
                st.markdown(f"**{col}**: {definition}")
    
    # Sales Data Section
    st.markdown("### éŠ·è²¨è³‡æ–™ä¸Šå‚³")
    with st.expander("å¿…è¦æ¬„ä½åˆ—è¡¨", expanded=False):
        st.markdown("""
        **è¨‚å–®å±¤ç´šæ¬„ä½ï¼š**
        éŠ·è²¨å–®è™Ÿ, è¨‚å–®å–®è™Ÿ, éŠ·è²¨æ—¥æœŸ, å®¢æˆ¶ä»£è™Ÿ, å®¢æˆ¶åç¨±, éƒ¨é–€ä»£è™Ÿ, éƒ¨é–€åç¨±, ç™¼ç¥¨è™Ÿç¢¼, 
        æœªç¨…å°è¨ˆ, ç‡Ÿæ¥­ç¨…, æŠ˜è®“é‡‘é¡, ç¨…å‰æŠ˜åƒ¹, ç¸½è¨ˆé‡‘é¡, å¯¦æ”¶ç¸½é¡, æˆæœ¬ç¸½é¡, æ¯›åˆ©, æ¯›åˆ©ç‡
        
        **ç”¢å“å±¤ç´šæ¬„ä½ï¼š**
        ç”¢å“ä»£è™Ÿ, ç”¢å“åç¨±, å€‰åˆ¥ä»£è™Ÿ, å€‰åˆ¥åç¨±, æ•¸é‡, å–®ä½, å–®åƒ¹, å°è¨ˆ, æˆæœ¬ç¸½å€¼, ç”¢å“æ¯›åˆ©, 
        ç”¢å“æ¯›åˆ©ç‡, éŠ·å”®å–®åƒ¹1, ç²¾æº–æˆæœ¬, ç²¾æº–æ¯›åˆ©, å–®ä½ç®¡éŠ·æˆæœ¬, ç®¡éŠ·æˆæœ¬åˆè¨ˆ, *éŠ·è²¨æ—¥æœŸ, 
        *å®¢æˆ¶ä»£è™Ÿ, *å®¢æˆ¶æ¢ä»¶, *éƒ¨é–€ä»£è™Ÿ, *æ¥­å‹™ä»£è™Ÿ, *æ¥­å‹™æ¢ä»¶
        """)
    
    sales_view = st.radio(
        "é¸æ“‡æ“ä½œ:",
        ["ä¸Šå‚³æ–°æª”æ¡ˆ", "ä½¿ç”¨å·²ä¸Šå‚³çš„æª”æ¡ˆ"],
        horizontal=True,
        key="sales_view_option",
        label_visibility="collapsed"
    )
    
    if sales_view == "ä¸Šå‚³æ–°æª”æ¡ˆ":
        sales_files = st.file_uploader(
            "ä¸Šå‚³éŠ·è²¨å–®æ¯›åˆ©åˆ†æè¡¨ (å¯å¤šé¸)",
            type=["xlsx", "xls"],
            accept_multiple_files=True,
            key="sales_uploader"
        )
        
        if sales_files:
            saved_paths = []
            with st.spinner("æ­£åœ¨å„²å­˜ä¸Šå‚³çš„éŠ·è²¨æª”æ¡ˆ..."):
                for file in sales_files:
                    saved_path = save_uploaded_file(file, UPLOADED_SALES_DIR)
                    if saved_path: 
                        saved_paths.append(saved_path)
                        
            if len(saved_paths) == len(sales_files):
                st.success(f"å·²æˆåŠŸå„²å­˜ {len(saved_paths)} å€‹éŠ·è²¨æª”æ¡ˆã€‚")
                st.session_state.sales_files_uploaded = True
                
                # Load and process data
                try:
                    dfs = []
                    filenames = []
                    for file in sales_files:
                        try:
                            df = pd.read_excel(file)
                            if not df.empty:
                                dfs.append(df)
                                filenames.append(file.name)
                                st.session_state.debug_info.append(f"æˆåŠŸè®€å– {file.name}ï¼Œæœ‰ {len(df)} è¡Œ")
                            else:
                                st.session_state.debug_info.append(f"è­¦å‘Š: {file.name} æª”æ¡ˆç‚ºç©º")
                        except Exception as e:
                            st.session_state.debug_info.append(f"è®€å– {file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    
                    if dfs:
                        st.session_state.sales_data = process_sales_data(dfs, filenames)
                        generate_monthly_sales_report(st.session_state.sales_data)
                    else:
                        st.session_state.debug_info.append("æ²’æœ‰æˆåŠŸè®€å–ä»»ä½•éŠ·å”®æª”æ¡ˆ")
                except Exception as e:
                    st.session_state.debug_info.append(f"è™•ç†éŠ·å”®æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            else: 
                st.warning("éƒ¨åˆ†éŠ·è²¨æª”æ¡ˆå„²å­˜å¤±æ•—ã€‚")
            
            # Show preview of the first file
            if sales_files:
                with st.expander("è³‡æ–™é è¦½ (é¦–å€‹æª”æ¡ˆ)"):
                    try:
                        preview_df = pd.read_excel(sales_files[0])
                        st.dataframe(preview_df.head())
                        
                        # Display sample values from important columns
                        if not preview_df.empty:
                            st.write("é‡è¦æ¬„ä½æ¨£æœ¬:")
                            for col in ['éŠ·è²¨æ—¥æœŸ', 'ç¸½è¨ˆé‡‘é¡', 'æ¯›åˆ©']:
                                if col in preview_df.columns:
                                    st.write(f"{col} å‰5ç­†: {preview_df[col].head().tolist()}")
                                    st.write(f"{col} æ•¸æ“šé¡å‹: {preview_df[col].dtype}")
                    except Exception as e:
                        st.error(f"é è¦½æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    else:
        available_sales_files = get_uploaded_files(UPLOADED_SALES_DIR)
        if not available_sales_files:
            st.info(f"'{UPLOADED_SALES_DIR}' ç›®éŒ„ä¸‹æ²’æœ‰æ‰¾åˆ°ä»»ä½•éŠ·è²¨æª”æ¡ˆã€‚")
            st.session_state.sales_files_uploaded = False
        else:
            selected_files = st.multiselect(
                "é¸æ“‡è¦ä½¿ç”¨çš„æª”æ¡ˆ:",
                options=available_sales_files
            )
            
            if selected_files:
                st.session_state.sales_files_uploaded = True
                
                # Load and process selected files
                try:
                    dfs = []
                    filenames = []
                    for file in selected_files:
                        file_path = os.path.join(UPLOADED_SALES_DIR, file)
                        try:
                            df = pd.read_excel(file_path)
                            if not df.empty:
                                dfs.append(df)
                                filenames.append(file)
                                st.session_state.debug_info.append(f"æˆåŠŸè®€å– {file}ï¼Œæœ‰ {len(df)} è¡Œ")
                            else:
                                st.session_state.debug_info.append(f"è­¦å‘Š: {file} æª”æ¡ˆç‚ºç©º")
                        except Exception as e:
                            st.session_state.debug_info.append(f"è®€å–æª”æ¡ˆ '{file}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    
                    if dfs:
                        st.session_state.sales_data = process_sales_data(dfs, filenames)
                        generate_monthly_sales_report(st.session_state.sales_data)
                    else:
                        st.session_state.debug_info.append("æ²’æœ‰æˆåŠŸè®€å–ä»»ä½•éŠ·å”®æª”æ¡ˆ")
                except Exception as e:
                    st.session_state.debug_info.append(f"è™•ç†éŠ·å”®æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                
                # Show preview of first selected file
                if selected_files:
                    with st.expander(f"è³‡æ–™é è¦½: {selected_files[0]}"):
                        try:
                            file_path = os.path.join(UPLOADED_SALES_DIR, selected_files[0])
                            preview_df = pd.read_excel(file_path)
                            st.dataframe(preview_df.head())
                            
                            # Display sample values from important columns
                            if not preview_df.empty:
                                st.write("é‡è¦æ¬„ä½æ¨£æœ¬:")
                                for col in ['éŠ·è²¨æ—¥æœŸ', 'ç¸½è¨ˆé‡‘é¡', 'æ¯›åˆ©']:
                                    if col in preview_df.columns:
                                        st.write(f"{col} å‰5ç­†: {preview_df[col].head().tolist()}")
                                        st.write(f"{col} æ•¸æ“šé¡å‹: {preview_df[col].dtype}")
                        except Exception as e:
                            st.error(f"é è¦½æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    st.markdown("---")
    
    # BC Inventory Data Section
    st.markdown("### BC ç”¢å“è³‡æ–™ä¸Šå‚³")
    with st.expander("å¿…è¦æ¬„ä½åˆ—è¡¨", expanded=False):
        st.markdown("""
        **å¿…è¦æ¬„ä½ï¼š**
        ç”¢å“ä»£è™Ÿ, ç”¢å“åç¨±, æ•¸é‡, å€‰åº«, å–®ä½, æˆæœ¬å–®åƒ¹, æˆæœ¬ç¸½åƒ¹, å®‰å…¨å­˜é‡, å» å•†ä»£è™Ÿ, 
        å» å•†ç°¡ç¨±, æœ€å¾Œå‡ºè²¨æ—¥, æœ€å¾Œé€²è²¨æ—¥, éŠ·å”®å–®åƒ¹1, éŠ·å”®å–®åƒ¹2, éŠ·å”®å–®åƒ¹3, éŠ·å”®å–®åƒ¹4, 
        æœ€ä½å”®åƒ¹, æ•¸é‡ç‚ºé›¶è‡ªå‹•ä¸‹æ¶, æŒçºŒä¸Šæ¶, åœæ­¢ä¸Šæ¶, å¤§é¡åç¨±, ä¸­é¡åç¨±, å°é¡åç¨±, 
        å‚™è¨», EAN13ç¢¼, CO128ç¢¼, å»ºè­°å”®åƒ¹, æ¯›åˆ©ç‡
        """)
    
    bc_view = st.radio(
        "é¸æ“‡æ“ä½œ:",
        ["ä¸Šå‚³æ–°æª”æ¡ˆ", "ä½¿ç”¨å·²ä¸Šå‚³çš„æª”æ¡ˆ"],
        horizontal=True,
        key="bc_view_option",
        label_visibility="collapsed"
    )
    
    if bc_view == "ä¸Šå‚³æ–°æª”æ¡ˆ":
        bc_file = st.file_uploader(
            "ä¸Šå‚³ BC ç”¢å“ SKU è¡¨",
            type=["xlsx", "xls"],
            key="bc_uploader"
        )
        
        if bc_file:
            with st.spinner("æ­£åœ¨å„²å­˜ä¸Šå‚³çš„ BC æª”æ¡ˆ..."):
                saved_bc_path = save_uploaded_file(bc_file, UPLOADED_BC_DIR)
            
            if saved_bc_path: 
                st.success(f"å·²æˆåŠŸå„²å­˜ BC æª”æ¡ˆ: {bc_file.name}")
                st.session_state.bc_file_uploaded = True
                
                # Load BC data
                try:
                    st.session_state.bc_data = pd.read_excel(bc_file)
                    st.session_state.debug_info.append(f"æˆåŠŸè®€å– BC æª”æ¡ˆ {bc_file.name}ï¼Œæœ‰ {len(st.session_state.bc_data)} è¡Œ")
                except Exception as e:
                    st.session_state.debug_info.append(f"è®€å– BC æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            else: 
                st.error("å„²å­˜ BC æª”æ¡ˆå¤±æ•—ã€‚")
            
            # Show preview
            with st.expander("è³‡æ–™é è¦½"):
                try:
                    preview_bc_df = pd.read_excel(bc_file)
                    st.dataframe(preview_bc_df.head())
                except Exception as e:
                    st.error(f"é è¦½ BC æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    else:
        available_bc_files = get_uploaded_files(UPLOADED_BC_DIR)
        if not available_bc_files:
            st.info(f"'{UPLOADED_BC_DIR}' ç›®éŒ„ä¸‹æ²’æœ‰æ‰¾åˆ°ä»»ä½• BC æª”æ¡ˆã€‚")
            st.session_state.bc_file_uploaded = False
        else:
            selected_bc_file = st.selectbox(
                "é¸æ“‡è¦ä½¿ç”¨çš„ BC æª”æ¡ˆ:",
                options=["è«‹é¸æ“‡..."] + available_bc_files,
                key="bc_file_selector"
            )
            
            if selected_bc_file and selected_bc_file != "è«‹é¸æ“‡...":
                st.session_state.bc_file_uploaded = True
                
                # Load selected BC file
                file_path = os.path.join(UPLOADED_BC_DIR, selected_bc_file)
                try:
                    st.session_state.bc_data = pd.read_excel(file_path)
                    st.session_state.debug_info.append(f"æˆåŠŸè®€å– BC æª”æ¡ˆ {selected_bc_file}ï¼Œæœ‰ {len(st.session_state.bc_data)} è¡Œ")
                except Exception as e:
                    st.session_state.debug_info.append(f"è®€å– BC æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                
                with st.expander(f"é è¦½: {selected_bc_file}"):
                    try:
                        preview_bc_df = pd.read_excel(file_path)
                        st.dataframe(preview_bc_df.head())
                    except Exception as e:
                        st.error(f"é è¦½ BC æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    st.markdown("---")
    
    # Generate button in sidebar
    if st.button("åˆ†æä¸¦æ›´æ–°å ±è¡¨", key="analyze_button", help="åˆ†ææ•¸æ“šä¸¦ç”Ÿæˆå ±è¡¨", use_container_width=True, type="primary"):
        st.session_state.analyze_triggered = True
        st.session_state.debug_info = []  # Reset debug info on new analysis
        st.session_state.debug_info.append("é–‹å§‹æ–°çš„åˆ†æ...")
        
        # Run analysis when button is clicked
        if st.session_state.sales_data is not None:
            generate_monthly_sales_report(st.session_state.sales_data)
    else:
        if 'analyze_triggered' not in st.session_state:
            st.session_state.analyze_triggered = False
    
    # Debug toggle
    show_debug = st.checkbox("é¡¯ç¤ºé™¤éŒ¯è³‡è¨Š", value=True)
    
    # Show debug information if enabled
    if show_debug and st.session_state.debug_info:
        with st.expander("é™¤éŒ¯è³‡è¨Š", expanded=True):
            for info in st.session_state.debug_info:
                st.text(info)

# Main content area
# Page title
st.markdown("<h1 style='text-align: center;'>æ•¸æ“šåœ–è¡¨ç”Ÿæˆå™¨</h1>", unsafe_allow_html=True)

# Display processing status directly without showing data structure information

# Display processing status
status_cols = st.columns(3)

with status_cols[0]:
    if st.session_state.processed_data['order_level'] is not None:
        st.success(f"å·²æˆåŠŸè™•ç† {len(st.session_state.processed_data['order_level'])} ç­†è¨‚å–®è³‡æ–™")
    
with status_cols[1]:
    if st.session_state.processed_data['product_level'] is not None:
        st.success(f"å·²æˆåŠŸè™•ç† {len(st.session_state.processed_data['product_level'])} ç­†ç”¢å“è³‡æ–™")
    
with status_cols[2]:
    if st.session_state.processed_data['monthly_summary'] is not None:
        st.success(f"å·²ç”Ÿæˆ {len(st.session_state.processed_data['monthly_summary'])} å€‹æœˆçš„éŠ·å”®å ±è¡¨")

# Add a big visual for the total ç¸½è¨ˆé‡‘é¡
if st.session_state.processed_data['order_level'] is not None:
    # Get the order-level data
    order_df = st.session_state.processed_data['order_level']
    
    # Debug the processed data
    st.session_state.debug_info.append(f"è¨ˆç®—ç¸½è¨ˆé‡‘é¡: è¨‚å–®è³‡æ–™æœ‰ {len(order_df)} è¡Œ")
    if 'éŠ·è²¨å–®è™Ÿ' in order_df.columns:
        unique_orders = order_df['éŠ·è²¨å–®è™Ÿ'].nunique()
        st.session_state.debug_info.append(f"è¨ˆç®—ç¸½è¨ˆé‡‘é¡: æœ‰ {unique_orders} å€‹å”¯ä¸€è¨‚å–®è™Ÿ")
    
    # Use ç¸½è¨ˆé‡‘é¡ (Total Amount) for the total sales revenue after taxes
    if 'ç¸½è¨ˆé‡‘é¡' in order_df.columns:
        # Ensure the value is numeric before formatting
        try:
            # Convert to numeric if not already
            if not pd.api.types.is_numeric_dtype(order_df['ç¸½è¨ˆé‡‘é¡']):
                order_df['ç¸½è¨ˆé‡‘é¡'] = pd.to_numeric(order_df['ç¸½è¨ˆé‡‘é¡'], errors='coerce')
            
            # Check for duplicates in order data
            if 'éŠ·è²¨å–®è™Ÿ' in order_df.columns:
                has_duplicates = order_df['éŠ·è²¨å–®è™Ÿ'].duplicated().any()
                if has_duplicates:
                    st.session_state.debug_info.append("è­¦å‘Š: ç™¼ç¾é‡è¤‡è¨‚å–®è™Ÿï¼Œä½¿ç”¨å»é‡è¤‡å¾Œçš„æ•¸æ“šè¨ˆç®—ç¸½é¡")
                    # Use unique orders to calculate totals
                    unique_order_df = order_df.drop_duplicates(subset=['éŠ·è²¨å–®è™Ÿ'])
                    total_amount = unique_order_df['ç¸½è¨ˆé‡‘é¡'].sum()
                    
                    # Log for diagnostics
                    st.session_state.debug_info.append(f"å»é‡è¤‡å‰ç¸½é¡: {order_df['ç¸½è¨ˆé‡‘é¡'].sum()}")
                    st.session_state.debug_info.append(f"å»é‡è¤‡å¾Œç¸½é¡: {total_amount}")
                else:
                    total_amount = order_df['ç¸½è¨ˆé‡‘é¡'].sum()
            else:
                total_amount = order_df['ç¸½è¨ˆé‡‘é¡'].sum()
            
            # Sample of order amounts for verification
            sample_amounts = order_df['ç¸½è¨ˆé‡‘é¡'].sample(min(5, len(order_df))).tolist()
            st.session_state.debug_info.append(f"è¨‚å–®é‡‘é¡æ¨£æœ¬: {sample_amounts}")
            
            # Format the number for display
            formatted_amount = f"${total_amount:,.2f}" if pd.notna(total_amount) else "$0.00"
            
            # Display the total in a large, prominent format
            st.markdown("### éŠ·å”®ç¸½è¦½")
            
            # Use a container with border and styling
            with st.container(border=True):
                # Create a big centered header for the total
                st.markdown(
                    f"""
                    <div style="text-align: center; padding: 2rem 0;">
                        <h2 style="font-size: 1.5rem; color: #555;">ç¸½éŠ·å”®é‡‘é¡ (ç¸½è¨ˆé‡‘é¡)</h2>
                        <h1 style="font-size: 3rem; font-weight: bold; color: #0f5132;">{formatted_amount}</h1>
                    </div>
                    """,
                    unsafe_allow_html=True
                )
        except Exception as e:
            st.error(f"è¨ˆç®—å¯¦æ”¶ç¸½é¡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            st.markdown("### éŠ·å”®ç¸½è¦½")
            with st.container(border=True):
                st.markdown(
                    """
                    <div style="text-align: center; padding: 2rem 0;">
                        <h2 style="font-size: 1.5rem; color: #555;">ç¸½éŠ·å”®é‡‘é¡ (å¯¦æ”¶ç¸½é¡)</h2>
                        <h1 style="font-size: 3rem; font-weight: bold; color: #0f5132;">$0.00</h1>
                        <p style="color: #dc3545;">ç„¡æ³•è¨ˆç®—ç¸½é¡ï¼Œè«‹æª¢æŸ¥æ•¸æ“šæ ¼å¼</p>
                    </div>
                    """,
                    unsafe_allow_html=True
                )
        
        # If we have profit data, show that as well
        if 'æ¯›åˆ©' in order_df.columns:
            try:
                # Ensure the value is numeric before formatting
                if not pd.api.types.is_numeric_dtype(order_df['æ¯›åˆ©']):
                    order_df['æ¯›åˆ©'] = pd.to_numeric(order_df['æ¯›åˆ©'], errors='coerce')
                
                # Use the same approach as for total_amount to handle potential duplicates
                if 'éŠ·è²¨å–®è™Ÿ' in order_df.columns and order_df['éŠ·è²¨å–®è™Ÿ'].duplicated().any():
                    # Use unique orders to calculate totals
                    unique_order_df = order_df.drop_duplicates(subset=['éŠ·è²¨å–®è™Ÿ'])
                    total_profit = unique_order_df['æ¯›åˆ©'].sum()
                    
                    # Log for diagnostics
                    st.session_state.debug_info.append(f"å»é‡è¤‡å‰æ¯›åˆ©ç¸½é¡: {order_df['æ¯›åˆ©'].sum()}")
                    st.session_state.debug_info.append(f"å»é‡è¤‡å¾Œæ¯›åˆ©ç¸½é¡: {total_profit}")
                else:
                    total_profit = order_df['æ¯›åˆ©'].sum()
                
                # Sample of profit amounts for verification
                sample_profits = order_df['æ¯›åˆ©'].sample(min(5, len(order_df))).tolist()
                st.session_state.debug_info.append(f"æ¯›åˆ©æ¨£æœ¬: {sample_profits}")
                
                # Format for display
                formatted_profit = f"${total_profit:,.2f}" if pd.notna(total_profit) else "$0.00"
                
                # Display the total gross profit
                col1, col2 = st.columns(2)
                
                with col1:
                    with st.container(border=True):
                        st.markdown(
                            f"""
                            <div style="text-align: center; padding: 1rem 0;">
                                <h3 style="font-size: 1.2rem; color: #555;">ç¸½æ¯›åˆ©</h3>
                                <h2 style="font-size: 2rem; font-weight: bold; color: #0d6efd;">{formatted_profit}</h2>
                            </div>
                            """,
                            unsafe_allow_html=True
                        )
                
                # For profit margin, use the average of the æ¯›åˆ©ç‡ column if available
                if 'æ¯›åˆ©ç‡' in order_df.columns:
                    try:
                        # Ensure the value is numeric before calculating
                        if not pd.api.types.is_numeric_dtype(order_df['æ¯›åˆ©ç‡']):
                            order_df['æ¯›åˆ©ç‡'] = pd.to_numeric(order_df['æ¯›åˆ©ç‡'], errors='coerce')
                            
                        # Use the same DataFrame as for other calculations
                        if 'éŠ·è²¨å–®è™Ÿ' in order_df.columns and order_df['éŠ·è²¨å–®è™Ÿ'].duplicated().any():
                            unique_order_df = order_df.drop_duplicates(subset=['éŠ·è²¨å–®è™Ÿ'])
                            avg_profit_margin = unique_order_df['æ¯›åˆ©ç‡'].dropna().mean()
                        else:
                            avg_profit_margin = order_df['æ¯›åˆ©ç‡'].dropna().mean()
                        
                        # Format for display
                        formatted_margin = f"{avg_profit_margin:.2f}%" if pd.notna(avg_profit_margin) else "0.00%"
                        
                        with col2:
                            with st.container(border=True):
                                st.markdown(
                                    f"""
                                    <div style="text-align: center; padding: 1rem 0;">
                                        <h3 style="font-size: 1.2rem; color: #555;">å¹³å‡æ¯›åˆ©ç‡</h3>
                                        <h2 style="font-size: 2rem; font-weight: bold; color: #d63384;">{formatted_margin}</h2>
                                    </div>
                                    """,
                                    unsafe_allow_html=True
                                )
                    except Exception as e:
                        st.error(f"è¨ˆç®—å¹³å‡æ¯›åˆ©ç‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        with col2:
                            with st.container(border=True):
                                st.markdown(
                                    """
                                    <div style="text-align: center; padding: 1rem 0;">
                                        <h3 style="font-size: 1.2rem; color: #555;">å¹³å‡æ¯›åˆ©ç‡</h3>
                                        <h2 style="font-size: 2rem; font-weight: bold; color: #d63384;">0.00%</h2>
                                        <p style="color: #dc3545;">ç„¡æ³•è¨ˆç®—æ¯›åˆ©ç‡ï¼Œè«‹æª¢æŸ¥æ•¸æ“šæ ¼å¼</p>
                                    </div>
                                    """,
                                    unsafe_allow_html=True
                                )
                else:
                    # Calculate profit margin from æ¯›åˆ© and ç¸½è¨ˆé‡‘é¡ if æ¯›åˆ©ç‡ column is not available
                    try:
                        profit_margin = (total_profit / total_amount * 100) if total_amount > 0 else 0
                        
                        # Format for display
                        formatted_margin = f"{profit_margin:.2f}%" if pd.notna(profit_margin) else "0.00%"
                        
                        with col2:
                            with st.container(border=True):
                                st.markdown(
                                    f"""
                                    <div style="text-align: center; padding: 1rem 0;">
                                        <h3 style="font-size: 1.2rem; color: #555;">è¨ˆç®—æ¯›åˆ©ç‡</h3>
                                        <h2 style="font-size: 2rem; font-weight: bold; color: #d63384;">{formatted_margin}</h2>
                                        <p style="color: #6c757d; font-size: 0.8rem;">æ³¨æ„: æ¯›åˆ©ç‡æ¬„ä½ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ¯›åˆ©/ç¸½è¨ˆé‡‘é¡è¨ˆç®—</p>
                                    </div>
                                    """,
                                    unsafe_allow_html=True
                                )
                    except Exception as e:
                        st.error(f"è¨ˆç®—æ¯›åˆ©ç‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        with col2:
                            with st.container(border=True):
                                st.markdown(
                                    """
                                    <div style="text-align: center; padding: 1rem 0;">
                                        <h3 style="font-size: 1.2rem; color: #555;">è¨ˆç®—æ¯›åˆ©ç‡</h3>
                                        <h2 style="font-size: 2rem; font-weight: bold; color: #d63384;">0.00%</h2>
                                        <p style="color: #dc3545;">ç„¡æ³•è¨ˆç®—æ¯›åˆ©ç‡ï¼Œè«‹æª¢æŸ¥æ•¸æ“šæ ¼å¼</p>
                                    </div>
                                    """,
                                    unsafe_allow_html=True
                                )
            except Exception as e:
                st.error(f"è¨ˆç®—æ¯›åˆ©ç›¸é—œæŒ‡æ¨™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                col1, col2 = st.columns(2)
                with col1:
                    with st.container(border=True):
                        st.markdown(
                            """
                            <div style="text-align: center; padding: 1rem 0;">
                                <h3 style="font-size: 1.2rem; color: #555;">ç¸½æ¯›åˆ©</h3>
                                <h2 style="font-size: 2rem; font-weight: bold; color: #0d6efd;">$0.00</h2>
                                <p style="color: #dc3545;">ç„¡æ³•è¨ˆç®—ç¸½æ¯›åˆ©ï¼Œè«‹æª¢æŸ¥æ•¸æ“šæ ¼å¼</p>
                            </div>
                            """,
                            unsafe_allow_html=True
                        )
    else:
        # If ç¸½è¨ˆé‡‘é¡ is not found, show an error
        st.error("ç„¡æ³•è¨ˆç®—ç¸½éŠ·å”®é‡‘é¡: æ‰¾ä¸åˆ°ç¸½è¨ˆé‡‘é¡æ¬„ä½")
        st.markdown("### éŠ·å”®ç¸½è¦½")
        
        with st.container(border=True):
            st.markdown(
                """
                <div style="text-align: center; padding: 2rem 0;">
                    <h2 style="font-size: 1.5rem; color: #555;">ç¸½éŠ·å”®é‡‘é¡</h2>
                    <h1 style="font-size: 3rem; font-weight: bold; color: #0f5132;">$0.00</h1>
                    <p style="color: #dc3545;">ç„¡æ³•æ‰¾åˆ°ç¸½è¨ˆé‡‘é¡æ¬„ä½ï¼Œè«‹ç¢ºèªæ•¸æ“šæ ¼å¼</p>
                </div>
                """,
                unsafe_allow_html=True
            )
        
        # If we have profit data, show that as well (using total_amount as the base)
        if 'æ¯›åˆ©' in order_df.columns:
            try:
                # Ensure the value is numeric before formatting
                if not pd.api.types.is_numeric_dtype(order_df['æ¯›åˆ©']):
                    order_df['æ¯›åˆ©'] = pd.to_numeric(order_df['æ¯›åˆ©'], errors='coerce')
                
                # Use the sum of the æ¯›åˆ© column for total gross profit
                total_profit = order_df['æ¯›åˆ©'].sum()
                
                # Format for display
                formatted_profit = f"${total_profit:,.2f}" if pd.notna(total_profit) else "$0.00"
                
                # Display the total gross profit
                col1, col2 = st.columns(2)
                
                with col1:
                    with st.container(border=True):
                        st.markdown(
                            f"""
                            <div style="text-align: center; padding: 1rem 0;">
                                <h3 style="font-size: 1.2rem; color: #555;">ç¸½æ¯›åˆ©</h3>
                                <h2 style="font-size: 2rem; font-weight: bold; color: #0d6efd;">{formatted_profit}</h2>
                            </div>
                            """,
                            unsafe_allow_html=True
                        )
                
                # For profit margin, use the average of the æ¯›åˆ©ç‡ column if available
                if 'æ¯›åˆ©ç‡' in order_df.columns:
                    try:
                        # Ensure the value is numeric before calculating
                        if not pd.api.types.is_numeric_dtype(order_df['æ¯›åˆ©ç‡']):
                            order_df['æ¯›åˆ©ç‡'] = pd.to_numeric(order_df['æ¯›åˆ©ç‡'], errors='coerce')
                            
                        # Calculate the average profit margin (remove NaN values)
                        avg_profit_margin = order_df['æ¯›åˆ©ç‡'].dropna().mean()
                        
                        # Format for display
                        formatted_margin = f"{avg_profit_margin:.2f}%" if pd.notna(avg_profit_margin) else "0.00%"
                        
                        with col2:
                            with st.container(border=True):
                                st.markdown(
                                    f"""
                                    <div style="text-align: center; padding: 1rem 0;">
                                        <h3 style="font-size: 1.2rem; color: #555;">å¹³å‡æ¯›åˆ©ç‡</h3>
                                        <h2 style="font-size: 2rem; font-weight: bold; color: #d63384;">{formatted_margin}</h2>
                                    </div>
                                    """,
                                    unsafe_allow_html=True
                                )
                    except Exception as e:
                        st.error(f"è¨ˆç®—å¹³å‡æ¯›åˆ©ç‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        with col2:
                            with st.container(border=True):
                                st.markdown(
                                    """
                                    <div style="text-align: center; padding: 1rem 0;">
                                        <h3 style="font-size: 1.2rem; color: #555;">å¹³å‡æ¯›åˆ©ç‡</h3>
                                        <h2 style="font-size: 2rem; font-weight: bold; color: #d63384;">0.00%</h2>
                                        <p style="color: #dc3545;">ç„¡æ³•è¨ˆç®—æ¯›åˆ©ç‡ï¼Œè«‹æª¢æŸ¥æ•¸æ“šæ ¼å¼</p>
                                    </div>
                                    """,
                                    unsafe_allow_html=True
                                )
                else:
                    # Calculate profit margin from æ¯›åˆ© and ç¸½è¨ˆé‡‘é¡ if æ¯›åˆ©ç‡ column is not available
                    try:
                        profit_margin = (total_profit / total_amount * 100) if total_amount > 0 else 0
                        
                        # Format for display
                        formatted_margin = f"{profit_margin:.2f}%" if pd.notna(profit_margin) else "0.00%"
                        
                        with col2:
                            with st.container(border=True):
                                st.markdown(
                                    f"""
                                    <div style="text-align: center; padding: 1rem 0;">
                                        <h3 style="font-size: 1.2rem; color: #555;">è¨ˆç®—æ¯›åˆ©ç‡</h3>
                                        <h2 style="font-size: 2rem; font-weight: bold; color: #d63384;">{formatted_margin}</h2>
                                        <p style="color: #6c757d; font-size: 0.8rem;">æ³¨æ„: æ¯›åˆ©ç‡æ¬„ä½ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ¯›åˆ©/ç¸½è¨ˆé‡‘é¡è¨ˆç®—</p>
                                    </div>
                                    """,
                                    unsafe_allow_html=True
                                )
                    except Exception as e:
                        st.error(f"è¨ˆç®—æ¯›åˆ©ç‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        with col2:
                            with st.container(border=True):
                                st.markdown(
                                    """
                                    <div style="text-align: center; padding: 1rem 0;">
                                        <h3 style="font-size: 1.2rem; color: #555;">è¨ˆç®—æ¯›åˆ©ç‡</h3>
                                        <h2 style="font-size: 2rem; font-weight: bold; color: #d63384;">0.00%</h2>
                                        <p style="color: #dc3545;">ç„¡æ³•è¨ˆç®—æ¯›åˆ©ç‡ï¼Œè«‹æª¢æŸ¥æ•¸æ“šæ ¼å¼</p>
                                    </div>
                                    """,
                                    unsafe_allow_html=True
                                )
            except Exception as e:
                st.error(f"è¨ˆç®—æ¯›åˆ©ç›¸é—œæŒ‡æ¨™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                col1, col2 = st.columns(2)
                with col1:
                    with st.container(border=True):
                        st.markdown(
                            """
                            <div style="text-align: center; padding: 1rem 0;">
                                <h3 style="font-size: 1.2rem; color: #555;">ç¸½æ¯›åˆ©</h3>
                                <h2 style="font-size: 2rem; font-weight: bold; color: #0d6efd;">$0.00</h2>
                                <p style="color: #dc3545;">ç„¡æ³•è¨ˆç®—ç¸½æ¯›åˆ©ï¼Œè«‹æª¢æŸ¥æ•¸æ“šæ ¼å¼</p>
                            </div>
                            """,
                            unsafe_allow_html=True
                        )

# Top Products Section
st.markdown("### ç†±é–€ç”¢å“éŠ·å”®")

# Check if we have any sales data processed
if st.session_state.sales_data is not None:
    # Try using the original combined data first
    st.session_state.debug_info.append("å˜—è©¦å¾åŸå§‹éŠ·å”®æ•¸æ“šç”Ÿæˆç†±é–€ç”¢å“æ¸…å–®")
    top_products = get_top_products(st.session_state.sales_data, n=50)
    
    # If that didn't work, try using the product_level data if available
    if (top_products is None or top_products.empty) and st.session_state.processed_data['product_level'] is not None:
        st.session_state.debug_info.append("å¾åŸå§‹æ•¸æ“šç”Ÿæˆå¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨è™•ç†å¾Œçš„ç”¢å“å±¤ç´šæ•¸æ“š")
        top_products = get_top_products(st.session_state.processed_data['product_level'], n=50)
    
    if top_products is not None and not top_products.empty:
        st.success(f"æˆåŠŸæ‰¾åˆ° {len(top_products)} é …ç†±é–€ç”¢å“")
        
        # Create container with fixed height and scrolling for table
        with st.container(border=True):
            # Display all columns with nicer column headers
            column_rename = {
                "æ’å": "æ’å",
                "ç”¢å“ä»£è™Ÿ": "ç”¢å“ä»£è™Ÿ",
                "ç”¢å“åç¨±": "ç”¢å“åç¨±",
                "æ•¸é‡": "éŠ·å”®æ•¸é‡",
                "å°è¨ˆ": "éŠ·å”®é‡‘é¡(å…ƒ)"
            }
            display_df = top_products.rename(columns=column_rename)
            
            # Set height of table to show all rows but be scrollable
            st.dataframe(display_df, hide_index=True, height=600, use_container_width=True)
    else:
        st.warning("ç„¡æ³•ç²å–ç†±é–€ç”¢å“æ¸…å–®")
        with st.expander("å¯èƒ½çš„åŸå› åŠè§£æ±ºæ–¹æ³•"):
            st.markdown("""
            **å¯èƒ½çš„åŸå› :**
            1. ç”¢å“å±¤ç´šçš„æ•¸æ“šç¼ºå°‘å¿…è¦æ¬„ä½ (ç”¢å“ä»£è™Ÿ, ç”¢å“åç¨±, æ•¸é‡, å°è¨ˆ)
            2. æ•¸æ“šçµæ§‹èˆ‡é æœŸä¸ç¬¦
            3. æ•¸å€¼æ¬„ä½çš„æ ¼å¼ä¸æ­£ç¢º
            
            **è§£æ±ºæ–¹æ³•:**
            1. ç¢ºèªä¸Šå‚³çš„æª”æ¡ˆåŒ…å«æ‰€æœ‰å¿…è¦çš„ç”¢å“è³‡è¨Šæ¬„ä½
            2. æŸ¥çœ‹é™¤éŒ¯è³‡è¨Šä»¥å–å¾—æ›´è©³ç´°çš„éŒ¯èª¤è¨Šæ¯
            3. ç¢ºèªç”¢å“æ•¸æ“šä¸­çš„æ•¸é‡å’Œé‡‘é¡æ¬„ä½æ˜¯æ•¸å€¼æ ¼å¼
            """)
else:
    st.info("è«‹å…ˆä¸Šå‚³ä¸¦è™•ç†éŠ·å”®æ•¸æ“šï¼Œä»¥æŸ¥çœ‹ç†±é–€ç”¢å“æ¸…å–®ã€‚")

# Monthly Comparison Section (only show when there are multiple months of data)
if st.session_state.processed_data['monthly_summary'] is not None:
    monthly_summary = st.session_state.processed_data['monthly_summary']
    
    # Check if we have multiple months to compare
    if len(monthly_summary) > 1:
        st.markdown("---")
        st.markdown("### æœˆåº¦éŠ·å”®æ¯”è¼ƒ")
        
        # Create and display the monthly comparison table
        comparison_table = create_monthly_comparison_table(monthly_summary)
        if comparison_table is not None and not comparison_table.empty:
            st.subheader("æœˆåº¦éŠ·å”®æ•¸æ“šæ¯”è¼ƒ")
            with st.container(border=True):
                st.dataframe(comparison_table, hide_index=True, use_container_width=True)
        
        # Daily sales comparison chart
        if st.session_state.processed_data['order_level'] is not None:
            order_df = st.session_state.processed_data['order_level']
            daily_comparison_chart = create_monthly_comparison_chart(order_df)
            
            if daily_comparison_chart is not None:
                st.subheader("æ¯æ—¥éŠ·å”®é‡‘é¡æ¯”è¼ƒ")
                with st.container(border=True):
                    st.plotly_chart(daily_comparison_chart, use_container_width=True)

# Version 1.1: Add Daily Sales Chart Section to UI
# --------------------------------------------------
st.markdown("---")
st.markdown("### æ—¥éŠ·å”®åœ–è¡¨")

# Check if order level data exists
if st.session_state.processed_data['order_level'] is not None:
    order_df_daily = st.session_state.processed_data['order_level'].copy() # Use a copy
    
    # Initial chart generation (uses most recent month by default)
    daily_chart, available_months = create_daily_sales_chart(order_df_daily)
    
    # Check if we got months back (even if chart is initially empty)
    if available_months:
        # Add a month selector using the available months
        selected_month = st.selectbox(
            "é¸æ“‡æœˆä»½:",
            options=available_months,
            index=0, # Default to the most recent month
            key="daily_sales_month_selector"
        )
        
        # Regenerate chart with the selected month
        # Pass the selected month to the function
        daily_chart, _ = create_daily_sales_chart(order_df_daily, selected_month) 
        
        # Display the chart if it was generated successfully
        if daily_chart is not None:
            with st.container(border=True):
                st.plotly_chart(daily_chart, use_container_width=True)
                
                # Add some analysis text below the chart
                if selected_month:
                    # --- FIX: Ensure 'å¹´æœˆ' column exists before filtering ---
                    if 'å¹´æœˆ' not in order_df_daily.columns:
                        st.session_state.debug_info.append("é‡æ–°æ·»åŠ  'å¹´æœˆ' æ¬„ä½ä»¥é€²è¡Œåˆ†æ")
                        if 'éŠ·è²¨æ—¥æœŸ' in order_df_daily.columns and pd.api.types.is_datetime64_dtype(order_df_daily['éŠ·è²¨æ—¥æœŸ']):
                             order_df_daily['å¹´æœˆ'] = order_df_daily['éŠ·è²¨æ—¥æœŸ'].dt.strftime('%Y-%m')
                        else:
                            st.warning("ç„¡æ³•é‡æ–°ç”Ÿæˆ 'å¹´æœˆ' æ¬„ä½é€²è¡Œåˆ†æï¼Œç¼ºå°‘æœ‰æ•ˆçš„ 'éŠ·è²¨æ—¥æœŸ'")
                            # Skip analysis if column cannot be created
                            st.stop() 
                    # --- End FIX ---

                    # Filter data again for the selected month for analysis
                    month_data_analysis = order_df_daily[order_df_daily['å¹´æœˆ'] == selected_month]
                    
                    if not month_data_analysis.empty:
                        # Group by day for analysis metrics
                        # --- FIX: Ensure 'æ—¥' column exists before grouping ---
                        if 'æ—¥' not in month_data_analysis.columns:
                             st.session_state.debug_info.append("é‡æ–°æ·»åŠ  'æ—¥' æ¬„ä½ä»¥é€²è¡Œåˆ†æ")
                             if 'éŠ·è²¨æ—¥æœŸ' in month_data_analysis.columns and pd.api.types.is_datetime64_dtype(month_data_analysis['éŠ·è²¨æ—¥æœŸ']):
                                 month_data_analysis['æ—¥'] = month_data_analysis['éŠ·è²¨æ—¥æœŸ'].dt.day
                             else:
                                 st.warning("ç„¡æ³•é‡æ–°ç”Ÿæˆ 'æ—¥' æ¬„ä½é€²è¡Œåˆ†æï¼Œç¼ºå°‘æœ‰æ•ˆçš„ 'éŠ·è²¨æ—¥æœŸ'")
                                 st.stop() # Skip analysis
                        # --- End FIX ---
                        daily_sales_analysis = month_data_analysis.groupby(month_data_analysis['éŠ·è²¨æ—¥æœŸ'].dt.day)['ç¸½è¨ˆé‡‘é¡'].sum()
                        
                        if not daily_sales_analysis.empty:
                            max_day = daily_sales_analysis.idxmax()
                            max_sales = daily_sales_analysis.max()
                            avg_sales = daily_sales_analysis.mean()
                            total_sales_month = daily_sales_analysis.sum() # Recalculate for consistency
                            
                            # Display metrics in columns
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric(
                                    label="æœ¬æœˆç¸½éŠ·å”®é¡", 
                                    value=f"${total_sales_month:,.2f}"
                                )
                            with col2:
                                st.metric(
                                    label="æœ€é«˜éŠ·å”®æ—¥", 
                                    value=f"{max_day}æ—¥", 
                                    delta=f"${max_sales:,.2f}",
                                    delta_color="normal" # Use normal color for amount
                                )
                            with col3:
                                st.metric(
                                    label="å¹³å‡æ—¥éŠ·å”®é¡", 
                                    value=f"${avg_sales:,.2f}"
                                )
                        else:
                            st.info(f"{selected_month} æœˆä»½æ²’æœ‰æœ‰æ•ˆçš„æ—¥éŠ·å”®æ•¸æ“šå¯ä¾›åˆ†æã€‚")
                    else:
                         st.info(f"{selected_month} æœˆä»½æ²’æœ‰æ•¸æ“šå¯ä¾›åˆ†æã€‚")
        else:
            # Handle case where chart generation failed after selection
             st.warning(f"ç„¡æ³•ç‚ºæ‰€é¸æœˆä»½ {selected_month} ç”Ÿæˆæ—¥éŠ·å”®åœ–è¡¨ã€‚")

    else:
        # Handle case where no months were found initially
        st.warning("ç„¡æ³•ç”Ÿæˆæ—¥éŠ·å”®åœ–è¡¨ï¼Œæ‰¾ä¸åˆ°æœ‰æ•ˆçš„æœˆä»½æ•¸æ“šã€‚")
else:
    # Message if no order data is loaded yet
    st.info("è«‹å…ˆä¸Šå‚³ä¸¦è™•ç†éŠ·å”®æ•¸æ“šï¼Œä»¥æŸ¥çœ‹æ—¥éŠ·å”®åœ–è¡¨ã€‚")
