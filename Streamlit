import streamlit as st
import pandas as pd
import numpy as np
import os
import shutil
from io import BytesIO
from datetime import datetime

# Configure page and set up data directories
st.set_page_config(page_title="FAFA ERP æ™ºèƒ½éŠ·å”®åŠ©ç†", layout="wide")

# Initialize session state for section navigation if it doesn't exist
if 'current_section' not in st.session_state:
    st.session_state.current_section = "ç”Ÿæˆå ±è¡¨"

# Initialize session state for report generation if it doesn't exist
if 'report_generated' not in st.session_state:
    st.session_state.report_generated = False
if 'selected_months_display' not in st.session_state:
    st.session_state.selected_months_display = []
# Initialize dataframes in session state
if 'sales_df' not in st.session_state: st.session_state.sales_df = pd.DataFrame()
if 'summary_df' not in st.session_state: st.session_state.summary_df = pd.DataFrame()
if 'order_summary' not in st.session_state: st.session_state.order_summary = pd.DataFrame()
if 'bc_df' not in st.session_state: st.session_state.bc_df = pd.DataFrame()
if 'merged_df' not in st.session_state: st.session_state.merged_df = pd.DataFrame()
if 'customer_summary' not in st.session_state: st.session_state.customer_summary = pd.DataFrame() # Initialize customer summary
if 'selected_vendor' not in st.session_state: st.session_state.selected_vendor = "è«‹é¸æ“‡..." # Initialize vendor selection
if 'bc_load_error' not in st.session_state: st.session_state.bc_load_error = None # Initialize error state
if 'merge_possible' not in st.session_state: st.session_state.merge_possible = False # Initialize merge state

# No longer needed sections = ["ç”Ÿæˆå ±è¡¨", "è¨‚å–®èª¿é–±", "ç”¢å“åˆ†æ", "BC è³‡æ–™æ¯”å°", "å» å•†ç”¢å“æŸ¥è©¢"]

# Removed horizontal navigation bar

st.title(f"ğŸ“Š FAFA - ERP æ™ºèƒ½éŠ·å”®åŠ©ç† - {st.session_state.current_section}")

# Create data storage directories if they don't exist
DATA_DIR = "uploaded_data"
SALES_DIR = os.path.join(DATA_DIR, "sales_files")
BC_DIR = os.path.join(DATA_DIR, "bc_files")

os.makedirs(SALES_DIR, exist_ok=True)
os.makedirs(BC_DIR, exist_ok=True)

# =================== UTILITY FUNCTIONS ===================

def save_uploaded_file(uploaded_file, directory):
    """Save an uploaded file to the specified directory and return the file path
    If a file with the same name already exists, it will be overwritten"""
    filename = uploaded_file.name
    # Use the original filename without timestamp to avoid duplicates
    file_path = os.path.join(directory, filename)
    
    # Save the file, overwriting if it already exists
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    return file_path

def get_file_list(directory, file_type=None):
    """Get list of files from the specified directory"""
    if not os.path.exists(directory):
        return []
    
    files = []
    for filename in os.listdir(directory):
        if file_type and not filename.endswith(file_type):
            continue
        file_path = os.path.join(directory, filename)
        if os.path.isfile(file_path):
            # Get file modification time for sorting
            mod_time = os.path.getmtime(file_path)
            # Store filename and path (original name is now the same as filename)
            files.append({
                "name": filename, 
                "path": file_path, 
                "full_name": filename,
                "mod_time": mod_time,
                "mod_time_str": datetime.fromtimestamp(mod_time).strftime("%Y/%m/%d %H:%M")
            })
    
    # Sort by modification time (newest first)
    files.sort(key=lambda x: x["mod_time"], reverse=True)
    return files

def delete_file(file_path):
    """Delete a file from the filesystem"""
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
            return True
        return False
    except Exception as e:
        st.error(f"åˆªé™¤æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

def load_saved_excel_file(file_path):
    """Load a saved Excel file from disk"""
    return pd.read_excel(file_path)

def clean_numeric_columns(df, columns):
    for col in columns:
        if col in df.columns: # Check if column exists before cleaning
            df[col] = df[col].astype(str).str.replace(',', '', regex=False)
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).round(0).astype(int)
    return df

@st.cache_data # Cache loaded sales data based on file sources, selected months, and mapping
def load_sales_data(_files, selected_months, is_path=False, column_mapping=None):
    # Convert file objects to hashable representation (paths or names) for caching
    # Note: Streamlit's caching might handle UploadedFile objects directly, but being explicit is safer.
    # We use _files as the argument name to avoid modifying the original list inside the cached function.
    files = []
    if not is_path:
        # Use file names and sizes as a proxy for identity if they are UploadedFile objects
        files = [(f.name, f.size) for f in _files]
    else:
        # Use file paths if they are strings
        files = tuple(_files) # Convert list to tuple to make it hashable

    df_list = []
    
    for file_source in files:
        # Handle both file objects and file paths
        if is_path:
            file_name = os.path.basename(file_source)
            file_path = file_source
        else: # UploadedFile object
            file_name = file_source.name
            file_path = file_source # Pass the object itself to pd.read_excel
            
        # Check if the filename contains any of the selected months
        file_matches_month = False
        for month in selected_months:
            if month in file_name:
                file_matches_month = True
                break
        
        if file_matches_month:
            try:
                # pd.read_excel can handle both paths and file-like objects
                df = pd.read_excel(file_path)
                
                # Apply column mapping if provided
                if column_mapping and isinstance(column_mapping, dict):
                    df = df.rename(columns=column_mapping)
                
                # Process the hierarchical structure before adding to list
                order_columns = ['éŠ·è²¨å–®è™Ÿ', 'è¨‚å–®å–®è™Ÿ', 'éŠ·è²¨æ—¥æœŸ', 'å®¢æˆ¶ä»£è™Ÿ', 'å®¢æˆ¶åç¨±', 
                                'éƒ¨é–€ä»£è™Ÿ', 'éƒ¨é–€åç¨±', 'ç™¼ç¥¨è™Ÿç¢¼', 'æœªç¨…å°è¨ˆ', 'ç‡Ÿæ¥­ç¨…', 
                                'æŠ˜è®“é‡‘é¡', 'ç¨…å‰æŠ˜åƒ¹', 'ç¸½è¨ˆé‡‘é¡', 'å¯¦æ”¶ç¸½é¡', 'æˆæœ¬ç¸½é¡', 'æ¯›åˆ©', 'æ¯›åˆ©ç‡']
                
                # Add missing order columns with None
                for col in order_columns:
                    if col not in df.columns:
                        df[col] = None
                
                # Forward fill the order information
                df[order_columns] = df[order_columns].fillna(method='ffill')
                
                # Add order identification column
                if 'éŠ·è²¨å–®è™Ÿ' in df.columns:
                    df['order_id'] = df['éŠ·è²¨å–®è™Ÿ'].astype(str)
                
                df_list.append(df)
            except Exception as e:
                st.error(f"ç„¡æ³•è®€å–æª”æ¡ˆ {file_name}: {e}")
                
    if df_list:
        combined = pd.concat(df_list, ignore_index=True)
        
        # Process numeric columns (ensure they exist first)
        numeric_cols = ['æ•¸é‡', 'å°è¨ˆ', 'ç²¾æº–æˆæœ¬', 'ç²¾æº–æ¯›åˆ©']
        combined = clean_numeric_columns(combined, numeric_cols)
        
        # Also ensure numeric processing for order-level columns
        order_numeric_cols = ['æœªç¨…å°è¨ˆ', 'ç‡Ÿæ¥­ç¨…', 'æŠ˜è®“é‡‘é¡', 'ç¨…å‰æŠ˜åƒ¹', 
                             'ç¸½è¨ˆé‡‘é¡', 'å¯¦æ”¶ç¸½é¡', 'æˆæœ¬ç¸½é¡', 'æ¯›åˆ©']
        combined = clean_numeric_columns(combined, order_numeric_cols)
        
        return combined
    return pd.DataFrame()

@st.cache_data # Cache monthly summary generation
def generate_monthly_summary(_df):
    # Use _df to avoid modifying the original df inside the cached function
    df = _df.copy()
    if df.empty or 'ç”¢å“ä»£è™Ÿ' not in df.columns:
        return pd.DataFrame()
        
    agg_dict = {
        'æ•¸é‡': 'sum',
        'å°è¨ˆ': 'sum',
        'ç²¾æº–æˆæœ¬': 'sum',
        'ç²¾æº–æ¯›åˆ©': 'sum',
        'ç”¢å“åç¨±': 'first',
        'å€‰åˆ¥ä»£è™Ÿ': 'first',
        'å€‰åˆ¥åç¨±': 'first'
    }
    # Only include columns that actually exist in the dataframe
    valid_agg_dict = {k: v for k, v in agg_dict.items() if k in df.columns}

    if not valid_agg_dict: # Need at least one column to aggregate
        return pd.DataFrame()

    grouped = df.groupby('ç”¢å“ä»£è™Ÿ').agg(valid_agg_dict).reset_index()
    
    # Ensure essential columns exist after aggregation, fill with 0 if not
    for col in ['æ•¸é‡', 'å°è¨ˆ', 'ç²¾æº–æˆæœ¬', 'ç²¾æº–æ¯›åˆ©']:
        if col not in grouped.columns:
            grouped[col] = 0

    return grouped.sort_values(by='å°è¨ˆ', ascending=False)

@st.cache_data # Cache order summary generation
def generate_order_summary(_df):
    """Generate a summary table at the order level"""
    # Use _df to avoid modifying the original df inside the cached function
    df = _df.copy()
    if df.empty:
        return pd.DataFrame()
        
    required_cols = ['éŠ·è²¨å–®è™Ÿ'] # Minimal requirement
    if not all(col in df.columns for col in required_cols):
        return pd.DataFrame()
    
    # Get unique order rows based on the first occurrence of each order number
    order_df = df.drop_duplicates(subset=['éŠ·è²¨å–®è™Ÿ']).copy()
    
    # Define potential order-level columns
    order_cols = ['éŠ·è²¨å–®è™Ÿ', 'è¨‚å–®å–®è™Ÿ', 'éŠ·è²¨æ—¥æœŸ', 'å®¢æˆ¶ä»£è™Ÿ', 'å®¢æˆ¶åç¨±', 
                'éƒ¨é–€ä»£è™Ÿ', 'éƒ¨é–€åç¨±', 'ç™¼ç¥¨è™Ÿç¢¼', 'æœªç¨…å°è¨ˆ', 'ç‡Ÿæ¥­ç¨…', 
                'æŠ˜è®“é‡‘é¡', 'ç¨…å‰æŠ˜åƒ¹', 'ç¸½è¨ˆé‡‘é¡', 'å¯¦æ”¶ç¸½é¡', 'æˆæœ¬ç¸½é¡', 'æ¯›åˆ©', 'æ¯›åˆ©ç‡']
    
    # Filter to only include columns that actually exist in the order_df
    existing_cols = [col for col in order_cols if col in order_df.columns]
    
    if not existing_cols:
        return pd.DataFrame()
    
    # Start with just the existing order-level columns
    order_summary = order_df[existing_cols].copy()
    
    # Add product count for each order
    if 'éŠ·è²¨å–®è™Ÿ' in df.columns:
        product_counts = df.groupby('éŠ·è²¨å–®è™Ÿ').size().reset_index(name='ç”¢å“æ•¸é‡')
        order_summary = pd.merge(order_summary, product_counts, on='éŠ·è²¨å–®è™Ÿ', how='left')
    
    # Calculate order total from product subtotals ONLY if an order total column doesn't exist
    order_total_cols = ['ç¸½è¨ˆé‡‘é¡', 'å¯¦æ”¶ç¸½é¡', 'æœªç¨…å°è¨ˆ']
    existing_total_col = next((col for col in order_total_cols if col in order_summary.columns), None)

    if not existing_total_col and 'å°è¨ˆ' in df.columns and 'éŠ·è²¨å–®è™Ÿ' in df.columns:
        product_totals = df.groupby('éŠ·è²¨å–®è™Ÿ')['å°è¨ˆ'].sum().reset_index(name='è¨‚å–®ç”¢å“ç¸½é¡')
        order_summary = pd.merge(order_summary, product_totals, on='éŠ·è²¨å–®è™Ÿ', how='left')
    
    # Sort by date (newest first) and then by order number if date exists
    if 'éŠ·è²¨æ—¥æœŸ' in order_summary.columns:
        order_summary = order_summary.sort_values(by=['éŠ·è²¨æ—¥æœŸ', 'éŠ·è²¨å–®è™Ÿ'], ascending=[False, True])
    
    return order_summary

@st.cache_data # Cache customer summary generation
def generate_customer_summary(_order_summary_df):
    """Generate a summary of top customers from the order summary"""
    # Use _order_summary_df to avoid modifying the original df inside the cached function
    order_summary_df = _order_summary_df.copy()
    if order_summary_df.empty:
        return pd.DataFrame()
        
    # Determine which amount column to use from the order summary
    amount_col = None
    for col in ['ç¸½è¨ˆé‡‘é¡', 'å¯¦æ”¶ç¸½é¡', 'æœªç¨…å°è¨ˆ', 'è¨‚å–®ç”¢å“ç¸½é¡']: # Check order summary columns
        if col in order_summary_df.columns:
            amount_col = col
            break
    
    if 'å®¢æˆ¶åç¨±' not in order_summary_df.columns or amount_col is None:
        return pd.DataFrame()
    
    # Group by customer name and sum the total amount and count unique orders
    customer_summary = order_summary_df.groupby('å®¢æˆ¶åç¨±').agg(
        æ¶ˆè²»ç¸½é¡=(amount_col, 'sum'),
        è¨‚å–®æ•¸é‡=('éŠ·è²¨å–®è™Ÿ', 'nunique') # Count unique order numbers
    ).reset_index()
    
    # Calculate average order value, handle division by zero
    customer_summary['å¹³å‡å–®ç­†é‡‘é¡'] = customer_summary.apply(
        lambda row: row['æ¶ˆè²»ç¸½é¡'] / row['è¨‚å–®æ•¸é‡'] if row['è¨‚å–®æ•¸é‡'] > 0 else 0,
        axis=1
    )
    
    # Sort by total amount spent, descending
    customer_summary = customer_summary.sort_values(by='æ¶ˆè²»ç¸½é¡', ascending=False)
    
    return customer_summary

@st.cache_data # Cache dead stock identification
def identify_dead_stock(_summary_df, _bc_df):
    """Identifies products with inventory but no sales in the selected period."""
    summary_df = _summary_df.copy()
    bc_df = _bc_df.copy()

    if bc_df.empty or 'ç”¢å“ä»£è™Ÿ' not in bc_df.columns or 'æ•¸é‡' not in bc_df.columns:
        st.warning("BC data is missing required columns ('ç”¢å“ä»£è™Ÿ', 'æ•¸é‡') for dead stock analysis.")
        return pd.DataFrame()
    if summary_df.empty or 'ç”¢å“ä»£è™Ÿ' not in summary_df.columns:
         # If summary is empty, all BC stock is potentially "dead" for the period, but let's be cautious
         st.info("Sales summary is empty. Dead stock analysis might show all inventoried items.")
         # Create a dummy summary to allow the merge logic to proceed, marking all as having 0 sales
         summary_df = pd.DataFrame({'ç”¢å“ä»£è™Ÿ': [], 'éŠ·å”®æ•¸é‡': []}) # Use 'éŠ·å”®æ•¸é‡' as the merge function expects
    elif 'æ•¸é‡' in summary_df.columns:
         # Rename sales quantity if it exists in the summary
         summary_df.rename(columns={'æ•¸é‡': 'éŠ·å”®æ•¸é‡'}, inplace=True)
    else:
         # If 'æ•¸é‡' column doesn't exist in summary, add it as 0
         summary_df['éŠ·å”®æ•¸é‡'] = 0


    # Prepare BC data: select relevant columns and standardize product code
    bc_subset = bc_df[['ç”¢å“ä»£è™Ÿ', 'æ•¸é‡']].copy()
    bc_subset.rename(columns={'æ•¸é‡': 'åº«å­˜æ•¸é‡'}, inplace=True)
    bc_subset['ç”¢å“ä»£è™Ÿ'] = bc_subset['ç”¢å“ä»£è™Ÿ'].astype(str).str.strip()
    bc_subset['åº«å­˜æ•¸é‡'] = pd.to_numeric(bc_subset['åº«å­˜æ•¸é‡'], errors='coerce').fillna(0).astype(int)

    # Prepare Sales data: select relevant columns and standardize product code
    sales_subset = summary_df[['ç”¢å“ä»£è™Ÿ', 'éŠ·å”®æ•¸é‡']].copy()
    sales_subset['ç”¢å“ä»£è™Ÿ'] = sales_subset['ç”¢å“ä»£è™Ÿ'].astype(str).str.strip()
    sales_subset['éŠ·å”®æ•¸é‡'] = pd.to_numeric(sales_subset['éŠ·å”®æ•¸é‡'], errors='coerce').fillna(0).astype(int)


    # Merge BC data with sales data (right merge to keep all BC items)
    merged_for_dead_stock = pd.merge(sales_subset, bc_subset, on='ç”¢å“ä»£è™Ÿ', how='right')

    # Identify dead stock: Inventory > 0 and Sales == 0 (or NaN if no match in sales)
    dead_stock_df = merged_for_dead_stock[
        (merged_for_dead_stock['åº«å­˜æ•¸é‡'] > 0) &
        (merged_for_dead_stock['éŠ·å”®æ•¸é‡'].fillna(0) == 0)
    ].copy()

    # Add product names from original BC data if available
    if 'ç”¢å“åç¨±' in bc_df.columns:
        bc_names = bc_df[['ç”¢å“ä»£è™Ÿ', 'ç”¢å“åç¨±']].copy()
        bc_names['ç”¢å“ä»£è™Ÿ'] = bc_names['ç”¢å“ä»£è™Ÿ'].astype(str).str.strip()
        dead_stock_df = pd.merge(dead_stock_df, bc_names.drop_duplicates(subset=['ç”¢å“ä»£è™Ÿ']), on='ç”¢å“ä»£è™Ÿ', how='left')

    # Select and order final columns
    final_cols_order = ['ç”¢å“ä»£è™Ÿ', 'ç”¢å“åç¨±', 'åº«å­˜æ•¸é‡']
    final_cols = [col for col in final_cols_order if col in dead_stock_df.columns]
    dead_stock_df = dead_stock_df[final_cols]

    return dead_stock_df.sort_values(by='åº«å­˜æ•¸é‡', ascending=False)

@st.cache_data
def perform_abc_analysis(_summary_df):
    """Performs ABC analysis based on product sales value ('å°è¨ˆ')."""
    summary_df = _summary_df.copy()
    if summary_df.empty or 'å°è¨ˆ' not in summary_df.columns or 'ç”¢å“ä»£è™Ÿ' not in summary_df.columns:
        st.warning("Cannot perform ABC analysis: Summary data is empty or missing 'å°è¨ˆ' or 'ç”¢å“ä»£è™Ÿ'.")
        return pd.DataFrame()

    # Ensure 'å°è¨ˆ' is numeric
    summary_df['å°è¨ˆ'] = pd.to_numeric(summary_df['å°è¨ˆ'], errors='coerce').fillna(0)

    # Sort by sales value descending
    df_sorted = summary_df.sort_values(by='å°è¨ˆ', ascending=False)

    # Calculate cumulative sales and percentage
    df_sorted['ç´¯è¨ˆéŠ·å”®é¡'] = df_sorted['å°è¨ˆ'].cumsum()
    total_sales = df_sorted['å°è¨ˆ'].sum()

    if total_sales == 0:
        st.info("Total sales are zero, cannot calculate ABC percentages.")
        df_sorted['ç´¯è¨ˆéŠ·å”®ä½”æ¯”'] = 0.0
        df_sorted['ABCåˆ†é¡'] = 'C' # Assign all to C if no sales
    else:
        df_sorted['ç´¯è¨ˆéŠ·å”®ä½”æ¯”'] = (df_sorted['ç´¯è¨ˆéŠ·å”®é¡'] / total_sales) * 100

        # Assign ABC category
        def assign_abc(percentage):
            if percentage <= 70: # A items: top 70%
                return 'A'
            elif percentage <= 90: # B items: next 20% (70% to 90%)
                return 'B'
            else: # C items: remaining 10%
                return 'C'
        df_sorted['ABCåˆ†é¡'] = df_sorted['ç´¯è¨ˆéŠ·å”®ä½”æ¯”'].apply(assign_abc)

    # Select and reorder columns for clarity
    abc_cols = ['ç”¢å“ä»£è™Ÿ', 'ç”¢å“åç¨±', 'å°è¨ˆ', 'ç´¯è¨ˆéŠ·å”®é¡', 'ç´¯è¨ˆéŠ·å”®ä½”æ¯”', 'ABCåˆ†é¡']
    # Add other relevant columns if they exist
    optional_cols = ['æ•¸é‡', 'ç²¾æº–æ¯›åˆ©', 'åº«å­˜æ•¸é‡', 'å» å•†ç°¡ç¨±'] # Add more if needed
    final_cols = abc_cols + [col for col in optional_cols if col in df_sorted.columns]
    # Ensure all selected columns actually exist before returning
    final_cols = [col for col in final_cols if col in df_sorted.columns]


    return df_sorted[final_cols]


# Function to merge sales data with BC data
def merge_with_bc(sales_summary_df, bc_df):
    """
    Merges the sales summary DataFrame with the BC (inventory) DataFrame based on 'ç”¢å“ä»£è™Ÿ'.

    Args:
        sales_summary_df (pd.DataFrame): DataFrame containing summarized sales data with 'ç”¢å“ä»£è™Ÿ'.
        bc_df (pd.DataFrame): DataFrame containing BC inventory data with 'ç”¢å“ä»£è™Ÿ', 'æ•¸é‡' (inventory), and 'å» å•†ç°¡ç¨±'.

    Returns:
        pd.DataFrame: Merged DataFrame with sales data and corresponding BC inventory and vendor info.
                      Returns an empty DataFrame if inputs are invalid or missing required columns.
    """
    # Input validation
    if sales_summary_df.empty or 'ç”¢å“ä»£è™Ÿ' not in sales_summary_df.columns:
        st.warning("Sales summary data is empty or missing 'ç”¢å“ä»£è™Ÿ'. Cannot merge.")
        return pd.DataFrame()
    if bc_df.empty or 'ç”¢å“ä»£è™Ÿ' not in bc_df.columns:
        st.warning("BC data is empty or missing 'ç”¢å“ä»£è™Ÿ'. Cannot merge.")
        # Return the sales summary without BC info if BC data is invalid
        # Add a placeholder column to indicate missing BC data
        sales_summary_df_copy = sales_summary_df.copy()
        sales_summary_df_copy['åº«å­˜æ•¸é‡'] = 'N/A'
        sales_summary_df_copy['å» å•†ç°¡ç¨±'] = 'N/A'
        return sales_summary_df_copy

    # --- Data Preparation ---
    # Create copies to avoid modifying original dataframes
    sales_summary_copy = sales_summary_df.copy()
    bc_df_copy = bc_df.copy()

    # Standardize 'ç”¢å“ä»£è™Ÿ' (Product Code) to string and strip whitespace for reliable merging
    sales_summary_copy['ç”¢å“ä»£è™Ÿ'] = sales_summary_copy['ç”¢å“ä»£è™Ÿ'].astype(str).str.strip()
    bc_df_copy['ç”¢å“ä»£è™Ÿ'] = bc_df_copy['ç”¢å“ä»£è™Ÿ'].astype(str).str.strip()

    # Rename sales quantity column for clarity *before* merge
    if 'æ•¸é‡' in sales_summary_copy.columns:
        sales_summary_copy.rename(columns={'æ•¸é‡': 'éŠ·å”®æ•¸é‡'}, inplace=True)

    # Select and rename relevant columns from BC data
    bc_cols_to_merge = ['ç”¢å“ä»£è™Ÿ']
    bc_rename_map = {}
    if 'æ•¸é‡' in bc_df_copy.columns:
        bc_cols_to_merge.append('æ•¸é‡')
        bc_rename_map['æ•¸é‡'] = 'åº«å­˜æ•¸é‡' # Rename BC quantity to avoid conflict
    if 'å» å•†ç°¡ç¨±' in bc_df_copy.columns:
        bc_cols_to_merge.append('å» å•†ç°¡ç¨±')
        # No rename needed for 'å» å•†ç°¡ç¨±' unless it conflicts

    # Check if essential BC columns exist before subsetting
    missing_bc_cols = [col for col in ['æ•¸é‡', 'å» å•†ç°¡ç¨±'] if col not in bc_df_copy.columns]
    if missing_bc_cols:
        st.warning(f"BC data is missing columns: {', '.join(missing_bc_cols)}. Merge will proceed without them.")

    # Create a subset of BC data with only the necessary columns (handle missing ones)
    valid_bc_cols_to_merge = [col for col in bc_cols_to_merge if col in bc_df_copy.columns]
    if 'ç”¢å“ä»£è™Ÿ' not in valid_bc_cols_to_merge: # Should not happen due to initial check, but safety first
         st.error("Critical error: 'ç”¢å“ä»£è™Ÿ' somehow lost in BC data processing.")
         return sales_summary_copy # Return sales data only

    bc_subset = bc_df_copy[valid_bc_cols_to_merge].copy()

    # Apply renaming to the subset
    bc_subset.rename(columns=bc_rename_map, inplace=True)

    # --- Merge Operation ---
    # Perform left merge: keep all sales summary rows, add BC info where available
    merged_df = pd.merge(sales_summary_copy, bc_subset, on='ç”¢å“ä»£è™Ÿ', how='left')

    # --- Post-Merge Handling ---
    # Fill NaN values resulting from the merge (products sold but not in BC file or missing BC columns)
    if 'åº«å­˜æ•¸é‡' in merged_df.columns:
        # Convert to numeric first, coercing errors, then fillna, then int
        merged_df['åº«å­˜æ•¸é‡'] = pd.to_numeric(merged_df['åº«å­˜æ•¸é‡'], errors='coerce').fillna(0).astype(int)
    else:
        # If 'åº«å­˜æ•¸é‡' wasn't even in bc_subset, add it as 0
        merged_df['åº«å­˜æ•¸é‡'] = 0
        st.info("Note: 'åº«å­˜æ•¸é‡' column was not found in BC data; added as 0.") # Re-applying indentation fix


    if 'å» å•†ç°¡ç¨±' in merged_df.columns:
        merged_df['å» å•†ç°¡ç¨±'] = merged_df['å» å•†ç°¡ç¨±'].fillna('æœªçŸ¥å» å•†') # Placeholder for missing vendor info
    else:
        # If 'å» å•†ç°¡ç¨±' wasn't in bc_subset, add it
        merged_df['å» å•†ç°¡ç¨±'] = 'æœªçŸ¥å» å•†'
        st.info("Note: 'å» å•†ç°¡ç¨±' column was not found in BC data; added as 'æœªçŸ¥å» å•†'.")

    # Ensure 'éŠ·å”®æ•¸é‡' exists, filling with 0 if it wasn't in the original sales summary
    if 'éŠ·å”®æ•¸é‡' not in merged_df.columns:
        merged_df['éŠ·å”®æ•¸é‡'] = 0

    # --- Final Column Order (Optional but good practice) ---
    # Define desired column order, including the potentially renamed/added columns
    desired_cols_order = [
        'ç”¢å“ä»£è™Ÿ', 'ç”¢å“åç¨±', # From sales summary
        'éŠ·å”®æ•¸é‡', 'å°è¨ˆ', 'ç²¾æº–æˆæœ¬', 'ç²¾æº–æ¯›åˆ©', # From sales summary (renamed quantity)
        'åº«å­˜æ•¸é‡', 'å» å•†ç°¡ç¨±', # From BC data
        # Add other columns from sales_summary_df if they exist and are desired
        'å€‰åˆ¥ä»£è™Ÿ', 'å€‰åˆ¥åç¨±'
    ]
    # Filter to only include columns that actually exist in the merged_df
    final_columns = [col for col in desired_cols_order if col in merged_df.columns]
    # Add any remaining columns from merged_df not in the desired list
    final_columns.extend([col for col in merged_df.columns if col not in final_columns])

    return merged_df[final_columns] # Return with potentially reordered columns

def vendor_summary_table(merged_df):
    if merged_df.empty or 'å» å•†ç°¡ç¨±' not in merged_df.columns or 'å°è¨ˆ' not in merged_df.columns:
        return pd.DataFrame()
        
    summary = merged_df.groupby('å» å•†ç°¡ç¨±').agg(
        å» å•†ç¸½å°è¨ˆ=('å°è¨ˆ', 'sum')
    ).reset_index().sort_values(by='å» å•†ç¸½å°è¨ˆ', ascending=False)
    return summary

def extract_months_from_filenames(files):
    months = []
    for f in files:
        name = f.name
        try:
            import re
            date_patterns = [r'(\d{6})', r'(\d{4}[-_][01]\d)', r'(\d{4}å¹´[01]?\dæœˆ)']
            found_date = False
            for pattern in date_patterns:
                matches = re.findall(pattern, name)
                if matches:
                    for match in matches:
                        clean_match = re.sub(r'[-_å¹´æœˆ]', '', match)
                        if clean_match.startswith('20') and len(clean_match) >= 6:
                            year_month = clean_match[:6]
                            if year_month not in months: # Avoid duplicates
                                months.append(year_month)
                            found_date = True
                            break # Found a match for this file with this pattern
                if found_date: break # Found a match for this file
            if not found_date:
                digits = ''.join(c for c in name if c.isdigit())
                if len(digits) >= 6 and digits.startswith('20'):
                    year_month = digits[:6]
                    if year_month not in months:
                         months.append(year_month)
        except:
            continue
    return sorted(list(set(months)), reverse=True)

def check_required_columns(df, required_columns):
    """Check if all required columns exist and suggest mappings."""
    present_columns = []
    missing_columns = []
    suggestions = {}
    df_columns_lower = {col.lower(): col for col in df.columns} # Store original casing
    required_columns_lower = {col.lower() for col in required_columns}

    for req_col_orig in required_columns:
        req_col_lower = req_col_orig.lower()
        if req_col_lower in df_columns_lower:
            present_columns.append(df_columns_lower[req_col_lower]) # Use original casing
        else:
            missing_columns.append(req_col_orig)
            # Suggest similar column names (case-insensitive comparison)
            potential_matches = []
            for df_col_lower, df_col_orig in df_columns_lower.items():
                 # Simple substring check or more advanced fuzzy matching could be used
                 if req_col_lower in df_col_lower or df_col_lower in req_col_lower:
                     potential_matches.append(df_col_orig) # Suggest original casing
            if potential_matches:
                suggestions[req_col_orig] = potential_matches
                
    # Return original casing for present columns as well
    present_columns_original_case = [col for col in required_columns if col.lower() in df_columns_lower]

    return present_columns_original_case, missing_columns, suggestions

# =================== MAIN PAGE ===================

# Define required headers for later use
required_headers = [
    "éŠ·è²¨å–®è™Ÿ", "è¨‚å–®å–®è™Ÿ", "éŠ·è²¨æ—¥æœŸ", "å®¢æˆ¶ä»£è™Ÿ", "å®¢æˆ¶åç¨±", "éƒ¨é–€ä»£è™Ÿ", "éƒ¨é–€åç¨±",
    "ç™¼ç¥¨è™Ÿç¢¼", "æœªç¨…å°è¨ˆ", "ç‡Ÿæ¥­ç¨…", "æŠ˜è®“é‡‘é¡", "ç¨…å‰æŠ˜åƒ¹", "ç¸½è¨ˆé‡‘é¡", "å¯¦æ”¶ç¸½é¡",
    "æˆæœ¬ç¸½é¡", "æ¯›åˆ©", "æ¯›åˆ©ç‡", "ç”¢å“ä»£è™Ÿ", "ç”¢å“åç¨±", "å€‰åˆ¥ä»£è™Ÿ", "å€‰åˆ¥åç¨±",
    "æ•¸é‡", "å–®ä½", "å–®åƒ¹", "å°è¨ˆ", "æˆæœ¬ç¸½å€¼", "ç”¢å“æ¯›åˆ©", "ç”¢å“æ¯›åˆ©ç‡",
    "éŠ·å”®å–®åƒ¹1", "ç²¾æº–æˆæœ¬", "ç²¾æº–æ¯›åˆ©", "å–®ä½ç®¡éŠ·æˆæœ¬", "ç®¡éŠ·æˆæœ¬åˆè¨ˆ"
]

# Group headers by category for better organization
header_categories = {
    "è¨‚å–®åŸºæœ¬è³‡è¨Š": ["éŠ·è²¨å–®è™Ÿ", "è¨‚å–®å–®è™Ÿ", "éŠ·è²¨æ—¥æœŸ"],
    "å®¢æˆ¶è³‡è¨Š": ["å®¢æˆ¶ä»£è™Ÿ", "å®¢æˆ¶åç¨±", "éƒ¨é–€ä»£è™Ÿ", "éƒ¨é–€åç¨±", "ç™¼ç¥¨è™Ÿç¢¼"],
    "è¨‚å–®é‡‘é¡è³‡è¨Š": ["æœªç¨…å°è¨ˆ", "ç‡Ÿæ¥­ç¨…", "æŠ˜è®“é‡‘é¡", "ç¨…å‰æŠ˜åƒ¹", "ç¸½è¨ˆé‡‘é¡", "å¯¦æ”¶ç¸½é¡", "æˆæœ¬ç¸½é¡", "æ¯›åˆ©", "æ¯›åˆ©ç‡"],
    "ç”¢å“è³‡è¨Š": ["ç”¢å“ä»£è™Ÿ", "ç”¢å“åç¨±", "å€‰åˆ¥ä»£è™Ÿ", "å€‰åˆ¥åç¨±"],
    "ç”¢å“éŠ·å”®ç´°ç¯€": ["æ•¸é‡", "å–®ä½", "å–®åƒ¹", "å°è¨ˆ", "æˆæœ¬ç¸½å€¼", "ç”¢å“æ¯›åˆ©", "ç”¢å“æ¯›åˆ©ç‡", "éŠ·å”®å–®åƒ¹1", "ç²¾æº–æˆæœ¬", "ç²¾æº–æ¯›åˆ©"],
    "å…¶ä»–": ["å–®ä½ç®¡éŠ·æˆæœ¬", "ç®¡éŠ·æˆæœ¬åˆè¨ˆ"]
}

# Create sample data for reference and download
sample_data = {
    "éŠ·è²¨å–®è™Ÿ": ["S202407001", "S202407001", "S202407002", "S202407002"], "è¨‚å–®å–®è™Ÿ": ["O202407001", "O202407001", "O202407002", "O202407002"],
    "éŠ·è²¨æ—¥æœŸ": ["2024/07/01", "2024/07/01", "2024/07/05", "2024/07/05"], "å®¢æˆ¶ä»£è™Ÿ": ["C001", "C001", "C002", "C002"],
    "å®¢æˆ¶åç¨±": ["å¯µç‰©æ¨‚åœ’", "å¯µç‰©æ¨‚åœ’", "æ¯›å­©ä¹‹å®¶", "æ¯›å­©ä¹‹å®¶"], "éƒ¨é–€ä»£è™Ÿ": ["D01", "D01", "D01", "D01"],
    "éƒ¨é–€åç¨±": ["éŠ·å”®éƒ¨", "éŠ·å”®éƒ¨", "éŠ·å”®éƒ¨", "éŠ·å”®éƒ¨"], "ç™¼ç¥¨è™Ÿç¢¼": ["IV240701", "IV240701", "IV240705", "IV240705"],
    "æœªç¨…å°è¨ˆ": [3000, 3000, 2500, 2500], "ç‡Ÿæ¥­ç¨…": [150, 150, 125, 125], "ç¸½è¨ˆé‡‘é¡": [3150, 3150, 2625, 2625],
    "ç”¢å“ä»£è™Ÿ": ["P001", "P002", "P001", "P003"], "ç”¢å“åç¨±": ["ç‹—ç³§1kg", "è²“ç³§1kg", "ç‹—ç³§1kg", "è²“ç©å…·"],
    "å€‰åˆ¥ä»£è™Ÿ": ["W01", "W01", "W01", "W02"], "å€‰åˆ¥åç¨±": ["ä¸»å€‰", "ä¸»å€‰", "ä¸»å€‰", "é…ä»¶å€‰"],
    "æ•¸é‡": [10, 5, 8, 3], "å–®ä½": ["åŒ…", "åŒ…", "åŒ…", "å€‹"], "å–®åƒ¹": [200, 250, 200, 300],
    "å°è¨ˆ": [2000, 1250, 1600, 900], "ç²¾æº–æˆæœ¬": [1400, 900, 1120, 500], "ç²¾æº–æ¯›åˆ©": [600, 350, 480, 400]
}
sample_df = pd.DataFrame(sample_data)

# Divider
st.markdown("---")

# Add sidebar information
st.sidebar.title("è³‡æ–™ä¾†æºé¸æ“‡")
with st.sidebar.expander("â“ æª”æ¡ˆæ ¼å¼èªªæ˜"):
    st.write("""
    ### éŠ·è²¨æª”æ¡ˆè¦æ±‚
    - æª”æ¡ˆé¡å‹: Excel (.xlsx)
    - éŠ·è²¨æª”æ¡ˆæ‡‰åŒ…å«å®¢æˆ¶è³‡è¨Šã€ç”¢å“è³‡è¨Šã€éŠ·å”®æ•¸é‡åŠé‡‘é¡ç­‰æ¬„ä½
    - æª”æ¡ˆåç¨±å»ºè­°åŒ…å«å¹´æœˆï¼Œä¾‹å¦‚: `sales_202407.xlsx`
    ### BC æª”æ¡ˆè¦æ±‚
    - æª”æ¡ˆé¡å‹: Excel (.xlsx)
    - æ‡‰åŒ…å«ç”¢å“ä»£è™Ÿã€åº«å­˜æ•¸é‡ç­‰æ¬„ä½
    ### å¸¸è¦‹å•é¡Œ
    1. **ç¼ºå°‘æ¬„ä½**: å¦‚æœæç¤ºç¼ºå°‘æ¬„ä½ï¼Œè«‹æª¢æŸ¥æ‚¨çš„æª”æ¡ˆæ˜¯å¦åŒ…å«æ‰€æœ‰å¿…è¦æ¬„ä½
    2. **æ¬„ä½åç¨±ä¸åŒ¹é…**: è«‹ç¢ºä¿æ¬„ä½åç¨±èˆ‡ç³»çµ±éœ€æ±‚ä¸€è‡´
    3. **æª”æ¡ˆæœªé¡¯ç¤º**: è«‹ç¢ºä¿æª”æ¡ˆåç¨±ä¸­åŒ…å«å¹´æœˆæ¨™è¨˜
    """)

data_source = st.sidebar.radio("é¸æ“‡æ•¸æ“šä¾†æºæ–¹å¼", ["ä¸Šå‚³æ–°æª”æ¡ˆ", "ä½¿ç”¨å·²ä¸Šå‚³çš„æª”æ¡ˆ"])

if data_source == "ä¸Šå‚³æ–°æª”æ¡ˆ":
    with st.sidebar.expander("ğŸ“Š æ•¸æ“šæ ¼å¼ç¯„ä¾‹"):
        st.code("""
éŠ·è²¨å–®è™Ÿ    éŠ·è²¨æ—¥æœŸ    å®¢æˆ¶ä»£è™Ÿ    å®¢æˆ¶åç¨±    ç”¢å“ä»£è™Ÿ    ç”¢å“åç¨±    æ•¸é‡    å–®åƒ¹    å°è¨ˆ
S202301001  2023/01/05  C001       å¯µç‰©æ¨‚åœ’    P001       ç‹—ç³§1kg    10     200     2000
S202301001  2023/01/05  C001       å¯µç‰©æ¨‚åœ’    P002       è²“ç³§1kg    5      250     1250
S202301002  2023/01/10  C002       æ¯›å­©ä¹‹å®¶    P003       å¯µç‰©ç©å…·    8      150     1200
        """)

# Initialize file sources
sales_files = None
bc_file = None
sales_data_source = []
bc_data_source = None

# File Upload / Selection UI
if data_source == "ä¸Šå‚³æ–°æª”æ¡ˆ":
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("ä¸Šå‚³éŠ·è²¨æª”æ¡ˆ")
        sales_files = st.file_uploader("ä¸Šå‚³éŠ·è²¨å–®æ¯›åˆ©åˆ†æ Excelï¼ˆå¯å¤šé¸ï¼‰", type=['xlsx'], accept_multiple_files=True, key="sales_uploader")
        if sales_files:
            temp_saved_sales = []
            for file in sales_files:
                file_path = save_uploaded_file(file, SALES_DIR)
                temp_saved_sales.append({"name": file.name, "path": file_path})
            # Only update session state if new files are uploaded
            if temp_saved_sales:
                 st.session_state.saved_sales_files = temp_saved_sales
            sales_data_source = sales_files # Use the uploader objects directly for loading
    with col2:
        st.subheader("ä¸Šå‚³ BC è³‡æ–™")
        bc_file = st.file_uploader("ä¸Šå‚³ BC SKU è³‡æ–™ Excelï¼ˆå–®ä¸€ï¼‰", type=['xlsx'], key="bc_uploader")
        if bc_file:
            file_path = save_uploaded_file(bc_file, BC_DIR)
            st.session_state.saved_bc_file = {"name": bc_file.name, "path": file_path}
            bc_data_source = st.session_state.saved_bc_file['path'] # Use path for loading
else: # Use previously uploaded files
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("é¸æ“‡å·²ä¸Šå‚³çš„éŠ·è²¨æª”æ¡ˆ")
        available_sales_files = get_file_list(SALES_DIR, ".xlsx")
        if not available_sales_files:
            st.info("å°šæœªä¸Šå‚³ä»»ä½•éŠ·è²¨æª”æ¡ˆ")
        else:
            file_options = [f"{file['name']}" for file in available_sales_files]
            selected_sales_files_names = st.multiselect("é¸æ“‡éŠ·è²¨æª”æ¡ˆ", file_options, key="sales_selector")
            # Map selected file names to their paths for loading
            sales_data_source = [f['path'] for f in available_sales_files if f['name'] in selected_sales_files_names]
    with col2:
        st.subheader("é¸æ“‡å·²ä¸Šå‚³çš„ BC æª”æ¡ˆ")
        available_bc_files = get_file_list(BC_DIR, ".xlsx")
        if not available_bc_files:
            st.info("å°šæœªä¸Šå‚³ä»»ä½• BC æª”æ¡ˆ")
        else:
            file_options = [f"{file['name']}" for file in available_bc_files]
            selected_bc_file_name = st.selectbox("é¸æ“‡ BC æª”æ¡ˆ", file_options, key="bc_selector")
            # Map selected file name to its path for loading
            bc_data_source = next((f['path'] for f in available_bc_files if f['name'] == selected_bc_file_name), None)

# Month selection based on available files
months = []
files_for_month_extraction = []
if data_source == "ä¸Šå‚³æ–°æª”æ¡ˆ" and sales_files:
    files_for_month_extraction = sales_files
elif data_source == "ä½¿ç”¨å·²ä¸Šå‚³çš„æª”æ¡ˆ" and sales_data_source:
    # Create mock file objects with names for extraction function
    files_for_month_extraction = [type('obj', (object,), {'name': os.path.basename(path)}) for path in sales_data_source]

if files_for_month_extraction:
    months = extract_months_from_filenames(files_for_month_extraction)

selected_months = []
if months:
    selected_months = st.multiselect("é¸æ“‡æœˆä»½ï¼ˆå¯è¤‡é¸ï¼‰", months, key="month_selector")

# Initialize session state for column mapping if it doesn't exist
if 'column_mapping' not in st.session_state:
    st.session_state.column_mapping = {}

# Show preview of uploaded/selected files
st.markdown("---")
st.subheader("ğŸ“‹ ä¸Šå‚³/é¸æ“‡æª”æ¡ˆé è¦½")

preview_file_source = None
if data_source == "ä¸Šå‚³æ–°æª”æ¡ˆ" and sales_files:
    preview_file_source = sales_files[0] # Preview first uploaded file
elif data_source == "ä½¿ç”¨å·²ä¸Šå‚³çš„æª”æ¡ˆ" and sales_data_source:
    preview_file_source = sales_data_source[0] # Preview first selected file path

if preview_file_source:
    try:
        sample_df_preview = pd.read_excel(preview_file_source, nrows=5)
        st.write("éŠ·è²¨è³‡æ–™é è¦½ (å‰5ç­†):")
        st.dataframe(sample_df_preview, use_container_width=True)
        
        present_headers, missing_headers, suggestions = check_required_columns(sample_df_preview, required_headers)
        
        if missing_headers:
            st.warning(f"**æ³¨æ„:** ä¸Šå‚³/é¸æ“‡çš„æª”æ¡ˆç¼ºå°‘ {len(missing_headers)} å€‹å¿…è¦æ¬„ä½")
            with st.expander("ğŸ”„ æ¬„ä½å°æ‡‰è¨­å®š (è™•ç†æ¬„ä½åç¨±ä¸ä¸€è‡´)", expanded=True):
                st.write("æ‚¨å¯ä»¥å°‡æª”æ¡ˆä¸­çš„æ¬„ä½å°æ‡‰åˆ°ç³»çµ±éœ€è¦çš„æ¬„ä½åç¨±:")
                with st.form("column_mapping_form"):
                    essential_cols = ['ç”¢å“ä»£è™Ÿ', 'ç”¢å“åç¨±', 'æ•¸é‡', 'å–®åƒ¹', 'å°è¨ˆ', 'ç²¾æº–æˆæœ¬', 'ç²¾æº–æ¯›åˆ©'] # Added cost/profit
                    missing_essential = [col for col in essential_cols if col in missing_headers]
                    mapping_dict = {}
                    file_columns = list(sample_df_preview.columns)
                    
                    if missing_essential:
                        st.write("**å¿…è¦æ¬„ä½å°æ‡‰:**")
                        for col in missing_essential:
                            suggested_options = suggestions.get(col, [])
                            options = [""] + suggested_options + [c for c in file_columns if c not in suggested_options]
                            # Use current mapping as default if available
                            current_mapping = next((k for k, v in st.session_state.column_mapping.items() if v == col), None)
                            default_index = options.index(current_mapping) if current_mapping in options else 0
                            mapping_dict[col] = st.selectbox(f"è«‹é¸æ“‡å°æ‡‰ '{col}' çš„æ¬„ä½:", options, index=default_index)
                    
                    other_missing = [col for col in missing_headers if col not in missing_essential]
                    if other_missing and len(other_missing) <= 10: # Show more optional mappings
                        st.write("**å…¶ä»–æ¬„ä½å°æ‡‰ (é¸å¡«):**")
                        for col in other_missing:
                            suggested_options = suggestions.get(col, [])
                            options = [""] + suggested_options + [c for c in file_columns if c not in suggested_options]
                            current_mapping = next((k for k, v in st.session_state.column_mapping.items() if v == col), None)
                            default_index = options.index(current_mapping) if current_mapping in options else 0
                            mapping_dict[col] = st.selectbox(f"è«‹é¸æ“‡å°æ‡‰ '{col}' çš„æ¬„ä½:", options, index=default_index)
                    
                    submitted = st.form_submit_button("æ‡‰ç”¨æ¬„ä½å°æ‡‰")
                    if submitted:
                        final_mapping = {v: k for k, v in mapping_dict.items() if v} # Reversed: {file_col: required_col}
                        st.session_state.column_mapping = final_mapping
                        st.success(f"âœ… å·²è¨­å®š {len(final_mapping)} å€‹æ¬„ä½å°æ‡‰")
                        st.rerun() # Rerun to apply mapping immediately

                if st.session_state.column_mapping:
                    st.write("**ç›®å‰çš„æ¬„ä½å°æ‡‰:**")
                    mapping_items = [{"åŸå§‹æ¬„ä½": orig, "å°æ‡‰è‡³": mapped} for orig, mapped in st.session_state.column_mapping.items()]
                    st.dataframe(pd.DataFrame(mapping_items), use_container_width=True)
                    if st.button("æ¸…é™¤æ‰€æœ‰å°æ‡‰"):
                        st.session_state.column_mapping = {}
                        st.rerun()
            
            missing_data = [{"ç¼ºå°‘æ¬„ä½": col, "å»ºè­°å°æ‡‰": ", ".join(suggestions.get(col, ["ç„¡"])[:3])} for col in missing_headers]
            missing_df = pd.DataFrame(missing_data)
            st.dataframe(missing_df, use_container_width=True)
            st.info("ğŸ’¡ æç¤º: è‹¥æª”æ¡ˆä¸­ä½¿ç”¨äº†ä¸åŒçš„æ¬„ä½åç¨±ï¼Œè«‹ä½¿ç”¨ä¸Šæ–¹çš„æ¬„ä½å°æ‡‰åŠŸèƒ½ä¾†è§£æ±º")
        else:
            st.success("âœ… æ‰€æœ‰å¿…è¦æ¬„ä½éƒ½å·²å­˜åœ¨")
        st.write(f"æª”æ¡ˆçµ±è¨ˆ: {len(sample_df_preview.columns)} å€‹æ¬„ä½")
    except Exception as e:
        st.error(f"ç„¡æ³•è®€å–æª”æ¡ˆé è¦½: {e}")
else:
    st.info("è«‹ä¸Šå‚³æˆ–é¸æ“‡æª”æ¡ˆä»¥æŸ¥çœ‹é è¦½")

st.markdown("---")

# =================== SECTION DISPLAY LOGIC ===================

# Removed logic for sections: "å» å•†ç”¢å“æŸ¥è©¢", "ç”¢å“åˆ†æ", "BC è³‡æ–™æ¯”å°", "è¨‚å–®èª¿é–±"
    
if st.session_state.current_section == "ç”Ÿæˆå ±è¡¨":
    # Button to trigger report generation
    has_data_input = (data_source == "ä¸Šå‚³æ–°æª”æ¡ˆ" and sales_files) or \
                     (data_source == "ä½¿ç”¨å·²ä¸Šå‚³çš„æª”æ¡ˆ" and sales_data_source)
                     
    if st.button("â–¶ï¸ ç”Ÿæˆå ±è¡¨", key="generate_report_button") and has_data_input and selected_months:
        with st.spinner("è™•ç†è³‡æ–™ä¸­..."):
            try:
                # --- Reset previous report state ---
                st.session_state.report_generated = False
                keys_to_clear = ['sales_df', 'summary_df', 'order_summary', 'bc_df', 'merged_df', 'selected_months_display', 'customer_summary']
                for key in keys_to_clear:
                    if key in st.session_state: del st.session_state[key]
                # --- End Reset ---

                column_mapping = st.session_state.get('column_mapping', {})
                
                # Determine if loading from path or object
                load_is_path = (data_source == "ä½¿ç”¨å·²ä¸Šå‚³çš„æª”æ¡ˆ")
                
                # Load sales data
                sales_df_loaded = load_sales_data(sales_data_source, selected_months, is_path=load_is_path, column_mapping=column_mapping)

                if sales_df_loaded.empty:
                    st.error("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆé¸æ“‡æœˆä»½çš„è³‡æ–™ï¼Œè«‹æª¢æŸ¥æ‚¨çš„é¸æ“‡ã€‚")
                    st.session_state.report_generated = False
                else:
                    st.session_state.sales_df = sales_df_loaded
                    st.session_state.summary_df = generate_monthly_summary(sales_df_loaded)
                    st.session_state.order_summary = generate_order_summary(sales_df_loaded)
                    # Generate customer summary from order summary
                    st.session_state.customer_summary = generate_customer_summary(st.session_state.order_summary)
                    
                    # Perform ABC Analysis
                    st.session_state.abc_analysis_df = perform_abc_analysis(st.session_state.summary_df)
                    
                    # Initialize dead stock dataframe
                    st.session_state.dead_stock_df = pd.DataFrame() 

                    # --- Process BC data ---
                    bc_df_loaded = pd.DataFrame()
                    merged_df_loaded = pd.DataFrame()
                    bc_load_error = None
                    merge_possible = False

                    # Determine BC source path
                    current_bc_source = None
                    if data_source == "ä¸Šå‚³æ–°æª”æ¡ˆ" and bc_file:
                        if 'saved_bc_file' in st.session_state and st.session_state.saved_bc_file:
                            current_bc_source = st.session_state.saved_bc_file['path']
                    elif data_source == "ä½¿ç”¨å·²ä¸Šå‚³çš„æª”æ¡ˆ" and bc_data_source:
                        current_bc_source = bc_data_source

                    # Attempt to load BC data if source exists
                    if current_bc_source:
                        if os.path.exists(str(current_bc_source)):
                            try:
                                bc_df_loaded = pd.read_excel(current_bc_source)
                                if bc_df_loaded.empty:
                                    bc_load_error = "BC æª”æ¡ˆå·²è¼‰å…¥ä½†å…§å®¹ç‚ºç©ºã€‚"
                                elif 'ç”¢å“ä»£è™Ÿ' not in bc_df_loaded.columns:
                                    bc_load_error = "BC æª”æ¡ˆç¼ºå°‘ 'ç”¢å“ä»£è™Ÿ' æ¬„ä½ã€‚"
                            except Exception as e:
                                bc_load_error = f"è®€å– BC æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
                        else:
                            bc_load_error = "é¸æ“‡çš„ BC æª”æ¡ˆè·¯å¾‘ä¸å­˜åœ¨ã€‚"
                    # No warning if BC file wasn't provided intentionally
                    # elif (data_source == "ä¸Šå‚³æ–°æª”æ¡ˆ" and bc_file) or (data_source == "ä½¿ç”¨å·²ä¸Šå‚³çš„æª”æ¡ˆ" and bc_data_source):
                    #      bc_load_error = "é¸æ“‡çš„ BC æª”æ¡ˆç„¡æ³•æ‰¾åˆ°æˆ–è®€å–ã€‚"

                    # --- Standardize Product Codes before checking validity ---
                    summary_df_to_merge = st.session_state.summary_df.copy() # Work on a copy
                    if not summary_df_to_merge.empty and 'ç”¢å“ä»£è™Ÿ' in summary_df_to_merge.columns:
                        summary_df_to_merge['ç”¢å“ä»£è™Ÿ'] = summary_df_to_merge['ç”¢å“ä»£è™Ÿ'].astype(str).str.strip()
                    if not bc_df_loaded.empty and 'ç”¢å“ä»£è™Ÿ' in bc_df_loaded.columns:
                        # Ensure bc_df_loaded is updated after loading inside the try block
                        pass # Standardization will happen after loading now

                    # Check if merge is possible (re-check validity after standardization)
                    summary_df_valid = not summary_df_to_merge.empty and 'ç”¢å“ä»£è™Ÿ' in summary_df_to_merge.columns
                    # bc_df_valid check needs to happen after loading
                    
                    # Attempt to load BC data if source exists
                    if current_bc_source:
                        if os.path.exists(str(current_bc_source)):
                            try:
                                bc_df_loaded = pd.read_excel(current_bc_source)
                                if bc_df_loaded.empty:
                                    bc_load_error = "BC æª”æ¡ˆå·²è¼‰å…¥ä½†å…§å®¹ç‚ºç©ºã€‚"
                                elif 'ç”¢å“ä»£è™Ÿ' not in bc_df_loaded.columns:
                                    bc_load_error = "BC æª”æ¡ˆç¼ºå°‘ 'ç”¢å“ä»£è™Ÿ' æ¬„ä½ã€‚"
                                else:
                                    # Standardize BC product code here
                                    bc_df_loaded['ç”¢å“ä»£è™Ÿ'] = bc_df_loaded['ç”¢å“ä»£è™Ÿ'].astype(str).str.strip()
                            except Exception as e:
                                bc_load_error = f"è®€å– BC æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
                        else:
                            bc_load_error = "é¸æ“‡çš„ BC æª”æ¡ˆè·¯å¾‘ä¸å­˜åœ¨ã€‚"

                    # Now check BC validity again after potential loading and standardization
                    bc_df_valid = not bc_df_loaded.empty and 'ç”¢å“ä»£è™Ÿ' in bc_df_loaded.columns

                    if summary_df_valid and bc_df_valid:
                        merge_possible = True
                        try:
                            # Pass the potentially standardized dataframes to the merge function
                            merged_df_loaded = merge_with_bc(summary_df_to_merge, bc_df_loaded)
                        except Exception as e:
                             st.error(f"åˆä½µéŠ·å”®èˆ‡ BC è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                             merged_df_loaded = pd.DataFrame() # Ensure it's empty on merge error
                    elif bc_load_error:
                         st.warning(f"ç„¡æ³•é€²è¡Œ BC è³‡æ–™åˆä½µï¼Œå› ç‚ºï¼š{bc_load_error}")
                    elif not summary_df_valid:
                         st.warning("ç„¡æ³•é€²è¡Œ BC è³‡æ–™åˆä½µï¼Œå› ç‚ºéŠ·å”®å½™ç¸½è³‡æ–™ç„¡æ•ˆæˆ–ç¼ºå°‘ 'ç”¢å“ä»£è™Ÿ'ã€‚")
                    # If BC wasn't provided, no warning needed here, merge_possible remains False

                    # --- Identify Dead Stock (after BC and Summary are processed) ---
                    if bc_df_valid and summary_df_valid: # Only run if both inputs are valid
                        try:
                            st.session_state.dead_stock_df = identify_dead_stock(st.session_state.summary_df, bc_df_loaded)
                        except Exception as e:
                            st.error(f"è¨ˆç®—æ»¯éŠ·åº«å­˜æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                            st.session_state.dead_stock_df = pd.DataFrame() # Ensure it's empty on error
                    elif bc_df_valid: # BC is valid, but sales summary might be empty/invalid
                         # Attempt identification, function handles empty summary
                         try:
                             st.session_state.dead_stock_df = identify_dead_stock(st.session_state.summary_df, bc_df_loaded)
                         except Exception as e:
                             st.error(f"è¨ˆç®—æ»¯éŠ·åº«å­˜æ™‚ç™¼ç”ŸéŒ¯èª¤ (ç„¡éŠ·å”®è³‡æ–™): {e}")
                             st.session_state.dead_stock_df = pd.DataFrame()
                    # --- End Dead Stock Identification ---

                    # Store results in session state
                    st.session_state.bc_df = bc_df_loaded
                    st.session_state.merged_df = merged_df_loaded
                    st.session_state.report_generated = True
                    st.session_state.bc_load_error = bc_load_error # Store potential load error
                    st.session_state.merge_possible = merge_possible # Store if merge was attempted
                    st.session_state.selected_months_display = selected_months
                    st.rerun() # Rerun immediately to display the report section

            except Exception as e:
                st.error(f"ç”Ÿæˆå ±è¡¨æ™‚ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}")
                st.session_state.report_generated = False

    # --- Report Display Section ---
    if st.session_state.get('report_generated', False):
        # Retrieve data from session state
        sales_df = st.session_state.get('sales_df', pd.DataFrame())
        summary_df = st.session_state.get('summary_df', pd.DataFrame())
        order_summary = st.session_state.get('order_summary', pd.DataFrame())
        bc_df = st.session_state.get('bc_df', pd.DataFrame())
        merged_df = st.session_state.get('merged_df', pd.DataFrame())
        customer_summary = st.session_state.get('customer_summary', pd.DataFrame())
        selected_months_display = st.session_state.get('selected_months_display', [])

        if not sales_df.empty:
            order_tab, product_tab, bc_tab = st.tabs(["è¨‚å–®æ‘˜è¦", "ç”¢å“åˆ†æ", "BC è³‡æ–™æ¯”å°"])

            with product_tab:
                st.subheader(f"ğŸ§¾ {'ã€'.join(selected_months_display)} ç”¢å“éŠ·å”®å½™ç¸½è¡¨")
                # Add ABC Analysis tab
                product_tab1, product_tab2, product_tab3, product_tab4 = st.tabs(["å‰20å¤§ç”¢å“", "ç”¢å“åˆ†æåœ–è¡¨", "å®Œæ•´ç”¢å“åˆ—è¡¨", "ABC åˆ†æ"])
                show_debug = st.checkbox("é¡¯ç¤ºè¨ˆç®—é©—è­‰è³‡è¨Š", value=False, key="debug_checkbox_product", help="é–‹å•Ÿæ­¤é¸é …å¯ä»¥æª¢è¦–æ•¸æ“šè¨ˆç®—éç¨‹å’Œé©—è­‰è³‡è¨Š")
                
                with product_tab1:
                    st.markdown("### ğŸ† å‰20å¤§éŠ·å”®ç”¢å“")
                    if not summary_df.empty:
                        top_products = summary_df.sort_values(by='å°è¨ˆ', ascending=False).head(20).copy()
                        display_df = top_products.copy()
                        for col in ['å°è¨ˆ', 'ç²¾æº–æˆæœ¬', 'ç²¾æº–æ¯›åˆ©']:
                            if col in display_df.columns:
                                display_df[col] = display_df[col].apply(lambda x: f"${x:,.0f}")
                        display_df.insert(0, 'æ’å', range(1, len(display_df) + 1))
                        display_cols = ['æ’å', 'ç”¢å“ä»£è™Ÿ', 'ç”¢å“åç¨±', 'æ•¸é‡', 'å°è¨ˆ', 'ç²¾æº–æˆæœ¬', 'ç²¾æº–æ¯›åˆ©']
                        st.dataframe(display_df[[col for col in display_cols if col in display_df.columns]], use_container_width=True)
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.markdown("#### å‰20å¤§ç”¢å“éŠ·å”®é¡")
                            import altair as alt
                            chart_data = pd.DataFrame({'ç”¢å“': top_products['ç”¢å“åç¨±'] + ' (' + top_products['ç”¢å“ä»£è™Ÿ'] + ')', 'éŠ·å”®é¡': top_products['å°è¨ˆ']})
                            chart = alt.Chart(chart_data).mark_bar().encode(y=alt.Y('ç”¢å“:N', sort='-x', title=None), x=alt.X('éŠ·å”®é¡:Q', title='éŠ·å”®é¡'), color=alt.value('#4CAF50'), tooltip=['ç”¢å“', 'éŠ·å”®é¡']).properties(height=400)
                            text = chart.mark_text(align='left', baseline='middle', dx=5, color='black').encode(text=alt.Text('éŠ·å”®é¡', format='$,.0f'))
                            final_chart = (chart + text).configure_view(strokeWidth=0).configure_axis(labelFontSize=12, titleFontSize=14, grid=False)
                            st.altair_chart(final_chart, use_container_width=True)
                        with col2:
                            st.markdown("#### å‰20å¤§ç”¢å“éŠ·å”®ä½”æ¯”")
                            total_sales_all = summary_df['å°è¨ˆ'].sum()
                            if total_sales_all > 0:
                                top_products['ä½”æ¯”'] = top_products['å°è¨ˆ'] / total_sales_all * 100
                                top_products['æ¨™ç±¤'] = top_products.apply(lambda x: f"{x['ç”¢å“åç¨±']} ({x['ä½”æ¯”']:.1f}%)", axis=1)
                                if show_debug:
                                    st.write(f"ç¸½éŠ·å”®é¡: ${total_sales_all:,.0f}")
                                    st.write(f"å‰20å¤§ç”¢å“éŠ·å”®é¡: ${top_products['å°è¨ˆ'].sum():,.0f} ({top_products['å°è¨ˆ'].sum()/total_sales_all*100:.1f}%)")
                                import plotly.express as px
                                fig = px.pie(top_products, values='å°è¨ˆ', names='æ¨™ç±¤', title='å‰20å¤§ç”¢å“éŠ·å”®ä½”æ¯”', color_discrete_sequence=px.colors.qualitative.Pastel)
                                fig.update_traces(textposition='inside', textinfo='percent+label', hoverinfo='label+percent+value', textfont_size=10, marker=dict(line=dict(color='#FFFFFF', width=1)))
                                fig.update_layout(showlegend=False, margin=dict(t=50, b=20, l=0, r=0))
                                st.plotly_chart(fig, use_container_width=True)
                                st.info("ğŸ’¡ åœ“é¤…åœ–é¡¯ç¤ºå‰20å¤§ç”¢å“çš„éŠ·å”®é¡ä½”æ¯”ã€‚")
                            else: st.info("ç¸½éŠ·å”®é¡ç‚ºé›¶ï¼Œç„¡æ³•è¨ˆç®—ä½”æ¯”ã€‚")
                    else: st.warning("ç„¡ç”¢å“å½™ç¸½è³‡æ–™å¯é¡¯ç¤ºã€‚")

                with product_tab2:
                    st.markdown("### ğŸ“Š ç”¢å“åˆ†æåœ–è¡¨")
                    if not summary_df.empty:
                        viz_type = st.selectbox("é¸æ“‡åœ–è¡¨é¡å‹", ["ç”¢å“éŠ·é‡èˆ‡éŠ·å”®é¡æ¯”è¼ƒ", "ç”¢å“æ¯›åˆ©åˆ†æ", "ç”¢å“é¡åˆ¥åˆ†æ", "ç”¢å“æˆæœ¬çµæ§‹"], key="viz_type_selector")
                        # ... [Visualization code using summary_df, ensure keys are used for widgets] ...
                        # Example for Profit Analysis Scatter Plot
                        if viz_type == "ç”¢å“æ¯›åˆ©åˆ†æ":
                             if 'ç²¾æº–æ¯›åˆ©' in summary_df.columns and 'å°è¨ˆ' in summary_df.columns:
                                 profit_data = summary_df.copy()
                                 profit_data['æ¯›åˆ©ç‡'] = profit_data.apply(lambda x: (x['ç²¾æº–æ¯›åˆ©'] / x['å°è¨ˆ'] * 100) if x['å°è¨ˆ'] > 0 else 0, axis=1)
                                 profit_data['ç²¾æº–æ¯›åˆ©_abs'] = profit_data['ç²¾æº–æ¯›åˆ©'].abs()
                                 import plotly.express as px
                                 fig = px.scatter(profit_data, x='å°è¨ˆ', y='æ¯›åˆ©ç‡', size='ç²¾æº–æ¯›åˆ©_abs', color='ç²¾æº–æ¯›åˆ©', hover_name='ç”¢å“åç¨±', text='ç”¢å“ä»£è™Ÿ', title='ç”¢å“éŠ·å”®é¡ vs æ¯›åˆ©ç‡åˆ†æ', labels={'å°è¨ˆ': 'éŠ·å”®é¡', 'æ¯›åˆ©ç‡': 'æ¯›åˆ©ç‡ (%)', 'ç²¾æº–æ¯›åˆ©': 'æ¯›åˆ©é¡'}, color_continuous_scale=px.colors.sequential.Viridis)
                                 fig.update_traces(textposition='top center', marker=dict(sizemin=5), selector=dict(mode='markers+text'))
                                 fig.update_layout(xaxis=dict(title='éŠ·å”®é¡', tickprefix='$'), yaxis=dict(title='æ¯›åˆ©ç‡ (%)'))
                                 st.plotly_chart(fig, use_container_width=True)
                                 # ... [rest of profit analysis code] ...
                             else: st.warning("ç¼ºå°‘ 'ç²¾æº–æ¯›åˆ©' æˆ– 'å°è¨ˆ' æ¬„ä½ï¼Œç„¡æ³•é€²è¡Œæ¯›åˆ©åˆ†æã€‚")
                        # ... [Implement other viz_type options similarly] ...
                    else: st.warning("ç„¡ç”¢å“å½™ç¸½è³‡æ–™å¯ä¾›åˆ†æã€‚")
                    
                with product_tab3:
                    st.markdown("### ğŸ“‹ å®Œæ•´ç”¢å“åˆ—è¡¨")
                    st.dataframe(summary_df, use_container_width=True)
                    if not summary_df.empty:
                        csv_summary = summary_df.to_csv(index=False).encode('utf-8-sig')
                        st.download_button("ğŸ’¾ ä¸‹è¼‰ç”¢å“å½™ç¸½ CSV", csv_summary, f"product_summary_{'_'.join(selected_months_display)}.csv", "text/csv", key="download_summary")
                
                with product_tab4:
                    st.markdown("### ğŸ“ˆ ABC åˆ†æ (ä¾éŠ·å”®é¡)")
                    st.markdown("""
                    **åˆ†é¡æ¨™æº–:**
                    - **Aé¡:** ç´¯è¨ˆéŠ·å”®é¡ä½”æ¯”å‰ 70% çš„ç”¢å“
                    - **Bé¡:** ç´¯è¨ˆéŠ·å”®é¡ä½”æ¯” 70% - 90% çš„ç”¢å“
                    - **Cé¡:** ç´¯è¨ˆéŠ·å”®é¡ä½”æ¯”å¾Œ 10% çš„ç”¢å“
                    """)
                    abc_df = st.session_state.get('abc_analysis_df', pd.DataFrame())
                    if not abc_df.empty:
                        # Display summary counts
                        abc_counts = abc_df['ABCåˆ†é¡'].value_counts().sort_index()
                        st.metric("Aé¡ç”¢å“æ•¸é‡", abc_counts.get('A', 0))
                        st.metric("Bé¡ç”¢å“æ•¸é‡", abc_counts.get('B', 0))
                        st.metric("Cé¡ç”¢å“æ•¸é‡", abc_counts.get('C', 0))

                        # Format percentage column
                        display_abc_df = abc_df.copy()
                        if 'ç´¯è¨ˆéŠ·å”®ä½”æ¯”' in display_abc_df.columns:
                             display_abc_df['ç´¯è¨ˆéŠ·å”®ä½”æ¯”'] = display_abc_df['ç´¯è¨ˆéŠ·å”®ä½”æ¯”'].map('{:.2f}%'.format)
                        if 'å°è¨ˆ' in display_abc_df.columns:
                             display_abc_df['å°è¨ˆ'] = display_abc_df['å°è¨ˆ'].apply(lambda x: f"${x:,.0f}")
                        if 'ç´¯è¨ˆéŠ·å”®é¡' in display_abc_df.columns:
                             display_abc_df['ç´¯è¨ˆéŠ·å”®é¡'] = display_abc_df['ç´¯è¨ˆéŠ·å”®é¡'].apply(lambda x: f"${x:,.0f}")


                        st.dataframe(display_abc_df, use_container_width=True)
                        csv_abc = abc_df.to_csv(index=False).encode('utf-8-sig')
                        st.download_button(
                            "ğŸ’¾ ä¸‹è¼‰ ABC åˆ†æ CSV",
                            csv_abc,
                            f"abc_analysis_{'_'.join(selected_months_display)}.csv",
                            "text/csv",
                            key="download_abc"
                        )
                    else:
                        st.warning("ç„¡æ³•é¡¯ç¤º ABC åˆ†æï¼Œè«‹ç¢ºèªå·²ç”Ÿæˆå ±è¡¨ä¸”éŠ·å”®è³‡æ–™åŒ…å« 'å°è¨ˆ' æ¬„ä½ã€‚")


            with order_tab:
                st.subheader(f"ğŸ“‹ {'ã€'.join(selected_months_display)} è¨‚å–®æ‘˜è¦è¡¨")
                
                # Add tabs within Order Summary
                order_tab1, order_tab2, order_tab3, order_tab4 = st.tabs(["éŠ·å”®è¡¨ç¾æ‘˜è¦", "å‰10å¤§å®¢æˆ¶", "è¨‚å–®è©³ç´°è³‡æ–™", "éŠ·å”®è¶¨å‹¢"])

                with order_tab1: # Sales Performance Summary
                    if order_summary.empty:
                        st.warning("ç„¡æ³•ç”Ÿæˆè¨‚å–®æ‘˜è¦ï¼Œå¯èƒ½ç¼ºå°‘å¿…è¦çš„è¨‚å–®è³‡æ–™æ¬„ä½ã€‚")
                    else:
                        total_column = next((col for col in ['ç¸½è¨ˆé‡‘é¡', 'å¯¦æ”¶ç¸½é¡', 'æœªç¨…å°è¨ˆ', 'è¨‚å–®ç”¢å“ç¸½é¡'] if col in order_summary.columns), None)
                        st.markdown("### ğŸ“Š éŠ·å”®è¡¨ç¾æ‘˜è¦")
                        total_orders = len(order_summary)
                        total_amount = order_summary[total_column].sum() if total_column else 0
                        avg_order = total_amount / total_orders if total_orders > 0 else 0
                        
                        # Create a stylish metrics section with custom styling
                        metrics_container = st.container()
                        with metrics_container:
                            # Use CSS to style the metrics cards
                            st.markdown("""
                            <style>
                            .metric-card {
                                background-color: #f8f9fa;
                                border-radius: 10px;
                                padding: 20px;
                                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
                                text-align: center;
                                transition: transform 0.3s;
                            }
                            .metric-card:hover {
                                transform: translateY(-5px);
                            }
                            .metric-value {
                                font-size: 28px;
                                font-weight: bold;
                                color: #1E88E5;
                                margin: 10px 0;
                            }
                            .metric-title {
                                font-size: 16px;
                                color: #455A64;
                                margin-top: 5px;
                            }
                            </style>
                            """, unsafe_allow_html=True)

                            # Create three metric cards in columns
                            col1, col2, col3 = st.columns(3)

                            with col1:
                                st.markdown(f"""
                                <div class="metric-card">
                                    <div class="metric-title">è¨‚å–®ç¸½æ•¸</div>
                                    <div class="metric-value">{total_orders:,}</div>
                                    <div class="metric-title">ç­†</div>
                                </div>
                                """, unsafe_allow_html=True)

                            with col2:
                                st.markdown(f"""
                                <div class="metric-card">
                                    <div class="metric-title">ç¸½éŠ·å”®é¡</div>
                                    <div class="metric-value">${total_amount:,.0f}</div>
                                    <div class="metric-title">æ–°å°å¹£</div>
                                </div>
                                """, unsafe_allow_html=True)

                            with col3:
                                st.markdown(f"""
                                <div class="metric-card">
                                    <div class="metric-title">å¹³å‡è¨‚å–®é‡‘é¡</div>
                                    <div class="metric-value">${avg_order:,.0f}</div>
                                    <div class="metric-title">æ–°å°å¹£/ç­†</div>
                                </div>
                                """, unsafe_allow_html=True)

                        # Add spacing
                        st.markdown("<br>", unsafe_allow_html=True)

                with order_tab2: # Top 10 Customers
                    if order_summary.empty:
                         st.warning("ç„¡æ³•ç”Ÿæˆå®¢æˆ¶æ‘˜è¦ï¼Œå¯èƒ½ç¼ºå°‘å¿…è¦çš„è¨‚å–®è³‡æ–™æ¬„ä½ã€‚")
                    elif customer_summary.empty:
                         st.warning("ç„¡æ³•ç”Ÿæˆå®¢æˆ¶æ‘˜è¦ã€‚")
                    else:
                        st.markdown("### ğŸ† å‰10å¤§å®¢æˆ¶")
                        top_customers = customer_summary.head(10)
                        # Create two columns for visualization and table
                        viz_col, table_col = st.columns([3, 2])

                        with viz_col:
                            # Create a bar chart for top customers
                            data = {
                                'customer': top_customers['å®¢æˆ¶åç¨±'].tolist(),
                                'amount': top_customers['æ¶ˆè²»ç¸½é¡'].tolist(),
                                'orders': top_customers['è¨‚å–®æ•¸é‡'].tolist()
                            }

                            # Use Streamlit native charts instead of matplotlib to avoid font issues
                            import altair as alt

                            # Prepare the data in the format Altair expects
                            chart_data = pd.DataFrame({
                                'å®¢æˆ¶åç¨±': data['customer'],
                                'æ¶ˆè²»ç¸½é¡': data['amount'],
                                'è¨‚å–®æ•¸é‡': data['orders']
                            })

                            # Create label with formatted amount and order count
                            chart_data['label'] = chart_data.apply(
                                lambda x: f"${x['æ¶ˆè²»ç¸½é¡']:,.0f} ({x['è¨‚å–®æ•¸é‡']}ç­†)",
                                axis=1
                            )

                            # Create an Altair chart
                            chart = alt.Chart(chart_data).mark_bar().encode(
                                y=alt.Y('å®¢æˆ¶åç¨±:N', sort='-x', title=None),
                                x=alt.X('æ¶ˆè²»ç¸½é¡:Q', title='éŠ·å”®ç¸½é¡'),
                                color=alt.value('#2196F3'),
                                tooltip=['å®¢æˆ¶åç¨±', 'æ¶ˆè²»ç¸½é¡', 'è¨‚å–®æ•¸é‡', 'label']
                            ).properties(
                                title='å‰10å¤§å®¢æˆ¶éŠ·å”®é¡',
                                height=400
                            )

                            # Add text labels
                            text = chart.mark_text(
                                align='left',
                                baseline='middle',
                                dx=5,  # Offset the text slightly to the right of the bar
                                color='black'
                            ).encode(
                                text='label'
                            )

                            # Combine chart and text
                            final_chart = (chart + text).configure_view(
                                strokeWidth=0
                            ).configure_title(
                                fontSize=16,
                                font='Arial',
                                anchor='start',
                                fontWeight='bold'
                            ).configure_axis(
                                labelFontSize=12,
                                titleFontSize=14,
                                grid=False
                            )

                            # Display the chart
                            st.altair_chart(final_chart, use_container_width=True)

                        with table_col:
                            # Format the table for display
                            display_df = top_customers.copy()
                            display_df['æ¶ˆè²»ç¸½é¡'] = display_df['æ¶ˆè²»ç¸½é¡'].apply(lambda x: f"${x:,.0f}")
                            display_df['å¹³å‡å–®ç­†é‡‘é¡'] = display_df['å¹³å‡å–®ç­†é‡‘é¡'].apply(lambda x: f"${x:,.0f}")

                            # Display the table
                            st.dataframe(display_df, use_container_width=True)

                        # Add spacing
                        st.markdown("<br>", unsafe_allow_html=True)

                with order_tab3: # Order Details
                    if order_summary.empty:
                         st.warning("ç„¡æ³•é¡¯ç¤ºè¨‚å–®è©³ç´°è³‡æ–™ï¼Œå¯èƒ½ç¼ºå°‘å¿…è¦çš„è¨‚å–®è³‡æ–™æ¬„ä½ã€‚")
                    else:
                        st.markdown("### ğŸ“ è¨‚å–®è©³ç´°è³‡æ–™")
                        st.dataframe(order_summary, use_container_width=True)
                        csv_orders = order_summary.to_csv(index=False).encode('utf-8-sig')
                        st.download_button("ğŸ’¾ ä¸‹è¼‰è¨‚å–®æ‘˜è¦ CSV", csv_orders, f"order_summary_{'_'.join(selected_months_display)}.csv", "text/csv", key="download_orders")

                with order_tab4: # Sales Trend
                    st.markdown("### ğŸ“ˆ éŠ·å”®è¶¨å‹¢ (ä¾æœˆä»½)")
                    if order_summary.empty or 'éŠ·è²¨æ—¥æœŸ' not in order_summary.columns:
                        st.warning("ç„¡æ³•ç”ŸæˆéŠ·å”®è¶¨å‹¢åœ–ï¼Œç¼ºå°‘ 'éŠ·è²¨æ—¥æœŸ' æ¬„ä½ã€‚")
                    else:
                        # Ensure 'éŠ·è²¨æ—¥æœŸ' is datetime
                        order_summary['éŠ·è²¨æ—¥æœŸ'] = pd.to_datetime(order_summary['éŠ·è²¨æ—¥æœŸ'], errors='coerce')
                        # Drop rows where date conversion failed
                        order_summary_trend = order_summary.dropna(subset=['éŠ·è²¨æ—¥æœŸ']).copy()

                        if not order_summary_trend.empty:
                            # Determine the amount column to use
                            amount_col_trend = next((col for col in ['ç¸½è¨ˆé‡‘é¡', 'å¯¦æ”¶ç¸½é¡', 'æœªç¨…å°è¨ˆ', 'è¨‚å–®ç”¢å“ç¸½é¡'] if col in order_summary_trend.columns), None)

                            if amount_col_trend:
                                # Set date as index and resample by month
                                order_summary_trend.set_index('éŠ·è²¨æ—¥æœŸ', inplace=True)
                                monthly_sales = order_summary_trend.resample('ME')[amount_col_trend].sum().reset_index() # 'ME' for Month End
                                monthly_sales['éŠ·è²¨æœˆä»½'] = monthly_sales['éŠ·è²¨æ—¥æœŸ'].dt.strftime('%Y-%m') # Format for display

                                if not monthly_sales.empty:
                                    import altair as alt
                                    chart = alt.Chart(monthly_sales).mark_line(point=True).encode(
                                        x=alt.X('éŠ·è²¨æœˆä»½:T', title='æœˆä»½'),
                                        y=alt.Y(f'{amount_col_trend}:Q', title='æœˆéŠ·å”®é¡'),
                                        tooltip=['éŠ·è²¨æœˆä»½:T', alt.Tooltip(f'{amount_col_trend}:Q', format='$,.0f', title='éŠ·å”®é¡')]
                                    ).properties(
                                        title='æœˆéŠ·å”®é¡è¶¨å‹¢'
                                    )
                                    st.altair_chart(chart, use_container_width=True)
                                else:
                                    st.info("é¸å®šæœŸé–“å…§ç„¡è¶³å¤ è³‡æ–™ç¹ªè£½è¶¨å‹¢åœ–ã€‚")
                            else:
                                st.warning("è¨‚å–®æ‘˜è¦ä¸­æ‰¾ä¸åˆ°å¯ç”¨çš„é‡‘é¡æ¬„ä½ ('ç¸½è¨ˆé‡‘é¡', 'å¯¦æ”¶ç¸½é¡', 'æœªç¨…å°è¨ˆ', 'è¨‚å–®ç”¢å“ç¸½é¡') ä¾†ç¹ªè£½è¶¨å‹¢åœ–ã€‚")
                        else:
                             st.warning("ç„¡æ³•è½‰æ› 'éŠ·è²¨æ—¥æœŸ' ç‚ºæœ‰æ•ˆæ—¥æœŸæ ¼å¼ï¼Œç„¡æ³•ç¹ªè£½è¶¨å‹¢åœ–ã€‚")

            with bc_tab:
                st.subheader("ğŸ”„ BC è³‡æ–™æ¯”å°")
                bc_load_error_display = st.session_state.get('bc_load_error', None)
                merge_possible_display = st.session_state.get('merge_possible', False)

                if bc_load_error_display:
                    st.warning(f"BC è³‡æ–™è™•ç†å•é¡Œ: {bc_load_error_display}")
                    st.info("ç„¡æ³•é¡¯ç¤º BC ç›¸é—œå ±è¡¨ã€‚")
                elif bc_df.empty and not merge_possible_display:
                     # This case means BC file was likely not provided or selected
                     st.info("æœªæä¾› BC è³‡æ–™ï¼Œç„¡æ³•é€²è¡Œæ¯”å°åˆ†æã€‚")
                elif not merge_possible_display and not bc_df.empty:
                     # BC data loaded, but summary was invalid
                     st.warning("éŠ·å”®å½™ç¸½è³‡æ–™ç„¡æ•ˆï¼Œç„¡æ³•é€²è¡Œ BC æ¯”å°ã€‚")
                elif merge_possible_display and merged_df.empty:
                    # Merge was possible and attempted, but result is empty (no matches)
                    st.warning("éŠ·å”®è³‡æ–™ä¸­çš„ç”¢å“ä»£è™Ÿèˆ‡ BC è³‡æ–™ä¸­ç„¡åŒ¹é…é …ç›®ã€‚è«‹æª¢æŸ¥å…©ä»½æª”æ¡ˆä¸­çš„ç”¢å“ä»£è™Ÿæ˜¯å¦ä¸€è‡´ï¼ˆåŒ…æ‹¬æ ¼å¼èˆ‡ç©ºæ ¼ï¼‰ã€‚") # Refined message
                    # Optionally display the raw BC data here if needed
                    # st.subheader("å·²è¼‰å…¥çš„ BC è³‡æ–™ (ç„¡åŒ¹é…éŠ·å”®é …ç›®)")
                    # st.dataframe(bc_df, use_container_width=True)
                elif not merged_df.empty: # Corrected indentation for this block
                        # Merge successful and has data
                        # Add Dead Stock tab
                        bc_tab1, bc_tab2, bc_tab3, bc_tab4, bc_tab5 = st.tabs(["äº¤å‰æ¯”å°çµæœ", "å» å•†éŠ·å”®å½™ç¸½", "ç†±éŠ·å“ç²¾æº–æ¯›åˆ©é«˜åˆ°ä½", "å» å•†ç”¢å“æŸ¥è©¢", "æ»¯éŠ·åº«å­˜"])
                        with bc_tab1:
                            st.subheader("èˆ‡ BC è³‡æ–™äº¤å‰æ¯”å°çµæœ") # This header might be redundant now
                            st.dataframe(merged_df, use_container_width=True)
                            csv_merged = merged_df.to_csv(index=False).encode('utf-8-sig')
                            st.download_button("ä¸‹è¼‰äº¤å‰æ¯”å° CSV", csv_merged, f"merged_bc_sales_{'_'.join(selected_months_display)}.csv", "text/csv", key="download_merged")
                        with bc_tab2:
                            st.subheader("å» å•†éŠ·å”®å½™ç¸½")
                            vendor_df = vendor_summary_table(merged_df)
                            st.dataframe(vendor_df, use_container_width=True)
                            csv_vendor = vendor_df.to_csv(index=False).encode('utf-8-sig')
                            st.download_button("ä¸‹è¼‰å» å•†å½™ç¸½ CSV", csv_vendor, f"vendor_summary_{'_'.join(selected_months_display)}.csv", "text/csv", key="download_vendor")
                        with bc_tab3:
                            st.subheader("ç†±éŠ·å“ç²¾æº–æ¯›åˆ©é«˜åˆ°ä½")
                            if not summary_df.empty and 'ç²¾æº–æ¯›åˆ©' in summary_df.columns:
                                 profit_sorted_df = summary_df.sort_values(by='ç²¾æº–æ¯›åˆ©', ascending=False).copy()
                                 if not bc_df.empty and 'æ•¸é‡' in bc_df.columns and 'ç”¢å“ä»£è™Ÿ' in bc_df.columns:
                                     inventory_dict = dict(zip(bc_df['ç”¢å“ä»£è™Ÿ'], bc_df['æ•¸é‡']))
                                     profit_sorted_df['åº«å­˜'] = profit_sorted_df['ç”¢å“ä»£è™Ÿ'].map(inventory_dict)
                                     profit_sorted_df['åº«å­˜'] = profit_sorted_df['åº«å­˜'].astype(str).str.replace(',', '', regex=False)
                                     profit_sorted_df['åº«å­˜'] = pd.to_numeric(profit_sorted_df['åº«å­˜'], errors='coerce').fillna(0).astype(int)
                                     if 'æ•¸é‡' in profit_sorted_df.columns: # Check if sales quantity exists
                                         profit_sorted_df['éŠ·å”®/å¤©'] = profit_sorted_df['æ•¸é‡'] / 30
                                         profit_sorted_df['åº«å­˜å¤©æ•¸'] = profit_sorted_df.apply(lambda x: round(x['åº«å­˜'] / x['éŠ·å”®/å¤©']) if x['éŠ·å”®/å¤©'] > 0 else 0, axis=1)
                                         def inventory_status(days):
                                             if days <= 0: return "ç¼ºè²¨"
                                             elif days < 30: return "ä½"
                                             elif days < 90: return "é©ä¸­"
                                             else: return "éå¤š"
                                         profit_sorted_df['åº«å­˜ç‹€æ…‹'] = profit_sorted_df['åº«å­˜å¤©æ•¸'].apply(inventory_status)
                                     else: profit_sorted_df['åº«å­˜ç‹€æ…‹'] = "æœªçŸ¥ (ç„¡éŠ·å”®æ•¸é‡)"
                                 else: profit_sorted_df['åº«å­˜ç‹€æ…‹'] = "æœªçŸ¥ (ç„¡BCåº«å­˜)"
                                 # ... [Display table and download button for profit_sorted_df] ...
                                 st.dataframe(profit_sorted_df, use_container_width=True) # Simplified display for brevity
                                 csv_profit_sorted = profit_sorted_df.to_csv(index=False).encode('utf-8-sig')
                                 st.download_button("ä¸‹è¼‰ç²¾æº–æ¯›åˆ©æ’åº CSV", csv_profit_sorted, f"profit_sorted_products_{'_'.join(selected_months_display)}.csv", "text/csv", key="download_profit_sorted")
                            else: st.warning("ç¼ºå°‘è³‡æ–™ç„¡æ³•ç”Ÿæˆæ­¤å ±è¡¨ã€‚")
                        with bc_tab4:
                            st.subheader("å» å•†ç”¢å“æŸ¥è©¢ (æ–¼æœ¬æ¬¡å ±è¡¨ç¯„åœå…§)")
                            # Check if BC data and vendor column are available
                            if not bc_df.empty and 'å» å•†ç°¡ç¨±' in bc_df.columns:
                                # Check if merged data is available for vendor filtering
                                if not merged_df.empty and 'å» å•†ç°¡ç¨±' in merged_df.columns:
                                    # Get unique vendors from the *merged* data relevant to the report
                                    report_vendors = sorted(merged_df['å» å•†ç°¡ç¨±'].dropna().unique().tolist())
                                    # Optionally remove the placeholder vendor
                                    if 'æœªçŸ¥å» å•†' in report_vendors:
                                        report_vendors.remove('æœªçŸ¥å» å•†')

                                    # Proceed if there are actual vendors in the report data
                                    if report_vendors:
                                        selected_vendor_report = st.selectbox(
                                            "é¸æ“‡å» å•†æŸ¥çœ‹å…¶æœ¬æ¬¡éŠ·å”®ç”¢å“:",
                                            ["è«‹é¸æ“‡..."] + report_vendors,
                                            key="vendor_selector_report_tab" # Ensure unique key
                                        )
                                        # Display details if a vendor is selected
                                        if selected_vendor_report != "è«‹é¸æ“‡...":
                                            # Filter merged data for the selected vendor
                                            vendor_merged_products = merged_df[merged_df['å» å•†ç°¡ç¨±'] == selected_vendor_report].copy()
                                            st.write(f"**{selected_vendor_report}** åœ¨æœ¬æ¬¡å ±è¡¨ç¯„åœå…§éŠ·å”®äº† **{len(vendor_merged_products)}** é …ç”¢å“:")

                                            # Define and display relevant columns from the merged data
                                            display_cols_vendor = ['ç”¢å“ä»£è™Ÿ', 'ç”¢å“åç¨±', 'éŠ·å”®æ•¸é‡', 'å°è¨ˆ', 'ç²¾æº–æ¯›åˆ©', 'åº«å­˜æ•¸é‡']
                                            # Ensure columns exist before trying to display them
                                            final_display_cols = [col for col in display_cols_vendor if col in vendor_merged_products.columns]
                                            st.dataframe(
                                                vendor_merged_products[final_display_cols].sort_values(by='å°è¨ˆ', ascending=False),
                                                use_container_width=True
                                            )

                                            # Add an option to view all products from this vendor (using original BC data)
                                            if st.checkbox(f"é¡¯ç¤º {selected_vendor_report} çš„æ‰€æœ‰ BC ç”¢å“è³‡æ–™", key="show_all_vendor_bc_report"):
                                                # Filter original BC data for the selected vendor
                                                vendor_all_bc_products = bc_df[bc_df['å» å•†ç°¡ç¨±'] == selected_vendor_report].copy()
                                                st.write(f"**{selected_vendor_report}** åœ¨ BC è³‡æ–™ä¸­å…±æœ‰ **{len(vendor_all_bc_products)}** é …ç”¢å“:")
                                                # Define and display basic columns from BC data
                                                basic_columns_all = ['ç”¢å“ä»£è™Ÿ', 'ç”¢å“åç¨±', 'å–®ä½', 'æ•¸é‡'] # Note: 'æ•¸é‡' here is BC inventory
                                                display_columns_all = [col for col in basic_columns_all if col in vendor_all_bc_products.columns]
                                                st.dataframe(
                                                    vendor_all_bc_products[display_columns_all].sort_values('ç”¢å“ä»£è™Ÿ'),
                                                    use_container_width=True
                                                )
                                                # Provide download for all BC products of the vendor
                                                csv_vendor_all_bc = vendor_all_bc_products.to_csv(index=False).encode('utf-8-sig')
                                                st.download_button(
                                                    label=f"ä¸‹è¼‰ {selected_vendor_report} æ‰€æœ‰ BC ç”¢å“è³‡æ–™", # Corrected parameter name
                                                    data=csv_vendor_all_bc, # Corrected parameter name
                                                    file_name=f"{selected_vendor_report}_all_bc_products.csv", # Corrected parameter name
                                                    mime="text/csv", # Corrected parameter name
                                                    key="download_vendor_all_bc_report" # Ensure unique key
                                                )
                                    # Handle case where no vendors were found in the merged data
                                    else:
                                        st.info("æœ¬æ¬¡å ±è¡¨ç¯„åœå…§æ²’æœ‰ç‰¹å®šå» å•†çš„éŠ·å”®è³‡æ–™å¯ä¾›æŸ¥è©¢ã€‚")
                                # Handle case where merged data is missing vendor info
                                else:
                                    st.warning("åˆä½µå¾Œçš„è³‡æ–™ç¼ºå°‘å» å•†è³‡è¨Šï¼Œç„¡æ³•é€²è¡ŒæŸ¥è©¢ã€‚")
                            # Handle case where BC data is missing vendor info or not loaded
                            else:
                                st.warning("BCæª”æ¡ˆä¸­ç¼ºå°‘'å» å•†ç°¡ç¨±'æ¬„ä½æˆ–æœªè¼‰å…¥BCè³‡æ–™ï¼Œç„¡æ³•é€²è¡Œå» å•†ç”¢å“æŸ¥è©¢ã€‚")
                        with bc_tab5:
                             st.subheader("ğŸ“¦ æ»¯éŠ·åº«å­˜ (æœ‰åº«å­˜ä½†æœŸé–“ç„¡éŠ·å”®)")
                             dead_stock_df = st.session_state.get('dead_stock_df', pd.DataFrame())
                             if not dead_stock_df.empty:
                                 st.write(f"æ‰¾åˆ° {len(dead_stock_df)} é …æ»¯éŠ·ç”¢å“ (åº«å­˜ > 0 ä¸”æœ¬æœŸéŠ·å”® = 0)")
                                 st.dataframe(dead_stock_df, use_container_width=True)
                                 csv_dead_stock = dead_stock_df.to_csv(index=False).encode('utf-8-sig')
                                 st.download_button(
                                     "ğŸ’¾ ä¸‹è¼‰æ»¯éŠ·åº«å­˜ CSV",
                                     csv_dead_stock,
                                     f"dead_stock_{'_'.join(selected_months_display)}.csv",
                                     "text/csv",
                                     key="download_dead_stock"
                                 )
                             elif bc_df.empty or 'æ•¸é‡' not in bc_df.columns:
                                 st.info("ç„¡æ³•è¨ˆç®—æ»¯éŠ·åº«å­˜ï¼Œå› ç‚º BC è³‡æ–™æœªè¼‰å…¥æˆ–ç¼ºå°‘ 'æ•¸é‡' æ¬„ä½ã€‚")
                             else:
                                 st.success("âœ… åœ¨æ­¤æœŸé–“å…§ï¼Œæ‰€æœ‰æœ‰åº«å­˜çš„ç”¢å“éƒ½æœ‰éŠ·å”®ç´€éŒ„ã€‚")

        else:
            # This warning is for the main report generation failure
            st.warning("å ±è¡¨ç”Ÿæˆå¤±æ•—æˆ–æœªé¸æ“‡æœ‰æ•ˆæœˆä»½çš„è³‡æ–™ã€‚")

    # Display message if no report is generated yet and not in another section
    elif st.session_state.current_section == "ç”Ÿæˆå ±è¡¨":
         st.info("è«‹å…ˆé¸æ“‡æ•¸æ“šä¾†æºï¼Œé¸æ“‡æœˆä»½å¾ŒæŒ‰ä¸‹ã€ç”Ÿæˆå ±è¡¨ã€")

# --- File Management and Field Overview Sections ---
st.markdown("---")
st.subheader("ç³»çµ±è¨­å®šèˆ‡ç®¡ç†")
file_mgmt_col, fields_overview_col = st.columns(2)

with file_mgmt_col:
    st.markdown("### ğŸ“ æª”æ¡ˆç®¡ç†")
    if 'file_to_delete' not in st.session_state: st.session_state.file_to_delete = None
    if 'delete_type' not in st.session_state: st.session_state.delete_type = None
    file_mgmt_tab1, file_mgmt_tab2 = st.tabs(["éŠ·è²¨æª”æ¡ˆ", "BC æª”æ¡ˆ"])
    with file_mgmt_tab1:
        sales_files_list = get_file_list(SALES_DIR, ".xlsx")
        if st.button("ğŸ”„ é‡æ–°æ•´ç†éŠ·è²¨æª”æ¡ˆåˆ—è¡¨", key="refresh_sales"): st.rerun()
        if not sales_files_list: st.info("å°šæœªä¸Šå‚³ä»»ä½•éŠ·è²¨æª”æ¡ˆ")
        else:
            st.write(f"å·²ä¸Šå‚³çš„éŠ·è²¨æª”æ¡ˆ ({len(sales_files_list)})")
            for i, file in enumerate(sales_files_list):
                cols = st.columns([6, 3, 1])
                cols[0].text(f"{i+1}. {file['name']}")
                cols[1].text(f"ä¿®æ”¹: {file['mod_time_str']}")
                if cols[2].button("ğŸ—‘ï¸", key=f"delete_sales_{i}", help="åˆªé™¤æ­¤æª”æ¡ˆ"):
                    st.session_state.file_to_delete = file['path']
                    st.session_state.delete_type = "sales"
                    st.rerun()
            if st.button("ğŸ—‘ï¸ åˆªé™¤æ‰€æœ‰éŠ·è²¨æª”æ¡ˆ", type="secondary", key="delete_all_sales"):
                st.session_state.file_to_delete = "ALL_SALES"
                st.session_state.delete_type = "all_sales"
                st.rerun()
    with file_mgmt_tab2:
        bc_files_list = get_file_list(BC_DIR, ".xlsx")
        if st.button("ğŸ”„ é‡æ–°æ•´ç†BCæª”æ¡ˆåˆ—è¡¨", key="refresh_bc"): st.rerun()
        if not bc_files_list: st.info("å°šæœªä¸Šå‚³ä»»ä½• BC æª”æ¡ˆ")
        else:
            st.write(f"å·²ä¸Šå‚³çš„ BC æª”æ¡ˆ ({len(bc_files_list)})")
            for i, file in enumerate(bc_files_list):
                cols = st.columns([6, 3, 1])
                cols[0].text(f"{i+1}. {file['name']}")
                cols[1].text(f"ä¿®æ”¹: {file['mod_time_str']}")
                if cols[2].button("ğŸ—‘ï¸", key=f"delete_bc_{i}", help="åˆªé™¤æ­¤æª”æ¡ˆ"):
                    st.session_state.file_to_delete = file['path']
                    st.session_state.delete_type = "bc"
                    st.rerun()
            if st.button("ğŸ—‘ï¸ åˆªé™¤æ‰€æœ‰BCæª”æ¡ˆ", type="secondary", key="delete_all_bc"):
                st.session_state.file_to_delete = "ALL_BC"
                st.session_state.delete_type = "all_bc"
                st.rerun()

with fields_overview_col:
    with st.expander("ğŸ“ å¿…è¦æ¬„ä½ä¸€è¦½", expanded=False):
        st.markdown("ä¸Šå‚³æª”æ¡ˆéœ€åŒ…å«ä¸‹åˆ—æ¬„ä½ï¼Œå¦å‰‡å¯èƒ½å°è‡´åˆ†æå¤±æ•—ï¼š")
        header_tabs = st.tabs(list(header_categories.keys()))
        for i, (category, headers) in enumerate(header_categories.items()):
            with header_tabs[i]:
                category_headers = [h for h in headers if h in required_headers]
                if category_headers:
                    headers_df = pd.DataFrame({"æ¬„ä½åç¨±": category_headers})
                    st.dataframe(headers_df, use_container_width=True)
                else: st.info(f"æ­¤é¡åˆ¥ä¸­æ²’æœ‰å¿…è¦çš„æ¬„ä½")
        st.markdown("### ç¯„ä¾‹æª”æ¡ˆ")
        col1, col2 = st.columns(2)
        with col1:
            csv_sample = sample_df.to_csv(index=False).encode('utf-8-sig')
            st.download_button("â¬‡ï¸ ä¸‹è¼‰ç¯„ä¾‹æª”æ¡ˆCSV", csv_sample, "sales_sample.csv", "text/csv", key="download_csv_sample")
        with col2:
            excel_sample = BytesIO()
            sample_df.to_excel(excel_sample, index=False, engine='openpyxl')
            excel_sample.seek(0)
            st.download_button("â¬‡ï¸ ä¸‹è¼‰ç¯„ä¾‹æª”æ¡ˆExcel", excel_sample, "sales_sample.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", key="download_excel_sample")
        st.info("""ğŸ’¡ **æç¤º:** é—œéµæ¬„ä½å¦‚ã€Œç”¢å“ä»£è™Ÿã€ã€ã€Œæ•¸é‡ã€ã€ã€Œå°è¨ˆã€ç­‰å¿…é ˆæœ‰å€¼ã€‚""")

# Handle file deletion confirmation
if st.session_state.file_to_delete:
    with st.form(key="delete_confirmation_form"): # Added form key
        file_name_display = os.path.basename(st.session_state.file_to_delete) if st.session_state.delete_type in ["sales", "bc"] else "æ‰€æœ‰æª”æ¡ˆ"
        delete_msg = f"ç¢ºå®šè¦åˆªé™¤æª”æ¡ˆ '{file_name_display}' å—ï¼Ÿæ­¤æ“ä½œç„¡æ³•æ¢å¾©ã€‚"
        if st.session_state.delete_type == "all_sales": delete_msg = f"ç¢ºå®šè¦åˆªé™¤æ‰€æœ‰ {len(get_file_list(SALES_DIR, '.xlsx'))} å€‹éŠ·è²¨æª”æ¡ˆå—ï¼Ÿæ­¤æ“ä½œç„¡æ³•æ¢å¾©ã€‚"
        if st.session_state.delete_type == "all_bc": delete_msg = f"ç¢ºå®šè¦åˆªé™¤æ‰€æœ‰ {len(get_file_list(BC_DIR, '.xlsx'))} å€‹BCæª”æ¡ˆå—ï¼Ÿæ­¤æ“ä½œç„¡æ³•æ¢å¾©ã€‚"
        st.warning(delete_msg)
        
        col1, col2 = st.columns([1, 1])
        with col1: confirm_delete = st.form_submit_button("âš ï¸ ç¢ºèªåˆªé™¤")
        with col2: cancel_delete = st.form_submit_button("å–æ¶ˆ")
        
        if confirm_delete:
            deleted_count = 0
            if st.session_state.delete_type in ["sales", "bc"]:
                if delete_file(st.session_state.file_to_delete): deleted_count = 1
            elif st.session_state.delete_type == "all_sales":
                for file in get_file_list(SALES_DIR, ".xlsx"):
                    if delete_file(file['path']): deleted_count += 1
            elif st.session_state.delete_type == "all_bc":
                 for file in get_file_list(BC_DIR, ".xlsx"):
                    if delete_file(file['path']): deleted_count += 1
            
            st.success(f"å·²æˆåŠŸåˆªé™¤ {deleted_count} å€‹æª”æ¡ˆ")
            st.session_state.file_to_delete = None
            st.session_state.delete_type = None
            st.rerun()
        
        if cancel_delete:
            st.session_state.file_to_delete = None
            st.session_state.delete_type = None
            st.rerun()
